{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.layers = None\n",
    "        self.createLayers()\n",
    "\n",
    "    # See what data_format='channels_last' mean in MaxPooling2D ?\n",
    "    def createLayers(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(tf.layers.Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        \n",
    "        self.layers.append(tf.layers.Conv2D(64, (3, 3), padding='same', activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Conv2D(64, (3, 3), padding='same', activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        \n",
    "        self.layers.append(tf.layers.Dense(units=512, activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Dense(units=100))\n",
    "\n",
    "    def forward(self, x, apply_dropout, keep_prob_input=1.0, keep_prob_hidden=1.0):\n",
    "        layer_output = []\n",
    "        \n",
    "        y = x\n",
    "        \n",
    "        if (apply_dropout):\n",
    "            y = tf.nn.dropout(y, keep_prob_input)\n",
    "            \n",
    "        y = self.layers[0](y)\n",
    "        layer_output.append(y)\n",
    "        y = self.layers[1](y)\n",
    "        layer_output.append(y)\n",
    "        y = self.layers[2](y)\n",
    "        if (apply_dropout):\n",
    "            y = tf.nn.dropout(y, keep_prob_hidden)\n",
    "        layer_output.append(y)\n",
    "        \n",
    "        y = self.layers[3](y)\n",
    "        layer_output.append(y)\n",
    "        y = self.layers[4](y)\n",
    "        layer_output.append(y)\n",
    "        y = self.layers[5](y)\n",
    "        if (apply_dropout):\n",
    "            y = tf.nn.dropout(y, keep_prob_hidden)\n",
    "        layer_output.append(y)\n",
    "        \n",
    "        y = tf.layers.flatten(y)\n",
    "        y = self.layers[6](y)\n",
    "        if (apply_dropout):\n",
    "            y = tf.nn.dropout(y, keep_prob_hidden)\n",
    "        layer_output.append(y)\n",
    "        y = self.layers[7](y)\n",
    "        layer_output.append(y)\n",
    "        \n",
    "        return y, layer_output\n",
    "\n",
    "    def getLayerVariables(self):\n",
    "        l = []\n",
    "        for i in range(len(self.layers)):\n",
    "            l.extend(self.layers[i].variables)\n",
    "        return l\n",
    "    def name(self):\n",
    "        return 'n1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    pass\n",
    "#     task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints/'\n",
    "summaries_path = cur_dir + 'summaries/'\n",
    "data_path = task_home + 'cifar-100-python/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "def splitDataset(dataset, dataset_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    task_list = []\n",
    "    train_labels = np.argmax(dataset.train.labels, axis=1)\n",
    "    validation_labels = np.argmax(dataset.validation.labels, axis=1)\n",
    "    test_labels = np.argmax(dataset.test.labels, axis=1)\n",
    "    for i in range(len(dataset_split)):\n",
    "        cur_train_indices = [False] * dataset.train.images.shape[0]\n",
    "        cur_validation_indices = [False] * dataset.validation.images.shape[0]\n",
    "        cur_test_indices = [False] * dataset.test.images.shape[0]\n",
    "        for j in range(len(dataset_split[i])):\n",
    "            cur_train_indices = np.logical_or(cur_train_indices, (train_labels == dataset_split[i][j]))\n",
    "            cur_validation_indices = np.logical_or(cur_validation_indices, (validation_labels == dataset_split[i][j]))\n",
    "            cur_test_indices = np.logical_or(cur_test_indices, (test_labels == dataset_split[i][j]))\n",
    "\n",
    "        task = deepcopy(dataset)\n",
    "        task.train.images = task.train.images[cur_train_indices]\n",
    "        task.train.labels = task.train.labels[cur_train_indices]\n",
    "        task.validation.images = task.validation.images[cur_validation_indices]\n",
    "        task.validation.labels = task.validation.labels[cur_validation_indices]\n",
    "        task.test.images = task.test.images[cur_test_indices]\n",
    "        task.test.labels = task.test.labels[cur_test_indices]\n",
    "        task = MyTask(task)\n",
    "        task_list.append(task)\n",
    "\n",
    "    return task_list\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "\n",
    "class TempDataset(object):\n",
    "    def __init__(self):\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "    \n",
    "class TempTask(object):\n",
    "    def __init__(self):\n",
    "        self.train = TempDataset()\n",
    "        self.validation = TempDataset()\n",
    "        self.test = TempDataset()\n",
    "    \n",
    "    \n",
    "def readDatasets():\n",
    "    split = [list(np.arange(80))]\n",
    "    task_weights = [0.8]\n",
    "    for i in range(80, 100):\n",
    "        split.append([i])\n",
    "        task_weights.append(0.2/20)\n",
    "        \n",
    "    num_tasks = len(split)\n",
    "    \n",
    "    with open(data_path + 'train', 'rb') as f:\n",
    "        f_train_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    with open(data_path + 'test', 'rb') as f:\n",
    "        f_test_data = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "    num_class = 100\n",
    "        \n",
    "    cifar_100 = TempTask()\n",
    "    temp_train_labels = np.array(f_train_data[b'fine_labels'], dtype=np.int32)\n",
    "    temp_test_labels = np.array(f_test_data[b'fine_labels'], dtype=np.int32)\n",
    "    f_train_data[b'fine_labels'] = np.zeros((temp_train_labels.shape[0], num_class))\n",
    "    (f_train_data[b'fine_labels'])[range(temp_train_labels.shape[0]), temp_train_labels] = 1\n",
    "    f_test_data[b'fine_labels'] = np.zeros((temp_test_labels.shape[0], num_class))\n",
    "    (f_test_data[b'fine_labels'])[range(temp_test_labels.shape[0]), temp_test_labels] = 1\n",
    "    f_train_data[b'data'] = np.reshape(f_train_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_test_data[b'data'] = np.reshape(f_test_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_train_data[b'data'] = np.transpose(f_train_data[b'data'], (0, 2, 3, 1))\n",
    "    f_test_data[b'data'] = np.transpose(f_test_data[b'data'], (0, 2, 3, 1))\n",
    "    \n",
    "    max_intensity = np.max(f_train_data[b'data'])\n",
    "    f_train_data[b'data'] = f_train_data[b'data'].astype(np.float32) / max_intensity\n",
    "    f_test_data[b'data'] = f_test_data[b'data'].astype(np.float32) / max_intensity\n",
    "    \n",
    "    seed = 0\n",
    "    np.random.seed(0)\n",
    "    shuffle_train_perm = np.random.permutation(f_train_data[b'data'].shape[0])\n",
    "    f_train_data[b'data'] = f_train_data[b'data'][shuffle_train_perm]\n",
    "    f_train_data[b'fine_labels'] = f_train_data[b'fine_labels'][shuffle_train_perm]\n",
    "    \n",
    "    num_val_per_class = 20\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        pos = (np.argmax(f_train_data[b'fine_labels'], axis=1) == i)\n",
    "        \n",
    "        if (i == 0):\n",
    "            cifar_100.validation.images = (f_train_data[b'data'][pos])[0 : num_val_per_class]\n",
    "            cifar_100.validation.labels = (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]\n",
    "\n",
    "            cifar_100.train.images = (f_train_data[b'data'][pos])[num_val_per_class : ]\n",
    "            cifar_100.train.labels = (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]\n",
    "        else:\n",
    "            cifar_100.validation.images = np.concatenate((cifar_100.validation.images, (f_train_data[b'data'][pos])[0 : num_val_per_class]))\n",
    "            cifar_100.validation.labels = np.concatenate((cifar_100.validation.labels, (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]))\n",
    "\n",
    "            cifar_100.train.images = np.concatenate((cifar_100.train.images, (f_train_data[b'data'][pos])[num_val_per_class : ]))\n",
    "            cifar_100.train.labels = np.concatenate((cifar_100.train.labels, (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]))\n",
    "        \n",
    "    cifar_100.test.images = f_test_data[b'data']\n",
    "    cifar_100.test.labels = f_test_data[b'fine_labels']\n",
    "    \n",
    "    shuffle_train_perm = np.random.permutation(cifar_100.train.images.shape[0])\n",
    "    cifar_100.train.images = cifar_100.train.images[shuffle_train_perm]\n",
    "    cifar_100.train.labels = cifar_100.train.labels[shuffle_train_perm]\n",
    "    \n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(cifar_100)\n",
    "        \n",
    "    task_list = splitDataset(cifar_100, split, seed)\n",
    "    return split, num_tasks, task_weights, task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "output_shape = (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../classifiers.py:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "seed = 0\n",
    "np.random.seed(0)\n",
    "learning_rates = [1e-3]\n",
    "fisher_multipliers = [0.0]\n",
    "dropout_input_probs = [0.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(fisher_multipliers, dropout_input_probs, dropout_hidden_probs, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['fisher_multiplier'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[3]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.eval_frequency = 100\n",
    "\n",
    "num_epochs = 60\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, save_weights=False, num_updates=num_updates, update_fisher=False)\n",
    "\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "print(best_hparams)\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 128\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hparams_tuple = tuner.hparamsDictToTuple(best_hparams, tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, len(cur_res['loss']), tuner.eval_frequency)\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, cur_best_avg_updates // tuner.eval_frequency])))\n",
    "plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "plt.plot(cur_res['val_acc'][0], color='b')\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "# plt.plot(cur_res['val_acc'][2], color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 20\n",
    "learning_rates = list(np.logspace(-5, -3, 3))\n",
    "fisher_multipliers = [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0, 200.0, 500.0, 1000.0, 1e4, 2e4, 5e4, 1e5, 2e5, 5e5, 1e6]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(fisher_multipliers, dropout_input_probs, dropout_hidden_probs, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['fisher_multiplier'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[3]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "##################### temporary - subject to modifications ################\n",
    "for i in range(len(hparams) - 1):\n",
    "    tuner.hparams_list[0].append(tuner.hparams_list[0][0])\n",
    "###########################################################################\n",
    "    \n",
    "for i in range(1, t + 1):\n",
    "    tuner.hparams_list[i] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(hparams)\n",
    "num_epochs = 60\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg, best_hparams_index = tuner.tuneTasksInRange(1, t, BATCH_SIZE, num_hparams, \n",
    "                                                        num_updates=num_updates, update_fisher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, save_weights=False, num_updates=num_updates, update_fisher=False)\n",
    "\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [math.log10(h['fisher_multiplier'] + 1) for h in hparams]\n",
    "colors = []\n",
    "print(best_hparams)\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 128\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hparams_tuple = tuner.hparamsDictToTuple(best_hparams, tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, len(cur_res['loss']), tuner.eval_frequency)\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, cur_best_avg_updates // tuner.eval_frequency])))\n",
    "plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "plt.plot(cur_res['val_acc'][0], color='b')\n",
    "plt.plot(cur_res['val_acc'][1], color='g')\n",
    "# plt.plot(cur_res['val_acc'][2], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2\n",
    "seed = 0\n",
    "np.random.seed(0)\n",
    "learning_rates = list(np.logspace(-5, -1, 5))\n",
    "fisher_multipliers = [0.0, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0, 200.0, 500.0, 1000.0, 1e4, 2e4, 5e4, 1e5, 2e5, 5e5, 1e6]\n",
    "dropout_input_probs = [0.0]\n",
    "dropout_hidden_probs = [1.0, 0.8]\n",
    "prod = list(itertools.product(fisher_multipliers, dropout_input_probs, dropout_hidden_probs, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['fisher_multiplier'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[3]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.eval_frequency = 100\n",
    "\n",
    "num_epochs = 20\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, save_weights=False, num_updates=num_updates, update_fisher=False)\n",
    "\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.saveResultsList()\n",
    "tuner.saveBestHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(t + 1):\n",
    "    print(tuner.best_hparams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatrix(tuner):\n",
    "    num_labels = 10\n",
    "    pred = np.array([])\n",
    "    actual = np.array([])\n",
    "    for j in range(t + 1):\n",
    "        val_data = tuner.task_list[j].validation\n",
    "        feed_dict = tuner.classifier.createFeedDict(val_data.images, val_data.labels)\n",
    "        cur_scores, cur_y = tuner.classifier.getPredictions(sess, feed_dict)\n",
    "        cur_pred = np.argmax(cur_scores, 1)\n",
    "        cur_actual = np.argmax(cur_y, 1)\n",
    "        actual = np.concatenate([actual, cur_actual])\n",
    "        pred = np.concatenate([pred, cur_pred])\n",
    "    confusion_matrix = np.zeros((num_labels,num_labels), dtype=np.int64)\n",
    "\n",
    "    for i in range(actual.shape[0]):\n",
    "        confusion_matrix[int(actual[i]), int(pred[i])] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def printConfusionMatrix(confusion_matrix):\n",
    "    print(\"%3d\" % (0, ), end=' ')\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        print(\"%3d\" % (j, ), end=' ')\n",
    "    print(\"\")\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        print(\"%3d\" % (i, ), end=' ')\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            print(\"%3d\" % (confusion_matrix[i][j], ), end= ' ')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = getConfusionMatrix(tuner)\n",
    "printConfusionMatrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tuner.appended_task_list[1].train.images[0].reshape(28, 28), cmap='gray')\n",
    "examples_per_class_1 = np.sum(tuner.appended_task_list[1].train.labels, axis=0).astype(np.int64)\n",
    "examples_per_class_2 = np.sum(tuner.appended_task_list[2].train.labels, axis=0).astype(np.int64)\n",
    "print(examples_per_class_1)\n",
    "print(examples_per_class_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 32\n",
    "test_till_task = 4\n",
    "accuracy = tuner.test(test_till_task, TEST_BATCH_SIZE)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
