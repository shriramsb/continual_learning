{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.layers = None\n",
    "        self.createLayers()\n",
    "\n",
    "    def createLayers(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(tf.layers.Dense(units=256, activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Dense(units=256, activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Dense(units=10))\n",
    "\n",
    "    def forward(self, x, apply_dropout, keep_prob_input=1.0, keep_prob_hidden=1.0):\n",
    "        layer_output = []\n",
    "        input_shape = np.prod(x.shape.as_list()[1:])\n",
    "        x = tf.reshape(x, [-1, input_shape])\n",
    "        if (apply_dropout):\n",
    "            x = tf.nn.dropout(x, keep_prob_input)\n",
    "        y = x\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            y = self.layers[i](y)\n",
    "            if (apply_dropout):\n",
    "                y = tf.nn.dropout(y, keep_prob_hidden)\n",
    "            layer_output.append(y)\n",
    "        y = self.layers[-1](y)\n",
    "        layer_output.append(y)\n",
    "        return y, layer_output\n",
    "\n",
    "    def getLayerVariables(self):\n",
    "        l = []\n",
    "        for i in range(len(self.layers)):\n",
    "            l.extend(self.layers[i].variables)\n",
    "        return l\n",
    "    def name(self):\n",
    "        return 'fc256_fc256_fc10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    pass\n",
    "#     task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints/'\n",
    "summaries_path = cur_dir + 'summaries/'\n",
    "data_path = task_home + 'MNIST_data/'\n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "def split_mnist(mnist, dataset_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    task_list = []\n",
    "    train_labels = np.argmax(mnist.train.labels, axis=1)\n",
    "    validation_labels = np.argmax(mnist.validation.labels, axis=1)\n",
    "    test_labels = np.argmax(mnist.test.labels, axis=1)\n",
    "    for i in range(len(dataset_split)):\n",
    "        cur_train_indices = [False] * mnist.train.images.shape[0]\n",
    "        cur_validation_indices = [False] * mnist.validation.images.shape[0]\n",
    "        cur_test_indices = [False] * mnist.test.images.shape[0]\n",
    "        for j in range(len(dataset_split[i])):\n",
    "            cur_train_indices = np.logical_or(cur_train_indices, (train_labels == dataset_split[i][j]))\n",
    "            cur_validation_indices = np.logical_or(cur_validation_indices, (validation_labels == dataset_split[i][j]))\n",
    "            cur_test_indices = np.logical_or(cur_test_indices, (test_labels == dataset_split[i][j]))\n",
    "\n",
    "        task = deepcopy(mnist)\n",
    "        task.train._images = task.train._images[cur_train_indices]\n",
    "        task.train._labels = task.train._labels[cur_train_indices]\n",
    "        task.validation._images = task.validation._images[cur_validation_indices]\n",
    "        task.validation._labels = task.validation._labels[cur_validation_indices]\n",
    "        task.test._images = task.test._images[cur_test_indices]\n",
    "        task.test._labels = task.test._labels[cur_test_indices]\n",
    "        task = MyTask(task)\n",
    "        task_list.append(task)\n",
    "\n",
    "    return task_list\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "\n",
    "def readDatasets():\n",
    "    split = [[0, 1, 2, 3, 4, 5, 6, 7], [8], [9]]\n",
    "    num_tasks = 3\n",
    "    task_weights = [0.8, 0.1, 0.1]\n",
    "    \n",
    "    mnist = read_data_sets(data_path, one_hot=True)\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    shuffle_train_perm = np.random.permutation(mnist.train._images.shape[0])\n",
    "    mnist.train._images = mnist.train._images[shuffle_train_perm, :]\n",
    "    mnist.train._labels = mnist.train._labels[shuffle_train_perm, :]\n",
    "    \n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(mnist)\n",
    "        \n",
    "    task_list = split_mnist(mnist, split, seed)\n",
    "    return split, num_tasks, task_weights, task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28 * 28, )\n",
    "output_shape = (10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-ba0dbb805d6b>:44: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From ../../../classifiers.py:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.8,dropout_input_prob=0.8,fisher_multiplier=0.0,learning_rate=0.001,per_example_append=0,task=0.ckpt-6500\n"
     ]
    }
   ],
   "source": [
    "tuner.classifier.restoreModel(tuner.sess, tuner.best_hparams[t][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = sess.run(tuner.classifier.fisher_diagonal[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033404585 3.2578303e-08\n",
      "0.005159856 7.380013e-08\n",
      "0.003307979 3.0114474e-08\n",
      "0.0050348323 6.843579e-08\n",
      "0.0023066532 1.4250447e-08\n",
      "0.005910843 9.733458e-08\n",
      "0.0053031663 8.275407e-08\n",
      "0.0037771338 4.0740204e-08\n",
      "1.9614368e-09 9.533352e-21\n",
      "2.8567997e-09 2.1275817e-20\n"
     ]
    }
   ],
   "source": [
    "for i in range(output_shape[0]):\n",
    "    print(np.linalg.norm(final_weights[:, i]), np.var(final_weights[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tuner.task_list[t].validation\n",
    "feed_dict = tuner.classifier.createFeedDict(val_data.images, val_data.labels)\n",
    "cur_scores, cur_y = tuner.classifier.getPredictions(sess, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646.5424 88.330864\n",
      "523.2776 64.73493\n",
      "544.473 64.53515\n",
      "576.9781 81.04634\n",
      "616.284 89.30173\n",
      "595.80255 84.68893\n",
      "708.8196 101.67858\n",
      "602.1324 87.30986\n",
      "1074.6989 20.220638\n",
      "1039.156 18.482508\n"
     ]
    }
   ],
   "source": [
    "for i in range(output_shape[0]):\n",
    "    print(np.linalg.norm(cur_scores[:, i]), np.var(cur_scores[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
