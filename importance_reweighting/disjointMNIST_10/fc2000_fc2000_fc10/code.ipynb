{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.layers = None\n",
    "        self.createLayers()\n",
    "\n",
    "    def createLayers(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(tf.layers.Dense(units=2000, activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Dense(units=2000, activation=tf.nn.relu))\n",
    "        self.layers.append(tf.layers.Dense(units=10))\n",
    "\n",
    "    def forward(self, x, apply_dropout, keep_prob_input=1.0, keep_prob_hidden=1.0):\n",
    "        layer_output = []\n",
    "        input_shape = np.prod(x.shape.as_list()[1:])\n",
    "        x = tf.reshape(x, [-1, input_shape])\n",
    "        if (apply_dropout):\n",
    "            x = tf.nn.dropout(x, keep_prob_input)\n",
    "        y = x\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            y = self.layers[i](y)\n",
    "            if (apply_dropout):\n",
    "                y = tf.nn.dropout(y, keep_prob_hidden)\n",
    "            layer_output.append(y)\n",
    "        y = self.layers[-1](y)\n",
    "        layer_output.append(y)\n",
    "        return y, layer_output\n",
    "\n",
    "    def getLayerVariables(self):\n",
    "        l = []\n",
    "        for i in range(len(self.layers)):\n",
    "            l.extend(self.layers[i].variables)\n",
    "        return l\n",
    "    def name(self):\n",
    "        return 'fc2000_fc2000_fc10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints/'\n",
    "summaries_path = cur_dir + 'summaries/'\n",
    "data_path = task_home + 'MNIST_data/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "def readSplit(split_path):\n",
    "    split = []\n",
    "    try:\n",
    "        f = open(split_path)\n",
    "        while (True):\n",
    "            line = f.readline()\n",
    "            if (line == \"\"):\n",
    "                break\n",
    "            split.append([float(i) for i in line.split()])\n",
    "    except IOError:\n",
    "        print(\"split path file not found\")\n",
    "        exit(-1)\n",
    "    return split\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "    \n",
    "def permuteDataset(task, seed):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(task.train._images.shape[1])\n",
    "    permuted = deepcopy(task)\n",
    "    permuted.train._images = permuted.train._images[:, perm]\n",
    "    permuted.test._images = permuted.test._images[:, perm]\n",
    "    permuted.validation._images = permuted.validation._images[:, perm]\n",
    "    return MyTask(permuted)\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "    \n",
    "    \n",
    "def readDatasets():\n",
    "    num_tasks = 10\n",
    "    num_classes = 10\n",
    "    task_weights = []\n",
    "    split = [[] for _ in range(num_tasks)]\n",
    "    for i in range(num_tasks):\n",
    "        for j in range(num_classes):\n",
    "            split[i].append(j)\n",
    "        task_weights.append(1.0)\n",
    "    \n",
    "    mnist = read_data_sets(data_path, one_hot=True)\n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(mnist)\n",
    "        \n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    shuffle_train_perm = np.random.permutation(mnist.train._images.shape[0])\n",
    "    mnist.train._images = mnist.train._images[shuffle_train_perm, :]\n",
    "    mnist.train._labels = mnist.train._labels[shuffle_train_perm, :]\n",
    "    \n",
    "    task_list = []\n",
    "    task_list.append(MyTask(mnist))\n",
    "    for seed in range(1, num_tasks):\n",
    "        task_list.append(permuteDataset(mnist, seed))\n",
    "    return split, num_tasks, task_weights, task_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28 * 28, )\n",
    "output_shape = (10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-188b9db8bb45>:46: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From ../../classifiers.py:56: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.num_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# task 0 - 4\n",
    "t = 4\n",
    "learning_rates = [1e-4]\n",
    "fisher_multipliers = [0.0]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.75]\n",
    "prod = list(itertools.product(fisher_multipliers, dropout_input_probs, dropout_hidden_probs, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['fisher_multiplier'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[3]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "\n",
    "num_hparams = 1\n",
    "for i in range(t + 1):\n",
    "    tuner.hparams_list[i] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_updates = math.ceil(tuner.task_list[0].train.images.shape[0] / BATCH_SIZE) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=0\n",
      "validation accuracies: [0.204], loss: 2.356296\n",
      "validation accuracies: [0.9724], loss: 0.102200\n",
      "validation accuracies: [0.9806], loss: 0.093562\n",
      "validation accuracies: [0.9826], loss: 0.086754\n",
      "validation accuracies: [0.986], loss: 0.069538\n",
      "epochs: 20.000000, final train loss: 0.023734, validation accuracies: [0.9846]\n",
      "best epochs: 18.604651, best_avg: 0.986000, validation accuracies: [0.986]\n",
      "saving model dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=0 at time step 4299\n",
      "calculating penultimate output...\n",
      "time taken: %f 11.987691402435303\n",
      "saving penultimate output...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=0.ckpt-4299\n",
      "Training with dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=1\n",
      "Restoring paramters from dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=0\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=0.ckpt-4299\n",
      "validation accuracies: [0.983  0.0632], loss: 4.194827\n",
      "validation accuracies: [0.9848 0.966 ], loss: 0.102804\n",
      "validation accuracies: [0.9856 0.9792], loss: 0.083417\n",
      "validation accuracies: [0.9856 0.9828], loss: 0.094559\n",
      "validation accuracies: [0.9854 0.9848], loss: 0.069969\n",
      "epochs: 20.000000, final train loss: 0.049431, validation accuracies: [0.986  0.9854]\n",
      "best epochs: 19.534884, best_avg: 0.985700, validation accuracies: [0.986  0.9854]\n",
      "saving model dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=1 at time step 4299\n",
      "calculating penultimate output...\n",
      "time taken: %f 13.050683736801147\n",
      "saving penultimate output...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=1.ckpt-4299\n",
      "Training with dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=2\n",
      "Restoring paramters from dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=1\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=1.ckpt-4299\n",
      "validation accuracies: [0.9852 0.9842 0.0992], loss: 1.993711\n",
      "validation accuracies: [0.985  0.9828 0.9632], loss: 0.085194\n",
      "validation accuracies: [0.9852 0.9836 0.9766], loss: 0.120571\n",
      "validation accuracies: [0.9856 0.9866 0.9806], loss: 0.037933\n",
      "validation accuracies: [0.9844 0.985  0.9818], loss: 0.038982\n",
      "epochs: 20.000000, final train loss: 0.051139, validation accuracies: [0.9864 0.986  0.9824]\n",
      "best epochs: 19.534884, best_avg: 0.984933, validation accuracies: [0.9864 0.986  0.9824]\n",
      "saving model dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=2 at time step 4299\n",
      "calculating penultimate output...\n",
      "time taken: %f 24.39701509475708\n",
      "saving penultimate output...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=2.ckpt-4299\n",
      "Training with dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=3\n",
      "Restoring paramters from dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=2\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=2.ckpt-4299\n",
      "validation accuracies: [0.986  0.9856 0.9804 0.0792], loss: 1.469766\n",
      "validation accuracies: [0.9856 0.9854 0.9818 0.9668], loss: 0.101705\n",
      "validation accuracies: [0.9852     0.9846     0.98180001 0.976     ], loss: 0.043930\n",
      "validation accuracies: [0.9866 0.985  0.9822 0.9808], loss: 0.039068\n",
      "validation accuracies: [0.9852 0.9842 0.9832 0.9822], loss: 0.088298\n",
      "epochs: 20.000000, final train loss: 0.050340, validation accuracies: [0.9838 0.9838 0.9832 0.9816]\n",
      "best epochs: 18.139535, best_avg: 0.984250, validation accuracies: [0.9862 0.986  0.9826 0.9822]\n",
      "saving model dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=3 at time step 4299\n",
      "calculating penultimate output...\n",
      "time taken: %f 30.03745412826538\n",
      "saving penultimate output...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=3.ckpt-4299\n",
      "Training with dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=4\n",
      "Restoring paramters from dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=3\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=3.ckpt-4299\n",
      "validation accuracies: [0.9838 0.9854 0.983  0.9828 0.1324], loss: 1.107988\n",
      "validation accuracies: [0.986  0.984  0.9842 0.9824 0.9646], loss: 0.065857\n",
      "validation accuracies: [0.985  0.9844 0.9832 0.9816 0.9746], loss: 0.066659\n",
      "validation accuracies: [0.9856 0.984  0.9832 0.9816 0.9782], loss: 0.150302\n",
      "validation accuracies: [0.9862 0.985  0.9862 0.9804 0.981 ], loss: 0.046933\n",
      "epochs: 20.000000, final train loss: 0.060670, validation accuracies: [0.9848 0.985  0.985  0.9836 0.9808]\n",
      "best epochs: 19.069767, best_avg: 0.984240, validation accuracies: [0.9864 0.9864 0.9852 0.9816 0.9816]\n",
      "saving model dropout_hidden_prob=0.75,dropout_input_prob=0.75,fisher_multiplier=0.0,learning_rate=0.0001,old:new=0.5,task=4 at time step 4299\n",
      "calculating penultimate output...\n",
      "time taken: %f 38.271750926971436\n",
      "saving penultimate output...\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a bytes object larger than 4 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d67129030e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuneTasksInRange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/continual_learning/importance_reweighting/tuner.py\u001b[0m in \u001b[0;36mtuneTasksInRange\u001b[0;34m(self, start, end, batch_size, num_hparams, num_updates, verbose, equal_weights)\u001b[0m\n\u001b[1;32m    445\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving penultimate output...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner_hparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_penultimate_output.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                                                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenultimate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaskid_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcur_best_avg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a bytes object larger than 4 GiB"
     ]
    }
   ],
   "source": [
    "tuner.tuneTasksInRange(0, t, BATCH_SIZE, num_hparams, num_updates, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, save_weights=False)\n",
    "\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuple([v for k, v in sorted(hparams[i].items())])\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9849999994277954, 0.9851999998092651, 0.9847999994277954, 0.9825999994277954, 0.9807999994277954]\n"
     ]
    }
   ],
   "source": [
    "# print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 256\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.saveResultsList()\n",
    "tuner.saveBestHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"best_avg: %e, best_params: %s\" % (best_avg, str(best_params)))\n",
    "# print(\"best_params: dropout: %f, fisher_multiplier: %e, lr: %e\" % best_params)\n",
    "\n",
    "# for k, v in tuner.results_list.items():\n",
    "# best_avg_tuple = tuple([v for k, v in sorted(tuner.best_hparams[t][0].items())])\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuple([v for k, v in sorted(best_hparams.items())])\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, len(cur_res['loss']), tuner.eval_frequency)\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, cur_best_avg_updates // tuner.eval_frequency])))\n",
    "plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "plt.plot(cur_res['val_acc'][1], color='g')\n",
    "plt.plot(cur_res['val_acc'][2], color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(t + 1):\n",
    "    print(tuner.best_hparams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatrix(tuner):\n",
    "    num_labels = 10\n",
    "    pred = np.array([])\n",
    "    actual = np.array([])\n",
    "    for j in range(t + 1):\n",
    "        val_data = tuner.task_list[j].validation\n",
    "        feed_dict = tuner.classifier.createFeedDict(val_data.images, val_data.labels)\n",
    "        cur_scores, cur_y = tuner.classifier.getPredictions(sess, feed_dict)\n",
    "        cur_pred = np.argmax(cur_scores, 1)\n",
    "        cur_actual = np.argmax(cur_y, 1)\n",
    "        actual = np.concatenate([actual, cur_actual])\n",
    "        pred = np.concatenate([pred, cur_pred])\n",
    "    confusion_matrix = np.zeros((num_labels,num_labels), dtype=np.int64)\n",
    "\n",
    "    for i in range(actual.shape[0]):\n",
    "        confusion_matrix[int(actual[i]), int(pred[i])] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def printConfusionMatrix(confusion_matrix):\n",
    "    print(\"%3d\" % (0, ), end=' ')\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        print(\"%3d\" % (j, ), end=' ')\n",
    "    print(\"\")\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        print(\"%3d\" % (i, ), end=' ')\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            print(\"%3d\" % (confusion_matrix[i][j], ), end= ' ')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = getConfusionMatrix(tuner)\n",
    "printConfusionMatrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tuner.appended_task_list[1].train.images[0].reshape(28, 28), cmap='gray')\n",
    "examples_per_class_1 = np.sum(tuner.appended_task_list[1].train.labels, axis=0).astype(np.int64)\n",
    "examples_per_class_2 = np.sum(tuner.appended_task_list[2].train.labels, axis=0).astype(np.int64)\n",
    "print(examples_per_class_1)\n",
    "print(examples_per_class_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9849, 0.9842, 0.9837, 0.9808, 0.9768]\n"
     ]
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 32\n",
    "accuracy = tuner.test(t, TEST_BATCH_SIZE, restore_model=False)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating weight for each class\n",
    "num_classes = 10\n",
    "class_weights = [0.0 for _ in range(num_classes)]\n",
    "for i in range(num_classes):\n",
    "    cur_class_indices = np.argmax(tuner.appended_task_list[t].train.labels, axis=1) == i\n",
    "    class_weights[i] += np.sum(tuner.appended_task_list[t].train.weights[cur_class_indices])\n",
    "for i in range(num_classes):\n",
    "    print(\"%6d\" % (i, ), end='')\n",
    "print(\"\")\n",
    "print(\" \", end='')\n",
    "for i in range(num_classes):\n",
    "    print(\"%2.3f\" % (class_weights[i] * 100, ), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
