{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    pass\n",
    "#     task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints_0_distill/'\n",
    "summaries_path = cur_dir + 'summaries_0_distill/'\n",
    "data_path = task_home + 'cifar-100-python/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2,
     29,
     36,
     41
    ]
   },
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "def splitDataset(dataset, dataset_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    task_list = []\n",
    "    train_labels = np.argmax(dataset.train.labels, axis=1)\n",
    "    validation_labels = np.argmax(dataset.validation.labels, axis=1)\n",
    "    test_labels = np.argmax(dataset.test.labels, axis=1)\n",
    "    for i in range(len(dataset_split)):\n",
    "        cur_train_indices = [False] * dataset.train.images.shape[0]\n",
    "        cur_validation_indices = [False] * dataset.validation.images.shape[0]\n",
    "        cur_test_indices = [False] * dataset.test.images.shape[0]\n",
    "        for j in range(len(dataset_split[i])):\n",
    "            cur_train_indices = np.logical_or(cur_train_indices, (train_labels == dataset_split[i][j]))\n",
    "            cur_validation_indices = np.logical_or(cur_validation_indices, (validation_labels == dataset_split[i][j]))\n",
    "            cur_test_indices = np.logical_or(cur_test_indices, (test_labels == dataset_split[i][j]))\n",
    "\n",
    "        task = deepcopy(dataset)\n",
    "        task.train.images = task.train.images[cur_train_indices]\n",
    "        task.train.labels = task.train.labels[cur_train_indices]\n",
    "        task.validation.images = task.validation.images[cur_validation_indices]\n",
    "        task.validation.labels = task.validation.labels[cur_validation_indices]\n",
    "        task.test.images = task.test.images[cur_test_indices]\n",
    "        task.test.labels = task.test.labels[cur_test_indices]\n",
    "        task = MyTask(task)\n",
    "        task_list.append(task)\n",
    "\n",
    "    return task_list\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "\n",
    "class TempDataset(object):\n",
    "    def __init__(self):\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "    \n",
    "class TempTask(object):\n",
    "    def __init__(self):\n",
    "        self.train = TempDataset()\n",
    "        self.validation = TempDataset()\n",
    "        self.test = TempDataset()\n",
    "    \n",
    "    \n",
    "def readDatasets():\n",
    "    num_class = 100\n",
    "    labels_list = list(range(num_class))\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels_list)\n",
    "    split = []\n",
    "    task_weights = []\n",
    "    \n",
    "    split = [labels_list[ : 91]]\n",
    "    task_weights = [0.91]\n",
    "    for single_label in labels_list[91 : ]:\n",
    "        split.append([single_label])\n",
    "        task_weights.append(0.01)\n",
    "    num_tasks = len(split)\n",
    "    \n",
    "    with open(data_path + 'train', 'rb') as f:\n",
    "        f_train_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    with open(data_path + 'test', 'rb') as f:\n",
    "        f_test_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    cifar_100 = TempTask()\n",
    "    temp_train_labels = np.array(f_train_data[b'fine_labels'], dtype=np.int32)\n",
    "    temp_test_labels = np.array(f_test_data[b'fine_labels'], dtype=np.int32)\n",
    "    f_train_data[b'fine_labels'] = np.zeros((temp_train_labels.shape[0], num_class))\n",
    "    (f_train_data[b'fine_labels'])[range(temp_train_labels.shape[0]), temp_train_labels] = 1\n",
    "    f_test_data[b'fine_labels'] = np.zeros((temp_test_labels.shape[0], num_class))\n",
    "    (f_test_data[b'fine_labels'])[range(temp_test_labels.shape[0]), temp_test_labels] = 1\n",
    "    f_train_data[b'data'] = np.reshape(f_train_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_test_data[b'data'] = np.reshape(f_test_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_train_data[b'data'] = np.transpose(f_train_data[b'data'], (0, 2, 3, 1))\n",
    "    f_test_data[b'data'] = np.transpose(f_test_data[b'data'], (0, 2, 3, 1))\n",
    "    \n",
    "    tr_data = f_train_data[b'data']\n",
    "    te_data = f_test_data[b'data']\n",
    "    # normalizing data\n",
    "    avg = np.mean(tr_data, axis=(0, 1, 2))\n",
    "    std = np.std(tr_data, axis=(0, 1, 2))\n",
    "    \n",
    "    f_train_data[b'data'] = (tr_data - avg) / std\n",
    "    f_test_data[b'data'] = (te_data - avg) / std\n",
    "    \n",
    "    seed = 0\n",
    "    np.random.seed(0)\n",
    "    shuffle_train_perm = np.random.permutation(f_train_data[b'data'].shape[0])\n",
    "    f_train_data[b'data'] = f_train_data[b'data'][shuffle_train_perm]\n",
    "    f_train_data[b'fine_labels'] = f_train_data[b'fine_labels'][shuffle_train_perm]\n",
    "    \n",
    "    num_val_per_class = 20\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        pos = (np.argmax(f_train_data[b'fine_labels'], axis=1) == i)\n",
    "        \n",
    "        if (i == 0):\n",
    "            cifar_100.validation.images = (f_train_data[b'data'][pos])[0 : num_val_per_class]\n",
    "            cifar_100.validation.labels = (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]\n",
    "\n",
    "            cifar_100.train.images = (f_train_data[b'data'][pos])[num_val_per_class : ]\n",
    "            cifar_100.train.labels = (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]\n",
    "        else:\n",
    "            cifar_100.validation.images = np.concatenate((cifar_100.validation.images, (f_train_data[b'data'][pos])[0 : num_val_per_class]))\n",
    "            cifar_100.validation.labels = np.concatenate((cifar_100.validation.labels, (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]))\n",
    "\n",
    "            cifar_100.train.images = np.concatenate((cifar_100.train.images, (f_train_data[b'data'][pos])[num_val_per_class : ]))\n",
    "            cifar_100.train.labels = np.concatenate((cifar_100.train.labels, (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]))\n",
    "        \n",
    "    cifar_100.test.images = f_test_data[b'data']\n",
    "    cifar_100.test.labels = f_test_data[b'fine_labels']\n",
    "    \n",
    "    shuffle_train_perm = np.random.permutation(cifar_100.train.images.shape[0])\n",
    "    cifar_100.train.images = cifar_100.train.images[shuffle_train_perm]\n",
    "    cifar_100.train.labels = cifar_100.train.labels[shuffle_train_perm]\n",
    "    \n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(cifar_100)\n",
    "        \n",
    "    task_list = splitDataset(cifar_100, split, seed)\n",
    "    return split, num_tasks, task_weights, task_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample images and label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'train', 'rb') as f:\n",
    "    f_train_data = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "with open(data_path + 'meta', 'rb') as f:\n",
    "    f_meta_data = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_meta_data[b'fine_label_names'][78])\n",
    "print(f_meta_data[b'fine_label_names'][61])\n",
    "print(f_meta_data[b'fine_label_names'][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tuner object and train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "output_shape = (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../../classifiers.py:102: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=False, \n",
    "                            reweigh_points_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(int(1))\n",
    "tuner.updateTunerHparams({'mask_softmax' : True})\n",
    "tuner.updateTunerHparams({'bf_num_images' : 2000})\n",
    "tuner.setPerExampleAppend(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training each task separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 128\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = 1\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tasks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [(((49, 1e-1), (63, 1e-1 / 5), 1e-1 / (5 * 5)), (1e-1, ))]\n",
    "momentums = [0.9]\n",
    "regs = [0.00001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(0, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(5.0)\n",
    "t = 1\n",
    "learning_rates = [(((45, 1e-1), (55, 1e-1 / 5), 1e-1 / 25), ((9, 1e-1 / 5), 1e-1 / 25))]\n",
    "momentums = [0.9]\n",
    "regs = [0.00001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "T = [5]\n",
    "alphas = [0.2]\n",
    "epsilons = [1.0]\n",
    "# epsilons = [0.5]\n",
    "prod = list(itertools.product(T, alphas, regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates, \n",
    "                                epsilons))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['T'] = hparams_tuple[0]\n",
    "    cur_dict['alpha'] = hparams_tuple[1]\n",
    "    cur_dict['reg'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[4]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[4]\n",
    "    cur_dict['momentum'] = hparams_tuple[5]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[6]\n",
    "    cur_dict['epsilon'] = hparams_tuple[7]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(1, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "\n",
    "for i in range(0, 1):\n",
    "    for _ in range(len(hparams)):\n",
    "        tuner.hparams_list[i].append(tuner.hparams_list[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(hparams)\n",
    "num_epochs = 60\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "num_epochs_bf = 10\n",
    "num_updates_bf = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints_0_distill/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-23940\n",
      "Training with T=5,alpha=0.2,dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=1.0,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=5.0,task=1\n",
      "Restoring paramters from dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints_0_distill/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-23940\n",
      "epoch: 1, iter: 0/4, validation accuracies: [0.22802198 0.69999999], average train loss: 26.525276, average train accuracy: 0.281250\n",
      "epoch: 2, iter: 0/4, validation accuracies: [0.19285714 0.60000002], average train loss: 23.973391, average train accuracy: 0.355469\n",
      "epoch: 3, iter: 0/4, validation accuracies: [0.18186813 0.        ], average train loss: 22.558620, average train accuracy: 0.302734\n",
      "epoch: 4, iter: 0/4, validation accuracies: [0.18626374 0.5       ], average train loss: 22.185345, average train accuracy: 0.320312\n",
      "epoch: 5, iter: 0/4, validation accuracies: [0.18681319 0.89999998], average train loss: 21.719738, average train accuracy: 0.339844\n",
      "epoch: 6, iter: 0/4, validation accuracies: [0.20659341 0.69999999], average train loss: 21.520271, average train accuracy: 0.390625\n",
      "epoch: 7, iter: 0/4, validation accuracies: [0.22362637 0.25      ], average train loss: 21.143944, average train accuracy: 0.333984\n",
      "epoch: 8, iter: 0/4, validation accuracies: [0.23681319 0.64999998], average train loss: 21.154121, average train accuracy: 0.359375\n",
      "epoch: 9, iter: 0/4, validation accuracies: [0.23846154 0.64999998], average train loss: 21.051544, average train accuracy: 0.404297\n",
      "epoch: 10, iter: 0/4, validation accuracies: [0.23461539 0.55000001], average train loss: 21.048534, average train accuracy: 0.392578\n",
      "epoch: 11, iter: 0/4, validation accuracies: [0.2445055  0.69999999], average train loss: 20.270927, average train accuracy: 0.427734\n",
      "epoch: 12, iter: 0/4, validation accuracies: [0.25769231 0.69999999], average train loss: 20.234228, average train accuracy: 0.419922\n",
      "epoch: 13, iter: 0/4, validation accuracies: [0.29340659 0.55000001], average train loss: 19.903299, average train accuracy: 0.421875\n",
      "epoch: 14, iter: 0/4, validation accuracies: [0.29725275 0.64999998], average train loss: 20.392830, average train accuracy: 0.404297\n",
      "epoch: 15, iter: 0/4, validation accuracies: [0.29560439 0.69999999], average train loss: 20.160900, average train accuracy: 0.429688\n",
      "epoch: 16, iter: 0/4, validation accuracies: [0.29120879 0.60000002], average train loss: 20.086172, average train accuracy: 0.443359\n",
      "epoch: 17, iter: 0/4, validation accuracies: [0.29340659 0.5       ], average train loss: 19.753353, average train accuracy: 0.425781\n",
      "epoch: 18, iter: 0/4, validation accuracies: [0.28846154 0.75      ], average train loss: 19.826511, average train accuracy: 0.431641\n",
      "epoch: 19, iter: 0/4, validation accuracies: [0.3054945 0.75     ], average train loss: 19.946690, average train accuracy: 0.449219\n",
      "epoch: 20, iter: 0/4, validation accuracies: [0.30824175 0.60000002], average train loss: 19.406542, average train accuracy: 0.496094\n",
      "epoch: 21, iter: 0/4, validation accuracies: [0.30659341 0.60000002], average train loss: 19.532810, average train accuracy: 0.457031\n",
      "epoch: 22, iter: 0/4, validation accuracies: [0.30714286 0.69999999], average train loss: 19.658039, average train accuracy: 0.460938\n",
      "epoch: 23, iter: 0/4, validation accuracies: [0.2967033  0.60000002], average train loss: 19.789707, average train accuracy: 0.460938\n",
      "epoch: 24, iter: 0/4, validation accuracies: [0.31648352 0.64999998], average train loss: 19.967432, average train accuracy: 0.447266\n",
      "epoch: 25, iter: 0/4, validation accuracies: [0.29725274 0.60000002], average train loss: 19.247417, average train accuracy: 0.468750\n",
      "epoch: 26, iter: 0/4, validation accuracies: [0.29230769 0.75      ], average train loss: 19.819674, average train accuracy: 0.455078\n",
      "epoch: 27, iter: 0/4, validation accuracies: [0.3  0.75], average train loss: 19.357147, average train accuracy: 0.476562\n",
      "epoch: 28, iter: 0/4, validation accuracies: [0.31153847 0.5       ], average train loss: 19.358486, average train accuracy: 0.441406\n",
      "epoch: 29, iter: 0/4, validation accuracies: [0.31758242 0.75      ], average train loss: 19.748798, average train accuracy: 0.478516\n",
      "epoch: 30, iter: 0/4, validation accuracies: [0.31813187 0.60000002], average train loss: 19.628883, average train accuracy: 0.484375\n",
      "epoch: 31, iter: 0/4, validation accuracies: [0.33736263 0.69999999], average train loss: 19.711216, average train accuracy: 0.472656\n",
      "epoch: 32, iter: 0/4, validation accuracies: [0.33461538 0.75      ], average train loss: 19.501444, average train accuracy: 0.460938\n",
      "epoch: 33, iter: 0/4, validation accuracies: [0.33076924 0.75      ], average train loss: 19.004349, average train accuracy: 0.488281\n",
      "epoch: 34, iter: 0/4, validation accuracies: [0.32527472 0.75      ], average train loss: 19.066288, average train accuracy: 0.460938\n",
      "epoch: 35, iter: 0/4, validation accuracies: [0.32362637 0.85000002], average train loss: 19.618530, average train accuracy: 0.496094\n",
      "epoch: 36, iter: 0/4, validation accuracies: [0.34065934 0.69999999], average train loss: 18.850016, average train accuracy: 0.521484\n",
      "epoch: 37, iter: 0/4, validation accuracies: [0.34340659 0.75      ], average train loss: 19.081198, average train accuracy: 0.542969\n",
      "epoch: 38, iter: 0/4, validation accuracies: [0.34615384 0.69999999], average train loss: 18.843777, average train accuracy: 0.515625\n",
      "epoch: 39, iter: 0/4, validation accuracies: [0.34285714 0.69999999], average train loss: 19.452286, average train accuracy: 0.505859\n",
      "epoch: 40, iter: 0/4, validation accuracies: [0.33626373 0.60000002], average train loss: 19.101215, average train accuracy: 0.501953\n",
      "epoch: 41, iter: 0/4, validation accuracies: [0.33736264 0.80000001], average train loss: 19.241806, average train accuracy: 0.496094\n",
      "epoch: 42, iter: 0/4, validation accuracies: [0.33351649 0.80000001], average train loss: 19.292572, average train accuracy: 0.529297\n",
      "epoch: 43, iter: 0/4, validation accuracies: [0.35054945 0.69999999], average train loss: 19.084663, average train accuracy: 0.480469\n",
      "epoch: 44, iter: 0/4, validation accuracies: [0.35769231 0.64999998], average train loss: 18.792702, average train accuracy: 0.511719\n",
      "epoch: 45, iter: 0/4, validation accuracies: [0.35824175 0.55000001], average train loss: 18.960348, average train accuracy: 0.525391\n",
      "epoch: 46, iter: 0/4, validation accuracies: [0.36098901 0.60000002], average train loss: 18.736513, average train accuracy: 0.558594\n",
      "epoch: 47, iter: 0/4, validation accuracies: [0.37362637 0.64999998], average train loss: 19.113644, average train accuracy: 0.552734\n",
      "epoch: 48, iter: 0/4, validation accuracies: [0.37582417 0.69999999], average train loss: 19.187077, average train accuracy: 0.466797\n",
      "epoch: 49, iter: 0/4, validation accuracies: [0.37582418 0.69999999], average train loss: 18.637230, average train accuracy: 0.564453\n",
      "epoch: 50, iter: 0/4, validation accuracies: [0.37197802 0.69999999], average train loss: 18.580641, average train accuracy: 0.529297\n",
      "epoch: 51, iter: 0/4, validation accuracies: [0.37857143 0.69999999], average train loss: 18.874146, average train accuracy: 0.523438\n",
      "epoch: 52, iter: 0/4, validation accuracies: [0.37912088 0.69999999], average train loss: 18.673719, average train accuracy: 0.535156\n",
      "epoch: 53, iter: 0/4, validation accuracies: [0.37417582 0.69999999], average train loss: 18.684645, average train accuracy: 0.509766\n",
      "epoch: 54, iter: 0/4, validation accuracies: [0.37967033 0.69999999], average train loss: 18.803011, average train accuracy: 0.550781\n",
      "epoch: 55, iter: 0/4, validation accuracies: [0.38406594 0.64999998], average train loss: 18.999630, average train accuracy: 0.515625\n",
      "epoch: 56, iter: 0/4, validation accuracies: [0.38406594 0.64999998], average train loss: 18.750858, average train accuracy: 0.531250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 57, iter: 0/4, validation accuracies: [0.38681319 0.64999998], average train loss: 18.964481, average train accuracy: 0.521484\n",
      "epoch: 58, iter: 0/4, validation accuracies: [0.38846153 0.64999998], average train loss: 18.786396, average train accuracy: 0.523438\n",
      "epoch: 59, iter: 0/4, validation accuracies: [0.38736264 0.64999998], average train loss: 18.629047, average train accuracy: 0.558594\n",
      "epoch: 60, iter: 0/4, validation accuracies: [0.39120879 0.64999998], average train loss: 18.758699, average train accuracy: 0.533203\n",
      "epochs: 60.000000, final train loss: 19.232613, validation accuracies: [0.39120879 0.64999998]\n",
      "best epochs: 60.000000, best_avg: 0.394022, validation accuracies: [0.39120879 0.64999998]\n",
      "Training with T=5,alpha=0.2,dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=1.0,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=5.0,task=1\n",
      "epoch: 1, iter: 0/4, validation accuracies: [0.3967033  0.64999998], average train loss: 2.229209, average train accuracy: 0.488281\n",
      "epoch: 2, iter: 0/4, validation accuracies: [0.41813187 0.55000001], average train loss: 2.099282, average train accuracy: 0.490234\n",
      "epoch: 3, iter: 0/4, validation accuracies: [0.43626373 0.40000001], average train loss: 1.753493, average train accuracy: 0.531250\n",
      "epoch: 4, iter: 0/4, validation accuracies: [0.45164835 0.30000001], average train loss: 1.597267, average train accuracy: 0.556641\n",
      "epoch: 5, iter: 0/4, validation accuracies: [0.45934066 0.25      ], average train loss: 1.535954, average train accuracy: 0.593750\n",
      "epoch: 6, iter: 0/4, validation accuracies: [0.47692308 0.25      ], average train loss: 1.524424, average train accuracy: 0.611328\n",
      "epoch: 7, iter: 0/4, validation accuracies: [0.48406593 0.30000001], average train loss: 1.491986, average train accuracy: 0.623047\n",
      "epoch: 8, iter: 0/4, validation accuracies: [0.48901098 0.34999999], average train loss: 1.415380, average train accuracy: 0.621094\n",
      "epoch: 9, iter: 0/4, validation accuracies: [0.48901098 0.34999999], average train loss: 1.204513, average train accuracy: 0.677734\n",
      "epoch: 10, iter: 0/4, validation accuracies: [0.48846155 0.40000001], average train loss: 1.183593, average train accuracy: 0.681641\n",
      "epochs: 10.000000, final train loss: 1.341468, validation accuracies: [0.48846155 0.40000001]\n",
      "best epochs: 10.000000, best_avg: 0.487500, validation accuracies: [0.48846155 0.40000001]\n",
      "saving model T=5,alpha=0.2,dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=1.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=5.0,task=1 at time step 280\n",
      "calculating penultimate output...\n",
      "time taken: %f 7.174915790557861\n",
      "saving penultimate output...\n"
     ]
    }
   ],
   "source": [
    "best_avg, best_hparams_index, test_acc = tuner.tuneTasksInRange(1, t, BATCH_SIZE, num_hparams, \n",
    "                                                        num_updates=num_updates, verbose=True, \n",
    "                                                        random_crop_flip=True, \n",
    "                                                        is_sampling_reweighing=True, \n",
    "                                                        do_bf_finetuning=True, num_updates_bf=num_updates_bf, \n",
    "                                                        bf_only_penultimate_train=False, \n",
    "                                                        sigma=5.0, \n",
    "                                                        eval_test_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5071428571428571, 0.550000011920929]\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(test_acc)):\n",
    "    print(test_acc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(test_acc)):\n",
    "    print(test_acc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_old_examples = tuner.appended_task_list[t].train.images.shape[0] - tuner.task_list[t].train.images.shape[0]\n",
    "# print(np.sum(tuner.appended_task_list[t].train.old_task_equalized_weights[ : num_old_examples] != tuner.appended_task_list[t].train.old_task_equalized_weights[ : num_old_examples][0]))\n",
    "# print(tuner.appended_task_list[t].train.old_task_equalized_weights[num_old_examples : ])\n",
    "num_examples_per_class = 480\n",
    "print(np.sum(tuner.appended_task_list[t].train.old_task_equalized_weights[num_old_examples : ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[best_hparams_index], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res[0]['total_updates'] + cur_res[1]['total_updates'], cur_res[0]['updates_per_epoch'])\n",
    "cur_best_avg = cur_res[1]['best_avg']\n",
    "cur_best_epoch = cur_res[1]['best_epoch']\n",
    "updates_per_epoch = cur_res[1]['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_epoch))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res[1]['val_acc'])[:, (cur_best_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "# plt.plot(np.concatenate((cur_res['loss'][0][-1], cur_res['loss'][1][-1]), axis=0), color='b')\n",
    "# plt.plot(np.concatenate((cur_res[0]['val_loss'][-1], cur_res[1]['val_loss'][-1]), axis=0), color='b')\n",
    "# plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(np.concatenate((cur_res[0]['val_acc'][-1], cur_res[1]['val_acc'][-1]), axis=0), color='b')\n",
    "plt.plot(np.concatenate((cur_res[0]['val_acc'][1], cur_res[1]['val_acc'][1]), axis=0), color='g')\n",
    "plt.plot(np.concatenate((cur_res[0]['val_acc'][0], cur_res[1]['val_acc'][0]), axis=0), color='r')\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllCosineSimilarity(wts, init_classes, only_dot=False):\n",
    "    num_init_class = len(init_classes)\n",
    "    cosine_wts = [[0.0 for _ in range(num_init_class)] for _ in range(num_init_class)]\n",
    "    for i in range(num_init_class):\n",
    "        for j in range(num_init_class):\n",
    "            w_i = wts[0][:, init_classes[i]]\n",
    "            w_j = wts[0][:, init_classes[j]]\n",
    "            if (only_dot):\n",
    "                cosine_wts[i][j] = np.sum(w_i * w_j)\n",
    "            else:\n",
    "                cosine_wts[i][j] = np.sum(w_i * w_j) / np.sqrt(np.sum(w_i ** 2)) / np.sqrt(np.sum(w_j ** 2))\n",
    "    return cosine_wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(1.0)\n",
    "num_tasks_test = 10\n",
    "wts = [None for _ in range(num_tasks_test)]\n",
    "init_classes = tuner.split[0]\n",
    "cosine_sim_wts = [None for _ in range(num_tasks_test)]\n",
    "dot_sim_wts = [None for _ in range(num_tasks_test)]\n",
    "for i in range(num_tasks_test):\n",
    "    tuner.test(i, BATCH_SIZE, restore_model=True, hparams=tuner.hparams_list[i][0])\n",
    "    tuner.setPerExampleAppend(5.0)\n",
    "    wts[i] = sess.run([v for v in tf.all_variables() if 'dense' in v.name and 'kernel:0' in v.name])\n",
    "    cosine_sim_wts[i] = np.array(getAllCosineSimilarity(wts[i], init_classes, only_dot=False))\n",
    "    dot_sim_wts[i] = np.array(getAllCosineSimilarity(wts[i], init_classes, only_dot=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks_test):\n",
    "#     plt.imshow(cosine_sim_wts[i])\n",
    "    print(\"task\", i)\n",
    "    print(\"sum cosine: \", np.sum(cosine_sim_wts[i]) - num_init_class)\n",
    "    print(\"sum dot: \", np.sum(dot_sim_wts[i]) - np.sum(wts[i][0][: , init_classes] ** 2))\n",
    "    print(\"sum norms: \", np.sum(np.sqrt(np.sum(wts[i][0][:, init_classes] ** 2, axis=0))))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_class = 90\n",
    "        \n",
    "num_init_class = 90\n",
    "final_cosine_wts = [[0.0 for _ in range(num_init_class)] for _ in range(num_init_class)]\n",
    "for i in range(num_init_class):\n",
    "    for j in range(num_init_class):\n",
    "        w_i = final_weights[0][:, i]\n",
    "        w_j = final_weights[0][:, j]\n",
    "        final_cosine_wts[i][j] = np.sum(w_i * w_j) / np.sqrt(np.sum(w_i ** 2)) / np.sqrt(np.sum(w_j ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cosine_wts = np.array(init_cosine_wts)\n",
    "final_cosine_wts = np.array(final_cosine_wts)\n",
    "\n",
    "print(init_cosine_wts[0, 0])\n",
    "print(np.sum(init_cosine_wts))\n",
    "print(np.sum(final_cosine_wts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "TEST_BATCH_SIZE = 128\n",
    "test_accuracies = []\n",
    "for hparam in hparams:\n",
    "    accuracy = tuner.test(t, BATCH_SIZE, restore_model=True, hparams=hparam)\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(hparam['epsilon'], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weights given to each class\n",
    "num_class = 100\n",
    "for i in range(num_class):\n",
    "    print(i, np.sum(tuner.appended_task_list[-1].train.weights[(np.argmax(tuner.appended_task_list[-1].train.labels, axis=1) == i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = tuner.appended_task_list[1].train.weights[np.argmax(tuner.appended_task_list[1].train.labels, axis=1) != 99]\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "# plt.hist(old_weights)\n",
    "# seaborn.distplot(old_weights)\n",
    "print(old_weights.shape)\n",
    "print(np.sum(old_weights < 1e-5))\n",
    "print(np.sum((old_weights >= 1e-5) * (old_weights < 1e-2)))\n",
    "# for i in range(1000):\n",
    "#     print(w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.saveResultsList()\n",
    "tuner.saveBestHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "test_accuracies = []\n",
    "for i in range(t + 1):\n",
    "    accuracy = tuner.test(i, TEST_BATCH_SIZE, restore_model=True)\n",
    "    test_accuracies.append(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracies)\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r DB1_test_accuracies\n",
    "plt.plot(test_accuracies, color='g')\n",
    "plt.plot(DB1_test_accuracies, color='b')\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tuner.appended_task_list[-1].train.weights[0 : tuner.appended_task_list[-2].train.weights.shape[0]], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "filename='code_state.bak'\n",
    "my_shelf = shelve.open(filename,'n') # 'n' for new\n",
    "\n",
    "for key in dir():\n",
    "    try:\n",
    "        my_shelf[key] = globals()[key]\n",
    "    except TypeError:\n",
    "        #\n",
    "        # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "        #\n",
    "        print('ERROR shelving: {0}'.format(key))\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
