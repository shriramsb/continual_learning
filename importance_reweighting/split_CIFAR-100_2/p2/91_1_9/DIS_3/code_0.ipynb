{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    pass\n",
    "#     task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints_0_no_distill/'\n",
    "summaries_path = cur_dir + 'summaries_0_no_distill/'\n",
    "data_path = task_home + 'cifar-100-python/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     2,
     29,
     36,
     41
    ]
   },
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "def splitDataset(dataset, dataset_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    task_list = []\n",
    "    train_labels = np.argmax(dataset.train.labels, axis=1)\n",
    "    validation_labels = np.argmax(dataset.validation.labels, axis=1)\n",
    "    test_labels = np.argmax(dataset.test.labels, axis=1)\n",
    "    for i in range(len(dataset_split)):\n",
    "        cur_train_indices = [False] * dataset.train.images.shape[0]\n",
    "        cur_validation_indices = [False] * dataset.validation.images.shape[0]\n",
    "        cur_test_indices = [False] * dataset.test.images.shape[0]\n",
    "        for j in range(len(dataset_split[i])):\n",
    "            cur_train_indices = np.logical_or(cur_train_indices, (train_labels == dataset_split[i][j]))\n",
    "            cur_validation_indices = np.logical_or(cur_validation_indices, (validation_labels == dataset_split[i][j]))\n",
    "            cur_test_indices = np.logical_or(cur_test_indices, (test_labels == dataset_split[i][j]))\n",
    "\n",
    "        task = deepcopy(dataset)\n",
    "        task.train.images = task.train.images[cur_train_indices]\n",
    "        task.train.labels = task.train.labels[cur_train_indices]\n",
    "        task.validation.images = task.validation.images[cur_validation_indices]\n",
    "        task.validation.labels = task.validation.labels[cur_validation_indices]\n",
    "        task.test.images = task.test.images[cur_test_indices]\n",
    "        task.test.labels = task.test.labels[cur_test_indices]\n",
    "        task = MyTask(task)\n",
    "        task_list.append(task)\n",
    "\n",
    "    return task_list\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "\n",
    "class TempDataset(object):\n",
    "    def __init__(self):\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "    \n",
    "class TempTask(object):\n",
    "    def __init__(self):\n",
    "        self.train = TempDataset()\n",
    "        self.validation = TempDataset()\n",
    "        self.test = TempDataset()\n",
    "    \n",
    "    \n",
    "def readDatasets():\n",
    "    num_class = 100\n",
    "    labels_list = list(range(num_class))\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels_list)\n",
    "    split = []\n",
    "    task_weights = []\n",
    "    \n",
    "    split = [labels_list[ : 91]]\n",
    "    task_weights = [0.91]\n",
    "    for single_label in labels_list[91 : ]:\n",
    "        split.append([single_label])\n",
    "        task_weights.append(0.01)\n",
    "    num_tasks = len(split)\n",
    "    \n",
    "    with open(data_path + 'train', 'rb') as f:\n",
    "        f_train_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    with open(data_path + 'test', 'rb') as f:\n",
    "        f_test_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    cifar_100 = TempTask()\n",
    "    temp_train_labels = np.array(f_train_data[b'fine_labels'], dtype=np.int32)\n",
    "    temp_test_labels = np.array(f_test_data[b'fine_labels'], dtype=np.int32)\n",
    "    f_train_data[b'fine_labels'] = np.zeros((temp_train_labels.shape[0], num_class))\n",
    "    (f_train_data[b'fine_labels'])[range(temp_train_labels.shape[0]), temp_train_labels] = 1\n",
    "    f_test_data[b'fine_labels'] = np.zeros((temp_test_labels.shape[0], num_class))\n",
    "    (f_test_data[b'fine_labels'])[range(temp_test_labels.shape[0]), temp_test_labels] = 1\n",
    "    f_train_data[b'data'] = np.reshape(f_train_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_test_data[b'data'] = np.reshape(f_test_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_train_data[b'data'] = np.transpose(f_train_data[b'data'], (0, 2, 3, 1))\n",
    "    f_test_data[b'data'] = np.transpose(f_test_data[b'data'], (0, 2, 3, 1))\n",
    "    \n",
    "    tr_data = f_train_data[b'data']\n",
    "    te_data = f_test_data[b'data']\n",
    "    # normalizing data\n",
    "    avg = np.mean(tr_data, axis=(0, 1, 2))\n",
    "    std = np.std(tr_data, axis=(0, 1, 2))\n",
    "    \n",
    "    f_train_data[b'data'] = (tr_data - avg) / std\n",
    "    f_test_data[b'data'] = (te_data - avg) / std\n",
    "    \n",
    "    seed = 0\n",
    "    np.random.seed(0)\n",
    "    shuffle_train_perm = np.random.permutation(f_train_data[b'data'].shape[0])\n",
    "    f_train_data[b'data'] = f_train_data[b'data'][shuffle_train_perm]\n",
    "    f_train_data[b'fine_labels'] = f_train_data[b'fine_labels'][shuffle_train_perm]\n",
    "    \n",
    "    num_val_per_class = 20\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        pos = (np.argmax(f_train_data[b'fine_labels'], axis=1) == i)\n",
    "        \n",
    "        if (i == 0):\n",
    "            cifar_100.validation.images = (f_train_data[b'data'][pos])[0 : num_val_per_class]\n",
    "            cifar_100.validation.labels = (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]\n",
    "\n",
    "            cifar_100.train.images = (f_train_data[b'data'][pos])[num_val_per_class : ]\n",
    "            cifar_100.train.labels = (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]\n",
    "        else:\n",
    "            cifar_100.validation.images = np.concatenate((cifar_100.validation.images, (f_train_data[b'data'][pos])[0 : num_val_per_class]))\n",
    "            cifar_100.validation.labels = np.concatenate((cifar_100.validation.labels, (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]))\n",
    "\n",
    "            cifar_100.train.images = np.concatenate((cifar_100.train.images, (f_train_data[b'data'][pos])[num_val_per_class : ]))\n",
    "            cifar_100.train.labels = np.concatenate((cifar_100.train.labels, (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]))\n",
    "        \n",
    "    cifar_100.test.images = f_test_data[b'data']\n",
    "    cifar_100.test.labels = f_test_data[b'fine_labels']\n",
    "    \n",
    "    shuffle_train_perm = np.random.permutation(cifar_100.train.images.shape[0])\n",
    "    cifar_100.train.images = cifar_100.train.images[shuffle_train_perm]\n",
    "    cifar_100.train.labels = cifar_100.train.labels[shuffle_train_perm]\n",
    "    \n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(cifar_100)\n",
    "        \n",
    "    task_list = splitDataset(cifar_100, split, seed)\n",
    "    return split, num_tasks, task_weights, task_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample images and label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'train', 'rb') as f:\n",
    "    f_train_data = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "with open(data_path + 'meta', 'rb') as f:\n",
    "    f_meta_data = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_meta_data[b'fine_label_names'][78])\n",
    "print(f_meta_data[b'fine_label_names'][61])\n",
    "print(f_meta_data[b'fine_label_names'][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tuner object and train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "output_shape = (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../../classifiers.py:102: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=False, \n",
    "                            reweigh_points_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(int(1))\n",
    "tuner.updateTunerHparams({'mask_softmax' : True})\n",
    "tuner.updateTunerHparams({'bf_num_images' : 2000})\n",
    "tuner.setPerExampleAppend(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training each task separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 128\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = 1\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tasks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_layers/conv2d/kernel:0\n",
      "resnet_layers/conv2d_1/kernel:0\n",
      "resnet_layers/conv2d_2/kernel:0\n",
      "resnet_layers/conv2d_3/kernel:0\n",
      "resnet_layers/conv2d_4/kernel:0\n",
      "resnet_layers/conv2d_5/kernel:0\n",
      "resnet_layers/conv2d_6/kernel:0\n",
      "resnet_layers/conv2d_7/kernel:0\n",
      "resnet_layers/conv2d_8/kernel:0\n",
      "resnet_layers/conv2d_9/kernel:0\n",
      "resnet_layers/conv2d_10/kernel:0\n",
      "resnet_layers/conv2d_11/kernel:0\n",
      "resnet_layers/conv2d_12/kernel:0\n",
      "resnet_layers/conv2d_13/kernel:0\n",
      "resnet_layers/conv2d_14/kernel:0\n",
      "resnet_layers/conv2d_15/kernel:0\n",
      "resnet_layers/conv2d_16/kernel:0\n",
      "resnet_layers/conv2d_17/kernel:0\n",
      "resnet_layers/conv2d_18/kernel:0\n",
      "resnet_layers/conv2d_19/kernel:0\n",
      "resnet_layers/conv2d_20/kernel:0\n",
      "resnet_layers/conv2d_21/kernel:0\n",
      "resnet_layers/conv2d_22/kernel:0\n",
      "resnet_layers/conv2d_23/kernel:0\n",
      "resnet_layers/conv2d_24/kernel:0\n",
      "resnet_layers/conv2d_25/kernel:0\n",
      "resnet_layers/conv2d_26/kernel:0\n",
      "resnet_layers/conv2d_27/kernel:0\n",
      "resnet_layers/conv2d_28/kernel:0\n",
      "resnet_layers/conv2d_29/kernel:0\n",
      "resnet_layers/conv2d_30/kernel:0\n",
      "resnet_layers/conv2d_31/kernel:0\n",
      "resnet_layers/conv2d_32/kernel:0\n",
      "resnet_layers/dense/kernel:0\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    if ('bias' not in v.name and 'batch' not in v.name):\n",
    "        print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [(((49, 1e-1), (63, 1e-1 / 5), 1e-1 / (5 * 5)), (1e-1, ))]\n",
    "momentums = [0.9]\n",
    "regs = [0.00001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(0, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(20.0)\n",
    "t = 1\n",
    "learning_rates = [(((45, 1e-10), (55, 1e-10 / 5), 1e-10 / 25), ((9, 1e-10), 1e-10 / 5))]\n",
    "momentums = [0.9]\n",
    "regs = [0.00001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "epsilons = [1.0]\n",
    "# epsilons = [0.5]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates, \n",
    "                                epsilons))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    cur_dict['epsilon'] = hparams_tuple[5]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(1, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "\n",
    "for i in range(0, 1):\n",
    "    for _ in range(len(hparams)):\n",
    "        tuner.hparams_list[i].append(tuner.hparams_list[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(hparams)\n",
    "num_epochs = 1\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "num_epochs_bf = 1\n",
    "num_updates_bf = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints_0_no_distill/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-23940\n",
      "Training with T=None,alpha=0.0,dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=1.0,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=20.0,task=1\n",
      "Restoring paramters from dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints_0_no_distill/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-23940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/mnt/a99/d0/shriramsb/tf_venv/.env/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0/4, validation accuracies: [0.01428571 0.        ], average train loss: nan, average train accuracy: nan\n",
      "epoch: 1, iter: 0/4, validation accuracies: [0.01428571 0.        ], average train loss: 22.345795, average train accuracy: 0.017578\n",
      "epochs: 1.000000, final train loss: 23.415829, validation accuracies: [0.01428571 0.        ]\n",
      "best epochs: 0.000000, best_avg: 0.014130, validation accuracies: [0.01428571 0.        ]\n",
      "Training with T=None,alpha=0.0,dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=1.0,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=20.0,task=1\n",
      "epoch: 0, iter: 0/4, validation accuracies: [0.01428571 0.        ], average train loss: nan, average train accuracy: nan\n",
      "epoch: 1, iter: 0/4, validation accuracies: [0.01428571 0.        ], average train loss: 21.448380, average train accuracy: 0.011719\n",
      "epochs: 1.000000, final train loss: 21.263065, validation accuracies: [0.01428571 0.        ]\n",
      "best epochs: 0.000000, best_avg: 0.014130, validation accuracies: [0.01428571 0.        ]\n",
      "saving model dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=1.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=20.0,task=1 at time step 8\n",
      "calculating penultimate output...\n",
      "time taken: %f 3.1671314239501953\n",
      "saving penultimate output...\n"
     ]
    }
   ],
   "source": [
    "best_avg, best_hparams_index, test_acc = tuner.tuneTasksInRange(1, t, BATCH_SIZE, num_hparams, \n",
    "                                                        num_updates=num_updates, verbose=True, \n",
    "                                                        random_crop_flip=True, \n",
    "                                                        is_sampling_reweighing=True, \n",
    "                                                        do_bf_finetuning=True, num_updates_bf=num_updates_bf, \n",
    "                                                        bf_only_penultimate_train=False, \n",
    "                                                        sigma=5.0, \n",
    "                                                        eval_test_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2845054945054945, 0.4399999976158142]\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(test_acc)):\n",
    "    print(test_acc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints_0_no_distill/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-23940\n",
      "count new class: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6587912086602096]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.setPerExampleAppend(1.0)\n",
    "tuner.validationAccuracy(0, BATCH_SIZE, hparams=tuner.hparams_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(test_acc)):\n",
    "    print(test_acc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_old_examples = tuner.appended_task_list[t].train.images.shape[0] - tuner.task_list[t].train.images.shape[0]\n",
    "# print(np.sum(tuner.appended_task_list[t].train.old_task_equalized_weights[ : num_old_examples] != tuner.appended_task_list[t].train.old_task_equalized_weights[ : num_old_examples][0]))\n",
    "# print(tuner.appended_task_list[t].train.old_task_equalized_weights[num_old_examples : ])\n",
    "num_examples_per_class = 480\n",
    "print(np.sum(tuner.appended_task_list[t].train.old_task_equalized_weights[num_old_examples : ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[best_hparams_index], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res[0]['total_updates'] + cur_res[1]['total_updates'], cur_res[0]['updates_per_epoch'])\n",
    "cur_best_avg = cur_res[1]['best_avg']\n",
    "cur_best_epoch = cur_res[1]['best_epoch']\n",
    "updates_per_epoch = cur_res[1]['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_epoch))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res[1]['val_acc'])[:, (cur_best_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "# plt.plot(np.concatenate((cur_res['loss'][0][-1], cur_res['loss'][1][-1]), axis=0), color='b')\n",
    "# plt.plot(np.concatenate((cur_res[0]['val_loss'][-1], cur_res[1]['val_loss'][-1]), axis=0), color='b')\n",
    "# plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(np.concatenate((cur_res[0]['val_acc'][-1], cur_res[1]['val_acc'][-1]), axis=0), color='b')\n",
    "plt.plot(np.concatenate((cur_res[0]['val_acc'][1], cur_res[1]['val_acc'][1]), axis=0), color='g')\n",
    "plt.plot(np.concatenate((cur_res[0]['val_acc'][0], cur_res[1]['val_acc'][0]), axis=0), color='r')\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllCosineSimilarity(wts, init_classes, only_dot=False):\n",
    "    num_init_class = len(init_classes)\n",
    "    cosine_wts = [[0.0 for _ in range(num_init_class)] for _ in range(num_init_class)]\n",
    "    for i in range(num_init_class):\n",
    "        for j in range(num_init_class):\n",
    "            w_i = wts[0][:, init_classes[i]]\n",
    "            w_j = wts[0][:, init_classes[j]]\n",
    "            if (only_dot):\n",
    "                cosine_wts[i][j] = np.sum(w_i * w_j)\n",
    "            else:\n",
    "                cosine_wts[i][j] = np.sum(w_i * w_j) / np.sqrt(np.sum(w_i ** 2)) / np.sqrt(np.sum(w_j ** 2))\n",
    "    return cosine_wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(1.0)\n",
    "num_tasks_test = 10\n",
    "wts = [None for _ in range(num_tasks_test)]\n",
    "init_classes = tuner.split[0]\n",
    "cosine_sim_wts = [None for _ in range(num_tasks_test)]\n",
    "dot_sim_wts = [None for _ in range(num_tasks_test)]\n",
    "for i in range(num_tasks_test):\n",
    "    tuner.test(i, BATCH_SIZE, restore_model=True, hparams=tuner.hparams_list[i][0])\n",
    "    tuner.setPerExampleAppend(5.0)\n",
    "    wts[i] = sess.run([v for v in tf.all_variables() if 'dense' in v.name and 'kernel:0' in v.name])\n",
    "    cosine_sim_wts[i] = np.array(getAllCosineSimilarity(wts[i], init_classes, only_dot=False))\n",
    "    dot_sim_wts[i] = np.array(getAllCosineSimilarity(wts[i], init_classes, only_dot=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks_test):\n",
    "#     plt.imshow(cosine_sim_wts[i])\n",
    "    print(\"task\", i)\n",
    "    print(\"sum cosine: \", np.sum(cosine_sim_wts[i]) - num_init_class)\n",
    "    print(\"sum dot: \", np.sum(dot_sim_wts[i]) - np.sum(wts[i][0][: , init_classes] ** 2))\n",
    "    print(\"sum norms: \", np.sum(np.sqrt(np.sum(wts[i][0][:, init_classes] ** 2, axis=0))))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_class = 90\n",
    "        \n",
    "num_init_class = 90\n",
    "final_cosine_wts = [[0.0 for _ in range(num_init_class)] for _ in range(num_init_class)]\n",
    "for i in range(num_init_class):\n",
    "    for j in range(num_init_class):\n",
    "        w_i = final_weights[0][:, i]\n",
    "        w_j = final_weights[0][:, j]\n",
    "        final_cosine_wts[i][j] = np.sum(w_i * w_j) / np.sqrt(np.sum(w_i ** 2)) / np.sqrt(np.sum(w_j ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cosine_wts = np.array(init_cosine_wts)\n",
    "final_cosine_wts = np.array(final_cosine_wts)\n",
    "\n",
    "print(init_cosine_wts[0, 0])\n",
    "print(np.sum(init_cosine_wts))\n",
    "print(np.sum(final_cosine_wts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "TEST_BATCH_SIZE = 128\n",
    "test_accuracies = []\n",
    "for hparam in hparams:\n",
    "    accuracy = tuner.test(t, BATCH_SIZE, restore_model=True, hparams=hparam)\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(hparam['epsilon'], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weights given to each class\n",
    "num_class = 100\n",
    "for i in range(num_class):\n",
    "    print(i, np.sum(tuner.appended_task_list[-1].train.weights[(np.argmax(tuner.appended_task_list[-1].train.labels, axis=1) == i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = tuner.appended_task_list[1].train.weights[np.argmax(tuner.appended_task_list[1].train.labels, axis=1) != 99]\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "# plt.hist(old_weights)\n",
    "# seaborn.distplot(old_weights)\n",
    "print(old_weights.shape)\n",
    "print(np.sum(old_weights < 1e-5))\n",
    "print(np.sum((old_weights >= 1e-5) * (old_weights < 1e-2)))\n",
    "# for i in range(1000):\n",
    "#     print(w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.saveResultsList()\n",
    "tuner.saveBestHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "test_accuracies = []\n",
    "for i in range(t + 1):\n",
    "    accuracy = tuner.test(i, TEST_BATCH_SIZE, restore_model=True)\n",
    "    test_accuracies.append(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracies)\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r DB1_test_accuracies\n",
    "plt.plot(test_accuracies, color='g')\n",
    "plt.plot(DB1_test_accuracies, color='b')\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tuner.appended_task_list[-1].train.weights[0 : tuner.appended_task_list[-2].train.weights.shape[0]], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "filename='code_state.bak'\n",
    "my_shelf = shelve.open(filename,'n') # 'n' for new\n",
    "\n",
    "for key in dir():\n",
    "    try:\n",
    "        my_shelf[key] = globals()[key]\n",
    "    except TypeError:\n",
    "        #\n",
    "        # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "        #\n",
    "        print('ERROR shelving: {0}'.format(key))\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
