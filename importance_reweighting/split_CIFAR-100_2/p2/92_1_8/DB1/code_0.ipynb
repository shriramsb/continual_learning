{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    pass\n",
    "#     task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints_0/'\n",
    "summaries_path = cur_dir + 'summaries_0/'\n",
    "data_path = task_home + 'cifar-100-python/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2,
     29,
     36,
     41
    ]
   },
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "def splitDataset(dataset, dataset_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    task_list = []\n",
    "    train_labels = np.argmax(dataset.train.labels, axis=1)\n",
    "    validation_labels = np.argmax(dataset.validation.labels, axis=1)\n",
    "    test_labels = np.argmax(dataset.test.labels, axis=1)\n",
    "    for i in range(len(dataset_split)):\n",
    "        cur_train_indices = [False] * dataset.train.images.shape[0]\n",
    "        cur_validation_indices = [False] * dataset.validation.images.shape[0]\n",
    "        cur_test_indices = [False] * dataset.test.images.shape[0]\n",
    "        for j in range(len(dataset_split[i])):\n",
    "            cur_train_indices = np.logical_or(cur_train_indices, (train_labels == dataset_split[i][j]))\n",
    "            cur_validation_indices = np.logical_or(cur_validation_indices, (validation_labels == dataset_split[i][j]))\n",
    "            cur_test_indices = np.logical_or(cur_test_indices, (test_labels == dataset_split[i][j]))\n",
    "\n",
    "        task = deepcopy(dataset)\n",
    "        task.train.images = task.train.images[cur_train_indices]\n",
    "        task.train.labels = task.train.labels[cur_train_indices]\n",
    "        task.validation.images = task.validation.images[cur_validation_indices]\n",
    "        task.validation.labels = task.validation.labels[cur_validation_indices]\n",
    "        task.test.images = task.test.images[cur_test_indices]\n",
    "        task.test.labels = task.test.labels[cur_test_indices]\n",
    "        task = MyTask(task)\n",
    "        task_list.append(task)\n",
    "\n",
    "    return task_list\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "\n",
    "class TempDataset(object):\n",
    "    def __init__(self):\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "    \n",
    "class TempTask(object):\n",
    "    def __init__(self):\n",
    "        self.train = TempDataset()\n",
    "        self.validation = TempDataset()\n",
    "        self.test = TempDataset()\n",
    "    \n",
    "    \n",
    "def readDatasets():\n",
    "    num_class = 100\n",
    "    labels_list = list(range(num_class))\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels_list)\n",
    "    split = []\n",
    "    task_weights = []\n",
    "    \n",
    "    split = [labels_list[ : 92]]\n",
    "    task_weights = [0.92]\n",
    "    for single_label in labels_list[92 : ]:\n",
    "        split.append([single_label])\n",
    "        task_weights.append(0.01)\n",
    "    num_tasks = len(split)\n",
    "    \n",
    "    with open(data_path + 'train', 'rb') as f:\n",
    "        f_train_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    with open(data_path + 'test', 'rb') as f:\n",
    "        f_test_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    cifar_100 = TempTask()\n",
    "    temp_train_labels = np.array(f_train_data[b'fine_labels'], dtype=np.int32)\n",
    "    temp_test_labels = np.array(f_test_data[b'fine_labels'], dtype=np.int32)\n",
    "    f_train_data[b'fine_labels'] = np.zeros((temp_train_labels.shape[0], num_class))\n",
    "    (f_train_data[b'fine_labels'])[range(temp_train_labels.shape[0]), temp_train_labels] = 1\n",
    "    f_test_data[b'fine_labels'] = np.zeros((temp_test_labels.shape[0], num_class))\n",
    "    (f_test_data[b'fine_labels'])[range(temp_test_labels.shape[0]), temp_test_labels] = 1\n",
    "    f_train_data[b'data'] = np.reshape(f_train_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_test_data[b'data'] = np.reshape(f_test_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_train_data[b'data'] = np.transpose(f_train_data[b'data'], (0, 2, 3, 1))\n",
    "    f_test_data[b'data'] = np.transpose(f_test_data[b'data'], (0, 2, 3, 1))\n",
    "    \n",
    "    tr_data = f_train_data[b'data']\n",
    "    te_data = f_test_data[b'data']\n",
    "    # normalizing data\n",
    "    avg = np.mean(tr_data, axis=(0, 1, 2))\n",
    "    std = np.std(tr_data, axis=(0, 1, 2))\n",
    "    \n",
    "    f_train_data[b'data'] = (tr_data - avg) / std\n",
    "    f_test_data[b'data'] = (te_data - avg) / std\n",
    "    \n",
    "    seed = 0\n",
    "    np.random.seed(0)\n",
    "    shuffle_train_perm = np.random.permutation(f_train_data[b'data'].shape[0])\n",
    "    f_train_data[b'data'] = f_train_data[b'data'][shuffle_train_perm]\n",
    "    f_train_data[b'fine_labels'] = f_train_data[b'fine_labels'][shuffle_train_perm]\n",
    "    \n",
    "    num_val_per_class = 20\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        pos = (np.argmax(f_train_data[b'fine_labels'], axis=1) == i)\n",
    "        \n",
    "        if (i == 0):\n",
    "            cifar_100.validation.images = (f_train_data[b'data'][pos])[0 : num_val_per_class]\n",
    "            cifar_100.validation.labels = (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]\n",
    "\n",
    "            cifar_100.train.images = (f_train_data[b'data'][pos])[num_val_per_class : ]\n",
    "            cifar_100.train.labels = (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]\n",
    "        else:\n",
    "            cifar_100.validation.images = np.concatenate((cifar_100.validation.images, (f_train_data[b'data'][pos])[0 : num_val_per_class]))\n",
    "            cifar_100.validation.labels = np.concatenate((cifar_100.validation.labels, (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]))\n",
    "\n",
    "            cifar_100.train.images = np.concatenate((cifar_100.train.images, (f_train_data[b'data'][pos])[num_val_per_class : ]))\n",
    "            cifar_100.train.labels = np.concatenate((cifar_100.train.labels, (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]))\n",
    "        \n",
    "    cifar_100.test.images = f_test_data[b'data']\n",
    "    cifar_100.test.labels = f_test_data[b'fine_labels']\n",
    "    \n",
    "    shuffle_train_perm = np.random.permutation(cifar_100.train.images.shape[0])\n",
    "    cifar_100.train.images = cifar_100.train.images[shuffle_train_perm]\n",
    "    cifar_100.train.labels = cifar_100.train.labels[shuffle_train_perm]\n",
    "    \n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(cifar_100)\n",
    "        \n",
    "    task_list = splitDataset(cifar_100, split, seed)\n",
    "    return split, num_tasks, task_weights, task_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tuner object and train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "output_shape = (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../../classifiers.py:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(1.0)\n",
    "tuner.updateTunerHparams({'mask_softmax' : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training each task separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 128\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = 1\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tasks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [(((49, 1e-1), (63, 1e-1 / 5), 1e-1 / (5 * 5)), (1e-1, ))]\n",
    "momentums = [0.9]\n",
    "regs = [0.00001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(0, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(hparams)\n",
    "num_epochs = 70\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with T=None,alpha=0.0,dropout_hidden_prob=0.9,dropout_input_prob=0.9,epsilon=0.0,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0\n",
      "epoch: 0, iter: 100/345, loss: 4.065792, accuracy: 0.054688\n",
      "epoch: 0, iter: 200/345, loss: 3.757479, accuracy: 0.132812\n",
      "epoch: 0, iter: 300/345, loss: 3.685406, accuracy: 0.156250\n",
      "epoch: 1, iter: 0/345, validation accuracies: [0.11467391], average train loss: 4.012445, average train accuracy: 0.072260\n",
      "epoch: 1, iter: 55/345, loss: 3.667204, accuracy: 0.156250\n",
      "epoch: 1, iter: 155/345, loss: 3.541535, accuracy: 0.085938\n",
      "epoch: 1, iter: 255/345, loss: 3.672745, accuracy: 0.101562\n",
      "epoch: 2, iter: 0/345, validation accuracies: [0.16358696], average train loss: 3.534137, average train accuracy: 0.142074\n",
      "epoch: 2, iter: 10/345, loss: 3.230022, accuracy: 0.226562\n",
      "epoch: 2, iter: 110/345, loss: 3.155966, accuracy: 0.218750\n",
      "epoch: 2, iter: 210/345, loss: 3.003378, accuracy: 0.250000\n",
      "epoch: 2, iter: 310/345, loss: 2.775810, accuracy: 0.281250\n",
      "epoch: 3, iter: 0/345, validation accuracies: [0.22826087], average train loss: 3.161631, average train accuracy: 0.212138\n",
      "epoch: 3, iter: 65/345, loss: 3.128375, accuracy: 0.171875\n",
      "epoch: 3, iter: 165/345, loss: 2.871029, accuracy: 0.273438\n",
      "epoch: 3, iter: 265/345, loss: 2.713093, accuracy: 0.312500\n",
      "epoch: 4, iter: 0/345, validation accuracies: [0.25978261], average train loss: 2.830092, average train accuracy: 0.274683\n",
      "epoch: 4, iter: 20/345, loss: 2.639944, accuracy: 0.289062\n",
      "epoch: 4, iter: 120/345, loss: 2.492246, accuracy: 0.343750\n",
      "epoch: 4, iter: 220/345, loss: 2.297718, accuracy: 0.375000\n",
      "epoch: 4, iter: 320/345, loss: 2.330138, accuracy: 0.359375\n",
      "epoch: 5, iter: 0/345, validation accuracies: [0.32989131], average train loss: 2.509714, average train accuracy: 0.340444\n",
      "epoch: 5, iter: 75/345, loss: 2.186163, accuracy: 0.398438\n",
      "epoch: 5, iter: 175/345, loss: 2.453552, accuracy: 0.312500\n",
      "epoch: 5, iter: 275/345, loss: 2.206851, accuracy: 0.414062\n",
      "epoch: 6, iter: 0/345, validation accuracies: [0.38913044], average train loss: 2.251921, average train accuracy: 0.397373\n",
      "epoch: 6, iter: 30/345, loss: 2.390834, accuracy: 0.359375\n",
      "epoch: 6, iter: 130/345, loss: 1.960548, accuracy: 0.390625\n",
      "epoch: 6, iter: 230/345, loss: 1.719984, accuracy: 0.523438\n",
      "epoch: 6, iter: 330/345, loss: 1.813938, accuracy: 0.500000\n",
      "epoch: 7, iter: 0/345, validation accuracies: [0.40054348], average train loss: 2.024337, average train accuracy: 0.447894\n",
      "epoch: 7, iter: 85/345, loss: 2.311787, accuracy: 0.390625\n",
      "epoch: 7, iter: 185/345, loss: 1.909036, accuracy: 0.460938\n",
      "epoch: 7, iter: 285/345, loss: 1.770294, accuracy: 0.468750\n",
      "epoch: 8, iter: 0/345, validation accuracies: [0.44891304], average train loss: 1.866746, average train accuracy: 0.485190\n",
      "epoch: 8, iter: 40/345, loss: 1.696023, accuracy: 0.539062\n",
      "epoch: 8, iter: 140/345, loss: 1.636640, accuracy: 0.539062\n",
      "epoch: 8, iter: 240/345, loss: 1.613124, accuracy: 0.523438\n",
      "epoch: 8, iter: 340/345, loss: 1.832483, accuracy: 0.523438\n",
      "epoch: 9, iter: 0/345, validation accuracies: [0.48152173], average train loss: 1.733059, average train accuracy: 0.515693\n",
      "epoch: 9, iter: 95/345, loss: 1.616636, accuracy: 0.523438\n",
      "epoch: 9, iter: 195/345, loss: 1.454950, accuracy: 0.585938\n",
      "epoch: 9, iter: 295/345, loss: 1.815950, accuracy: 0.476562\n",
      "epoch: 10, iter: 0/345, validation accuracies: [0.47173912], average train loss: 1.630946, average train accuracy: 0.545720\n",
      "epoch: 10, iter: 50/345, loss: 1.364086, accuracy: 0.648438\n",
      "epoch: 10, iter: 150/345, loss: 1.326403, accuracy: 0.617188\n",
      "epoch: 10, iter: 250/345, loss: 1.366153, accuracy: 0.578125\n",
      "epoch: 11, iter: 0/345, validation accuracies: [0.49782609], average train loss: 1.537551, average train accuracy: 0.563066\n",
      "epoch: 11, iter: 5/345, loss: 1.550905, accuracy: 0.539062\n",
      "epoch: 11, iter: 105/345, loss: 1.433757, accuracy: 0.617188\n",
      "epoch: 11, iter: 205/345, loss: 1.384872, accuracy: 0.570312\n",
      "epoch: 11, iter: 305/345, loss: 1.352278, accuracy: 0.593750\n",
      "epoch: 12, iter: 0/345, validation accuracies: [0.47880434], average train loss: 1.450583, average train accuracy: 0.587296\n",
      "epoch: 12, iter: 60/345, loss: 1.409726, accuracy: 0.625000\n",
      "epoch: 12, iter: 160/345, loss: 1.211002, accuracy: 0.656250\n",
      "epoch: 12, iter: 260/345, loss: 1.224455, accuracy: 0.664062\n",
      "epoch: 13, iter: 0/345, validation accuracies: [0.5125], average train loss: 1.383164, average train accuracy: 0.602921\n",
      "epoch: 13, iter: 15/345, loss: 1.360214, accuracy: 0.679688\n",
      "epoch: 13, iter: 115/345, loss: 1.332347, accuracy: 0.632812\n",
      "epoch: 13, iter: 215/345, loss: 1.439538, accuracy: 0.523438\n",
      "epoch: 13, iter: 315/345, loss: 1.370323, accuracy: 0.601562\n",
      "epoch: 14, iter: 0/345, validation accuracies: [0.53913043], average train loss: 1.330246, average train accuracy: 0.615399\n",
      "epoch: 14, iter: 70/345, loss: 1.344030, accuracy: 0.640625\n",
      "epoch: 14, iter: 170/345, loss: 1.475513, accuracy: 0.585938\n",
      "epoch: 14, iter: 270/345, loss: 1.086898, accuracy: 0.726562\n",
      "epoch: 15, iter: 0/345, validation accuracies: [0.52119565], average train loss: 1.264522, average train accuracy: 0.637477\n",
      "epoch: 15, iter: 25/345, loss: 1.270878, accuracy: 0.632812\n",
      "epoch: 15, iter: 125/345, loss: 1.220457, accuracy: 0.664062\n",
      "epoch: 15, iter: 225/345, loss: 1.394464, accuracy: 0.632812\n",
      "epoch: 15, iter: 325/345, loss: 1.015445, accuracy: 0.656250\n",
      "epoch: 16, iter: 0/345, validation accuracies: [0.54510869], average train loss: 1.213001, average train accuracy: 0.648007\n",
      "epoch: 16, iter: 80/345, loss: 1.004785, accuracy: 0.710938\n",
      "epoch: 16, iter: 180/345, loss: 1.131909, accuracy: 0.664062\n",
      "epoch: 16, iter: 280/345, loss: 1.116011, accuracy: 0.679688\n",
      "epoch: 17, iter: 0/345, validation accuracies: [0.56630435], average train loss: 1.171852, average train accuracy: 0.658967\n",
      "epoch: 17, iter: 35/345, loss: 1.190738, accuracy: 0.671875\n",
      "epoch: 17, iter: 135/345, loss: 0.916009, accuracy: 0.671875\n",
      "epoch: 17, iter: 235/345, loss: 1.027127, accuracy: 0.664062\n",
      "epoch: 17, iter: 335/345, loss: 1.131103, accuracy: 0.664062\n",
      "epoch: 18, iter: 0/345, validation accuracies: [0.5701087], average train loss: 1.134054, average train accuracy: 0.666168\n",
      "epoch: 18, iter: 90/345, loss: 1.233106, accuracy: 0.617188\n",
      "epoch: 18, iter: 190/345, loss: 1.076221, accuracy: 0.695312\n",
      "epoch: 18, iter: 290/345, loss: 0.934704, accuracy: 0.718750\n",
      "epoch: 19, iter: 0/345, validation accuracies: [0.57173913], average train loss: 1.091408, average train accuracy: 0.677355\n",
      "epoch: 19, iter: 45/345, loss: 0.910845, accuracy: 0.718750\n",
      "epoch: 19, iter: 145/345, loss: 1.043066, accuracy: 0.703125\n",
      "epoch: 19, iter: 245/345, loss: 1.072929, accuracy: 0.726562\n",
      "epoch: 20, iter: 0/345, validation accuracies: [0.5472826], average train loss: 1.065651, average train accuracy: 0.683741\n",
      "epoch: 20, iter: 0/345, loss: 0.936437, accuracy: 0.679688\n",
      "epoch: 20, iter: 100/345, loss: 1.069525, accuracy: 0.703125\n",
      "epoch: 20, iter: 200/345, loss: 1.156470, accuracy: 0.664062\n",
      "epoch: 20, iter: 300/345, loss: 1.491213, accuracy: 0.656250\n",
      "epoch: 21, iter: 0/345, validation accuracies: [0.57065216], average train loss: 1.033426, average train accuracy: 0.693229\n",
      "epoch: 21, iter: 55/345, loss: 1.079933, accuracy: 0.679688\n",
      "epoch: 21, iter: 155/345, loss: 1.008347, accuracy: 0.742188\n",
      "epoch: 21, iter: 255/345, loss: 0.944356, accuracy: 0.734375\n",
      "epoch: 22, iter: 0/345, validation accuracies: [0.58423913], average train loss: 1.006120, average train accuracy: 0.702468\n",
      "epoch: 22, iter: 10/345, loss: 0.995875, accuracy: 0.718750\n",
      "epoch: 22, iter: 110/345, loss: 1.070105, accuracy: 0.640625\n",
      "epoch: 22, iter: 210/345, loss: 0.892141, accuracy: 0.703125\n",
      "epoch: 22, iter: 310/345, loss: 0.998371, accuracy: 0.687500\n",
      "epoch: 23, iter: 0/345, validation accuracies: [0.57880435], average train loss: 0.966576, average train accuracy: 0.711481\n",
      "epoch: 23, iter: 65/345, loss: 1.186784, accuracy: 0.679688\n",
      "epoch: 23, iter: 165/345, loss: 0.907952, accuracy: 0.757812\n",
      "epoch: 23, iter: 265/345, loss: 0.844191, accuracy: 0.742188\n",
      "epoch: 24, iter: 0/345, validation accuracies: [0.59619566], average train loss: 0.944685, average train accuracy: 0.715987\n",
      "epoch: 24, iter: 20/345, loss: 0.780182, accuracy: 0.742188\n",
      "epoch: 24, iter: 120/345, loss: 0.854457, accuracy: 0.742188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, iter: 220/345, loss: 0.807204, accuracy: 0.781250\n",
      "epoch: 24, iter: 320/345, loss: 0.697206, accuracy: 0.742188\n",
      "epoch: 25, iter: 0/345, validation accuracies: [0.58315218], average train loss: 0.907201, average train accuracy: 0.727785\n",
      "epoch: 25, iter: 75/345, loss: 0.935612, accuracy: 0.695312\n",
      "epoch: 25, iter: 175/345, loss: 0.839009, accuracy: 0.703125\n",
      "epoch: 25, iter: 275/345, loss: 0.768374, accuracy: 0.796875\n",
      "epoch: 26, iter: 0/345, validation accuracies: [0.59456521], average train loss: 0.897390, average train accuracy: 0.731748\n",
      "epoch: 26, iter: 30/345, loss: 0.738674, accuracy: 0.804688\n",
      "epoch: 26, iter: 130/345, loss: 1.109453, accuracy: 0.648438\n",
      "epoch: 26, iter: 230/345, loss: 0.797547, accuracy: 0.742188\n",
      "epoch: 26, iter: 330/345, loss: 0.756879, accuracy: 0.781250\n",
      "epoch: 27, iter: 0/345, validation accuracies: [0.58260869], average train loss: 0.879265, average train accuracy: 0.732858\n",
      "epoch: 27, iter: 85/345, loss: 0.788258, accuracy: 0.765625\n",
      "epoch: 27, iter: 185/345, loss: 0.873418, accuracy: 0.765625\n",
      "epoch: 27, iter: 285/345, loss: 1.044455, accuracy: 0.656250\n",
      "epoch: 28, iter: 0/345, validation accuracies: [0.57663044], average train loss: 0.853040, average train accuracy: 0.744995\n",
      "epoch: 28, iter: 40/345, loss: 0.872970, accuracy: 0.718750\n",
      "epoch: 28, iter: 140/345, loss: 0.757868, accuracy: 0.773438\n",
      "epoch: 28, iter: 240/345, loss: 0.613614, accuracy: 0.804688\n",
      "epoch: 28, iter: 340/345, loss: 0.860133, accuracy: 0.773438\n",
      "epoch: 29, iter: 0/345, validation accuracies: [0.58315218], average train loss: 0.823303, average train accuracy: 0.751925\n",
      "epoch: 29, iter: 95/345, loss: 0.657954, accuracy: 0.765625\n",
      "epoch: 29, iter: 195/345, loss: 0.670726, accuracy: 0.812500\n",
      "epoch: 29, iter: 295/345, loss: 0.673359, accuracy: 0.781250\n",
      "epoch: 30, iter: 0/345, validation accuracies: [0.55652174], average train loss: 0.805728, average train accuracy: 0.754846\n",
      "epoch: 30, iter: 50/345, loss: 0.827184, accuracy: 0.734375\n",
      "epoch: 30, iter: 150/345, loss: 0.856847, accuracy: 0.742188\n",
      "epoch: 30, iter: 250/345, loss: 0.878753, accuracy: 0.703125\n",
      "epoch: 31, iter: 0/345, validation accuracies: [0.6], average train loss: 0.787534, average train accuracy: 0.758854\n",
      "epoch: 31, iter: 5/345, loss: 0.680906, accuracy: 0.820312\n",
      "epoch: 31, iter: 105/345, loss: 0.844871, accuracy: 0.773438\n",
      "epoch: 31, iter: 205/345, loss: 0.573695, accuracy: 0.796875\n",
      "epoch: 31, iter: 305/345, loss: 0.660889, accuracy: 0.796875\n",
      "epoch: 32, iter: 0/345, validation accuracies: [0.60923912], average train loss: 0.764637, average train accuracy: 0.765648\n",
      "epoch: 32, iter: 60/345, loss: 0.744053, accuracy: 0.781250\n",
      "epoch: 32, iter: 160/345, loss: 0.470560, accuracy: 0.851562\n",
      "epoch: 32, iter: 260/345, loss: 0.708545, accuracy: 0.820312\n",
      "epoch: 33, iter: 0/345, validation accuracies: [0.62119565], average train loss: 0.739264, average train accuracy: 0.772486\n",
      "epoch: 33, iter: 15/345, loss: 0.850093, accuracy: 0.757812\n",
      "epoch: 33, iter: 115/345, loss: 1.004608, accuracy: 0.718750\n",
      "epoch: 33, iter: 215/345, loss: 0.872750, accuracy: 0.765625\n",
      "epoch: 33, iter: 315/345, loss: 0.730515, accuracy: 0.734375\n",
      "epoch: 34, iter: 0/345, validation accuracies: [0.59836956], average train loss: 0.741450, average train accuracy: 0.773256\n",
      "epoch: 34, iter: 70/345, loss: 0.667719, accuracy: 0.796875\n",
      "epoch: 34, iter: 170/345, loss: 0.858627, accuracy: 0.750000\n",
      "epoch: 34, iter: 270/345, loss: 0.679466, accuracy: 0.796875\n",
      "epoch: 35, iter: 0/345, validation accuracies: [0.60054348], average train loss: 0.723304, average train accuracy: 0.776336\n",
      "epoch: 35, iter: 25/345, loss: 0.499255, accuracy: 0.851562\n",
      "epoch: 35, iter: 125/345, loss: 0.691849, accuracy: 0.796875\n",
      "epoch: 35, iter: 225/345, loss: 0.450853, accuracy: 0.820312\n",
      "epoch: 35, iter: 325/345, loss: 0.922802, accuracy: 0.742188\n",
      "epoch: 36, iter: 0/345, validation accuracies: [0.61086956], average train loss: 0.703229, average train accuracy: 0.784783\n",
      "epoch: 36, iter: 80/345, loss: 0.630027, accuracy: 0.820312\n",
      "epoch: 36, iter: 180/345, loss: 0.804093, accuracy: 0.789062\n",
      "epoch: 36, iter: 280/345, loss: 0.750248, accuracy: 0.796875\n",
      "epoch: 37, iter: 0/345, validation accuracies: [0.61902174], average train loss: 0.699046, average train accuracy: 0.785620\n",
      "epoch: 37, iter: 35/345, loss: 0.728442, accuracy: 0.781250\n",
      "epoch: 37, iter: 135/345, loss: 0.604002, accuracy: 0.812500\n",
      "epoch: 37, iter: 235/345, loss: 0.608393, accuracy: 0.781250\n",
      "epoch: 37, iter: 335/345, loss: 0.716589, accuracy: 0.750000\n",
      "epoch: 38, iter: 0/345, validation accuracies: [0.59184783], average train loss: 0.671542, average train accuracy: 0.791191\n",
      "epoch: 38, iter: 90/345, loss: 0.725563, accuracy: 0.804688\n",
      "epoch: 38, iter: 190/345, loss: 0.834334, accuracy: 0.773438\n",
      "epoch: 38, iter: 290/345, loss: 0.563247, accuracy: 0.804688\n",
      "epoch: 39, iter: 0/345, validation accuracies: [0.61032609], average train loss: 0.651371, average train accuracy: 0.796218\n",
      "epoch: 39, iter: 45/345, loss: 0.606664, accuracy: 0.835938\n",
      "epoch: 39, iter: 145/345, loss: 0.532532, accuracy: 0.835938\n",
      "epoch: 39, iter: 245/345, loss: 0.680756, accuracy: 0.789062\n",
      "epoch: 40, iter: 0/345, validation accuracies: [0.61630434], average train loss: 0.638485, average train accuracy: 0.800702\n",
      "epoch: 40, iter: 0/345, loss: 0.610650, accuracy: 0.765625\n",
      "epoch: 40, iter: 100/345, loss: 0.624824, accuracy: 0.796875\n",
      "epoch: 40, iter: 200/345, loss: 0.589548, accuracy: 0.804688\n",
      "epoch: 40, iter: 300/345, loss: 0.761950, accuracy: 0.789062\n",
      "epoch: 41, iter: 0/345, validation accuracies: [0.60923912], average train loss: 0.625340, average train accuracy: 0.803193\n",
      "epoch: 41, iter: 55/345, loss: 0.571606, accuracy: 0.820312\n",
      "epoch: 41, iter: 155/345, loss: 0.708819, accuracy: 0.773438\n",
      "epoch: 41, iter: 255/345, loss: 0.522704, accuracy: 0.835938\n",
      "epoch: 42, iter: 0/345, validation accuracies: [0.6048913], average train loss: 0.625555, average train accuracy: 0.804778\n",
      "epoch: 42, iter: 10/345, loss: 0.649968, accuracy: 0.812500\n",
      "epoch: 42, iter: 110/345, loss: 0.657855, accuracy: 0.781250\n",
      "epoch: 42, iter: 210/345, loss: 0.484178, accuracy: 0.843750\n",
      "epoch: 42, iter: 310/345, loss: 0.594324, accuracy: 0.820312\n",
      "epoch: 43, iter: 0/345, validation accuracies: [0.61141304], average train loss: 0.607976, average train accuracy: 0.809330\n",
      "epoch: 43, iter: 65/345, loss: 0.636236, accuracy: 0.789062\n",
      "epoch: 43, iter: 165/345, loss: 0.607811, accuracy: 0.828125\n",
      "epoch: 43, iter: 265/345, loss: 0.621075, accuracy: 0.789062\n",
      "epoch: 44, iter: 0/345, validation accuracies: [0.61250001], average train loss: 0.596110, average train accuracy: 0.813949\n",
      "epoch: 44, iter: 20/345, loss: 0.604474, accuracy: 0.843750\n",
      "epoch: 44, iter: 120/345, loss: 0.546140, accuracy: 0.843750\n",
      "epoch: 44, iter: 220/345, loss: 0.631901, accuracy: 0.812500\n",
      "epoch: 44, iter: 320/345, loss: 0.548213, accuracy: 0.804688\n",
      "epoch: 45, iter: 0/345, validation accuracies: [0.61467391], average train loss: 0.588233, average train accuracy: 0.815897\n",
      "epoch: 45, iter: 75/345, loss: 0.632099, accuracy: 0.820312\n",
      "epoch: 45, iter: 175/345, loss: 0.491955, accuracy: 0.851562\n",
      "epoch: 45, iter: 275/345, loss: 0.668642, accuracy: 0.804688\n",
      "epoch: 46, iter: 0/345, validation accuracies: [0.62228262], average train loss: 0.580074, average train accuracy: 0.819656\n",
      "epoch: 46, iter: 30/345, loss: 0.475978, accuracy: 0.835938\n",
      "epoch: 46, iter: 130/345, loss: 0.728201, accuracy: 0.757812\n",
      "epoch: 46, iter: 230/345, loss: 0.570630, accuracy: 0.851562\n",
      "epoch: 46, iter: 330/345, loss: 0.602152, accuracy: 0.789062\n",
      "epoch: 47, iter: 0/345, validation accuracies: [0.6103261], average train loss: 0.562754, average train accuracy: 0.822600\n",
      "epoch: 47, iter: 85/345, loss: 0.410404, accuracy: 0.851562\n",
      "epoch: 47, iter: 185/345, loss: 0.594441, accuracy: 0.812500\n",
      "epoch: 47, iter: 285/345, loss: 0.602453, accuracy: 0.796875\n",
      "epoch: 48, iter: 0/345, validation accuracies: [0.62391306], average train loss: 0.550043, average train accuracy: 0.826291\n",
      "epoch: 48, iter: 40/345, loss: 0.662949, accuracy: 0.820312\n",
      "epoch: 48, iter: 140/345, loss: 0.501846, accuracy: 0.859375\n",
      "epoch: 48, iter: 240/345, loss: 0.738756, accuracy: 0.718750\n",
      "epoch: 48, iter: 340/345, loss: 0.466529, accuracy: 0.835938\n",
      "epoch: 49, iter: 0/345, validation accuracies: [0.61684782], average train loss: 0.548711, average train accuracy: 0.829053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, iter: 95/345, loss: 0.515325, accuracy: 0.882812\n",
      "epoch: 49, iter: 195/345, loss: 0.279907, accuracy: 0.882812\n",
      "epoch: 49, iter: 295/345, loss: 0.514196, accuracy: 0.851562\n",
      "epoch: 50, iter: 0/345, validation accuracies: [0.65652174], average train loss: 0.406463, average train accuracy: 0.875476\n",
      "epoch: 50, iter: 50/345, loss: 0.386300, accuracy: 0.890625\n",
      "epoch: 50, iter: 150/345, loss: 0.256905, accuracy: 0.929688\n",
      "epoch: 50, iter: 250/345, loss: 0.311196, accuracy: 0.882812\n",
      "epoch: 51, iter: 0/345, validation accuracies: [0.65706522], average train loss: 0.353938, average train accuracy: 0.891395\n",
      "epoch: 51, iter: 5/345, loss: 0.430301, accuracy: 0.851562\n",
      "epoch: 51, iter: 105/345, loss: 0.523969, accuracy: 0.820312\n",
      "epoch: 51, iter: 205/345, loss: 0.345226, accuracy: 0.921875\n",
      "epoch: 51, iter: 305/345, loss: 0.427523, accuracy: 0.875000\n",
      "epoch: 52, iter: 0/345, validation accuracies: [0.66413043], average train loss: 0.332908, average train accuracy: 0.897622\n",
      "epoch: 52, iter: 60/345, loss: 0.325976, accuracy: 0.882812\n",
      "epoch: 52, iter: 160/345, loss: 0.271350, accuracy: 0.914062\n",
      "epoch: 52, iter: 260/345, loss: 0.369709, accuracy: 0.890625\n",
      "epoch: 53, iter: 0/345, validation accuracies: [0.66521739], average train loss: 0.313856, average train accuracy: 0.903487\n",
      "epoch: 53, iter: 15/345, loss: 0.270057, accuracy: 0.914062\n",
      "epoch: 53, iter: 115/345, loss: 0.288784, accuracy: 0.914062\n",
      "epoch: 53, iter: 215/345, loss: 0.352528, accuracy: 0.890625\n",
      "epoch: 53, iter: 315/345, loss: 0.294818, accuracy: 0.914062\n",
      "epoch: 54, iter: 0/345, validation accuracies: [0.66739131], average train loss: 0.309270, average train accuracy: 0.904778\n",
      "epoch: 54, iter: 70/345, loss: 0.354361, accuracy: 0.914062\n",
      "epoch: 54, iter: 170/345, loss: 0.395110, accuracy: 0.859375\n",
      "epoch: 54, iter: 270/345, loss: 0.250451, accuracy: 0.921875\n",
      "epoch: 55, iter: 0/345, validation accuracies: [0.65326088], average train loss: 0.296222, average train accuracy: 0.908333\n",
      "epoch: 55, iter: 25/345, loss: 0.348783, accuracy: 0.867188\n",
      "epoch: 55, iter: 125/345, loss: 0.255373, accuracy: 0.937500\n",
      "epoch: 55, iter: 225/345, loss: 0.149516, accuracy: 0.960938\n",
      "epoch: 55, iter: 325/345, loss: 0.206268, accuracy: 0.898438\n",
      "epoch: 56, iter: 0/345, validation accuracies: [0.66195653], average train loss: 0.292295, average train accuracy: 0.909760\n",
      "epoch: 56, iter: 80/345, loss: 0.307833, accuracy: 0.882812\n",
      "epoch: 56, iter: 180/345, loss: 0.245697, accuracy: 0.914062\n",
      "epoch: 56, iter: 280/345, loss: 0.233983, accuracy: 0.921875\n",
      "epoch: 57, iter: 0/345, validation accuracies: [0.66304349], average train loss: 0.275646, average train accuracy: 0.914538\n",
      "epoch: 57, iter: 35/345, loss: 0.222043, accuracy: 0.929688\n",
      "epoch: 57, iter: 135/345, loss: 0.288296, accuracy: 0.914062\n",
      "epoch: 57, iter: 235/345, loss: 0.331501, accuracy: 0.906250\n",
      "epoch: 57, iter: 335/345, loss: 0.297621, accuracy: 0.937500\n",
      "epoch: 58, iter: 0/345, validation accuracies: [0.65054347], average train loss: 0.277093, average train accuracy: 0.914719\n",
      "epoch: 58, iter: 90/345, loss: 0.251242, accuracy: 0.914062\n",
      "epoch: 58, iter: 190/345, loss: 0.367719, accuracy: 0.898438\n",
      "epoch: 58, iter: 290/345, loss: 0.218800, accuracy: 0.945312\n",
      "epoch: 59, iter: 0/345, validation accuracies: [0.65978261], average train loss: 0.270258, average train accuracy: 0.916033\n",
      "epoch: 59, iter: 45/345, loss: 0.205354, accuracy: 0.945312\n",
      "epoch: 59, iter: 145/345, loss: 0.302249, accuracy: 0.906250\n",
      "epoch: 59, iter: 245/345, loss: 0.176248, accuracy: 0.945312\n",
      "epoch: 60, iter: 0/345, validation accuracies: [0.65108697], average train loss: 0.266547, average train accuracy: 0.916327\n",
      "epoch: 60, iter: 0/345, loss: 0.240007, accuracy: 0.921875\n",
      "epoch: 60, iter: 100/345, loss: 0.261046, accuracy: 0.898438\n",
      "epoch: 60, iter: 200/345, loss: 0.149510, accuracy: 0.953125\n",
      "epoch: 60, iter: 300/345, loss: 0.248848, accuracy: 0.921875\n",
      "epoch: 61, iter: 0/345, validation accuracies: [0.65163043], average train loss: 0.256314, average train accuracy: 0.921218\n",
      "epoch: 61, iter: 55/345, loss: 0.298568, accuracy: 0.890625\n",
      "epoch: 61, iter: 155/345, loss: 0.272366, accuracy: 0.906250\n",
      "epoch: 61, iter: 255/345, loss: 0.306807, accuracy: 0.914062\n",
      "epoch: 62, iter: 0/345, validation accuracies: [0.6548913], average train loss: 0.256184, average train accuracy: 0.919882\n",
      "epoch: 62, iter: 10/345, loss: 0.155369, accuracy: 0.960938\n",
      "epoch: 62, iter: 110/345, loss: 0.205340, accuracy: 0.937500\n",
      "epoch: 62, iter: 210/345, loss: 0.247519, accuracy: 0.929688\n",
      "epoch: 62, iter: 310/345, loss: 0.283922, accuracy: 0.914062\n",
      "epoch: 63, iter: 0/345, validation accuracies: [0.65652173], average train loss: 0.245725, average train accuracy: 0.922441\n",
      "epoch: 63, iter: 65/345, loss: 0.279445, accuracy: 0.882812\n",
      "epoch: 63, iter: 165/345, loss: 0.174794, accuracy: 0.937500\n",
      "epoch: 63, iter: 265/345, loss: 0.267397, accuracy: 0.929688\n",
      "epoch: 64, iter: 0/345, validation accuracies: [0.66358695], average train loss: 0.233970, average train accuracy: 0.927559\n",
      "epoch: 64, iter: 20/345, loss: 0.254445, accuracy: 0.929688\n",
      "epoch: 64, iter: 120/345, loss: 0.220398, accuracy: 0.906250\n",
      "epoch: 64, iter: 220/345, loss: 0.186398, accuracy: 0.945312\n",
      "epoch: 64, iter: 320/345, loss: 0.271346, accuracy: 0.921875\n",
      "epoch: 65, iter: 0/345, validation accuracies: [0.66086958], average train loss: 0.221109, average train accuracy: 0.931114\n",
      "epoch: 65, iter: 75/345, loss: 0.191591, accuracy: 0.953125\n",
      "epoch: 65, iter: 175/345, loss: 0.191139, accuracy: 0.945312\n",
      "epoch: 65, iter: 275/345, loss: 0.292816, accuracy: 0.898438\n",
      "epoch: 66, iter: 0/345, validation accuracies: [0.66576087], average train loss: 0.221905, average train accuracy: 0.931567\n",
      "epoch: 66, iter: 30/345, loss: 0.195541, accuracy: 0.929688\n",
      "epoch: 66, iter: 130/345, loss: 0.219348, accuracy: 0.921875\n",
      "epoch: 66, iter: 230/345, loss: 0.188836, accuracy: 0.929688\n",
      "epoch: 66, iter: 330/345, loss: 0.193804, accuracy: 0.953125\n",
      "epoch: 67, iter: 0/345, validation accuracies: [0.66467392], average train loss: 0.219380, average train accuracy: 0.931793\n",
      "epoch: 67, iter: 85/345, loss: 0.167742, accuracy: 0.937500\n",
      "epoch: 67, iter: 185/345, loss: 0.172324, accuracy: 0.929688\n",
      "epoch: 67, iter: 285/345, loss: 0.135515, accuracy: 0.968750\n",
      "epoch: 68, iter: 0/345, validation accuracies: [0.66250001], average train loss: 0.215934, average train accuracy: 0.933379\n",
      "epoch: 68, iter: 40/345, loss: 0.167926, accuracy: 0.945312\n",
      "epoch: 68, iter: 140/345, loss: 0.254383, accuracy: 0.914062\n",
      "epoch: 68, iter: 240/345, loss: 0.149898, accuracy: 0.953125\n",
      "epoch: 68, iter: 340/345, loss: 0.132514, accuracy: 0.976562\n",
      "epoch: 69, iter: 0/345, validation accuracies: [0.66195652], average train loss: 0.213467, average train accuracy: 0.936028\n",
      "epoch: 69, iter: 95/345, loss: 0.200515, accuracy: 0.937500\n",
      "epoch: 69, iter: 195/345, loss: 0.181010, accuracy: 0.960938\n",
      "epoch: 69, iter: 295/345, loss: 0.203229, accuracy: 0.945312\n",
      "epoch: 70, iter: 0/345, validation accuracies: [0.66086956], average train loss: 0.210669, average train accuracy: 0.935575\n",
      "epochs: 70.000000, final train loss: 0.260986, validation accuracies: [0.66086956]\n",
      "best epochs: 54.000000, best_avg: 0.667391, validation accuracies: [0.66739131]\n",
      "saving model dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=1e-05,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0 at time step 24150\n",
      "calculating penultimate output...\n",
      "time taken: %f 4.128987550735474\n",
      "saving penultimate output...\n"
     ]
    }
   ],
   "source": [
    "best_avg, best_hparams_index = tuner.tuneTasksInRange(0, t, BATCH_SIZE, num_hparams, \n",
    "                                                        num_updates=num_updates, verbose=True, \n",
    "                                                        random_crop_flip=True, \n",
    "                                                        equal_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple][0]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_epoch = cur_res['best_epoch']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_epoch))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][-1], color='b', )\n",
    "plt.plot(cur_res['val_acc'][0], color='g', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "learning_rates = [(((49, 1e-1), (63, 1e-1 / 5), 1e-1 / (5 * 5)), (1e-1, ))]\n",
    "momentums = [0.9]\n",
    "regs = [0.00001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(1, t + 1):\n",
    "    tuner.hparams_list[i] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(hparams)\n",
    "num_epochs = 70\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg, best_hparams_index, test_acc = tuner.tuneTasksInRange(1, t, BATCH_SIZE, num_hparams, \n",
    "                                                        num_updates=num_updates, verbose=True, \n",
    "                                                        random_crop_flip=True, \n",
    "                                                        equal_weights=True, \n",
    "                                                        eval_test_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple][0]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_epoch = cur_res['best_epoch']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_epoch))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "# plt.plot(cur_res['loss'], color='m')\n",
    "# plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "# plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][-1], color='b', )\n",
    "plt.plot(cur_res['val_acc'][0], color='g', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllCosineSimilarity(wts, init_classes, only_dot=False):\n",
    "    num_init_class = len(init_classes)\n",
    "    cosine_wts = [[0.0 for _ in range(num_init_class)] for _ in range(num_init_class)]\n",
    "    for i in range(num_init_class):\n",
    "        for j in range(num_init_class):\n",
    "            w_i = wts[0][:, init_classes[i]]\n",
    "            w_j = wts[0][:, init_classes[j]]\n",
    "            if (only_dot):\n",
    "                cosine_wts[i][j] = np.sum(w_i * w_j)\n",
    "            else:\n",
    "                cosine_wts[i][j] = np.sum(w_i * w_j) / np.sqrt(np.sum(w_i ** 2)) / np.sqrt(np.sum(w_j ** 2))\n",
    "    return cosine_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks_test = 10\n",
    "wts = [None for _ in range(num_tasks_test)]\n",
    "init_classes = tuner.split[0]\n",
    "cosine_sim_wts = [None for _ in range(num_tasks_test)]\n",
    "dot_sim_wts = [None for _ in range(num_tasks_test)]\n",
    "for i in range(num_tasks_test):\n",
    "    tuner.test(i, BATCH_SIZE, restore_model=True, hparams=tuner.hparams_list[i][0])\n",
    "    wts[i] = sess.run([v for v in tf.all_variables() if 'dense' in v.name and 'kernel:0' in v.name])\n",
    "    cosine_sim_wts[i] = np.array(getAllCosineSimilarity(wts[i], init_classes, only_dot=False))\n",
    "    dot_sim_wts[i] = np.array(getAllCosineSimilarity(wts[i], init_classes, only_dot=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks_test):\n",
    "#     plt.imshow(cosine_sim_wts[i])\n",
    "    print(\"task\", i)\n",
    "    print(\"sum cosine: \", np.sum(cosine_sim_wts[i]) - num_init_class)\n",
    "    print(\"sum dot: \", np.sum(dot_sim_wts[i]) - np.sum(wts[i][0][: , init_classes] ** 2))\n",
    "    print(\"sum norms: \", np.sum(np.sqrt(np.sum(wts[i][0][:, init_classes] ** 2, axis=0))))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.saveResultsList()\n",
    "tuner.saveBestHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "test_accuracies = []\n",
    "for i in range(t + 1):\n",
    "    accuracy = tuner.test(i, TEST_BATCH_SIZE, restore_model=False)\n",
    "    test_accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(t + 1):\n",
    "    print(test_accuracies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "filename='code_state.bak'\n",
    "my_shelf = shelve.open(filename,'n') # 'n' for new\n",
    "\n",
    "for key in dir():\n",
    "    try:\n",
    "        my_shelf[key] = globals()[key]\n",
    "    except TypeError:\n",
    "        #\n",
    "        # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "        #\n",
    "        print('ERROR shelving: {0}'.format(key))\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
