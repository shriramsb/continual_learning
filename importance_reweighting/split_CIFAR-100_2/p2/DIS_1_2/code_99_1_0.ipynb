{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "from tuner import MyTask\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = True\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "\n",
    "if use_gpu:\n",
    "    import os\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "elif use_gpu:\n",
    "    sess = tf.Session(config=config)\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_home = ''\n",
    "if use_tpu:\n",
    "    pass\n",
    "#     task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = '../../../../'\n",
    "\n",
    "cur_dir = './'\n",
    "checkpoint_path = cur_dir + 'checkpoints_99_1_0/'\n",
    "summaries_path = cur_dir + 'summaries_99_1_0/'\n",
    "data_path = task_home + 'cifar-100-python/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'\n",
    "    \n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2,
     29,
     36,
     41
    ]
   },
   "outputs": [],
   "source": [
    "label_smooth_param = 0\n",
    "\n",
    "def splitDataset(dataset, dataset_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    task_list = []\n",
    "    train_labels = np.argmax(dataset.train.labels, axis=1)\n",
    "    validation_labels = np.argmax(dataset.validation.labels, axis=1)\n",
    "    test_labels = np.argmax(dataset.test.labels, axis=1)\n",
    "    for i in range(len(dataset_split)):\n",
    "        cur_train_indices = [False] * dataset.train.images.shape[0]\n",
    "        cur_validation_indices = [False] * dataset.validation.images.shape[0]\n",
    "        cur_test_indices = [False] * dataset.test.images.shape[0]\n",
    "        for j in range(len(dataset_split[i])):\n",
    "            cur_train_indices = np.logical_or(cur_train_indices, (train_labels == dataset_split[i][j]))\n",
    "            cur_validation_indices = np.logical_or(cur_validation_indices, (validation_labels == dataset_split[i][j]))\n",
    "            cur_test_indices = np.logical_or(cur_test_indices, (test_labels == dataset_split[i][j]))\n",
    "\n",
    "        task = deepcopy(dataset)\n",
    "        task.train.images = task.train.images[cur_train_indices]\n",
    "        task.train.labels = task.train.labels[cur_train_indices]\n",
    "        task.validation.images = task.validation.images[cur_validation_indices]\n",
    "        task.validation.labels = task.validation.labels[cur_validation_indices]\n",
    "        task.test.images = task.test.images[cur_test_indices]\n",
    "        task.test.labels = task.test.labels[cur_test_indices]\n",
    "        task = MyTask(task)\n",
    "        task_list.append(task)\n",
    "\n",
    "    return task_list\n",
    "    \n",
    "def smoothLabels(dataset):\n",
    "    train_labels = dataset.train.labels\n",
    "    train_labels_argmax = np.argmax(train_labels, axis=1)\n",
    "    train_labels = train_labels + label_smooth_param / (train_labels.shape[1] - 1)\n",
    "    train_labels[range(train_labels.shape[0]), train_labels_argmax] = 1 - label_smooth_param\n",
    "    dataset.train._labels = train_labels\n",
    "\n",
    "class TempDataset(object):\n",
    "    def __init__(self):\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "    \n",
    "class TempTask(object):\n",
    "    def __init__(self):\n",
    "        self.train = TempDataset()\n",
    "        self.validation = TempDataset()\n",
    "        self.test = TempDataset()\n",
    "    \n",
    "    \n",
    "def readDatasets():\n",
    "    num_class = 100\n",
    "    class_per_task = 2\n",
    "    k = 0\n",
    "    labels_list = list(range(num_class))\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels_list)\n",
    "    split = []\n",
    "    task_weights = []\n",
    "    \n",
    "    split = [range(99), [99]]\n",
    "    task_weights = [0.99, 0.01]\n",
    "    num_tasks = len(split)\n",
    "    \n",
    "    with open(data_path + 'train', 'rb') as f:\n",
    "        f_train_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    with open(data_path + 'test', 'rb') as f:\n",
    "        f_test_data = pickle.load(f, encoding='bytes')\n",
    "        \n",
    "    cifar_100 = TempTask()\n",
    "    temp_train_labels = np.array(f_train_data[b'fine_labels'], dtype=np.int32)\n",
    "    temp_test_labels = np.array(f_test_data[b'fine_labels'], dtype=np.int32)\n",
    "    f_train_data[b'fine_labels'] = np.zeros((temp_train_labels.shape[0], num_class))\n",
    "    (f_train_data[b'fine_labels'])[range(temp_train_labels.shape[0]), temp_train_labels] = 1\n",
    "    f_test_data[b'fine_labels'] = np.zeros((temp_test_labels.shape[0], num_class))\n",
    "    (f_test_data[b'fine_labels'])[range(temp_test_labels.shape[0]), temp_test_labels] = 1\n",
    "    f_train_data[b'data'] = np.reshape(f_train_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_test_data[b'data'] = np.reshape(f_test_data[b'data'], (-1, 3, 32, 32))\n",
    "    f_train_data[b'data'] = np.transpose(f_train_data[b'data'], (0, 2, 3, 1))\n",
    "    f_test_data[b'data'] = np.transpose(f_test_data[b'data'], (0, 2, 3, 1))\n",
    "    \n",
    "    tr_data = f_train_data[b'data']\n",
    "    te_data = f_test_data[b'data']\n",
    "    # normalizing data\n",
    "    avg = np.mean(tr_data, axis=(0, 1, 2))\n",
    "    std = np.std(tr_data, axis=(0, 1, 2))\n",
    "    \n",
    "    f_train_data[b'data'] = (tr_data - avg) / std\n",
    "    f_test_data[b'data'] = (te_data - avg) / std\n",
    "    \n",
    "    seed = 0\n",
    "    np.random.seed(0)\n",
    "    shuffle_train_perm = np.random.permutation(f_train_data[b'data'].shape[0])\n",
    "    f_train_data[b'data'] = f_train_data[b'data'][shuffle_train_perm]\n",
    "    f_train_data[b'fine_labels'] = f_train_data[b'fine_labels'][shuffle_train_perm]\n",
    "    \n",
    "    num_val_per_class = 20\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        pos = (np.argmax(f_train_data[b'fine_labels'], axis=1) == i)\n",
    "        \n",
    "        if (i == 0):\n",
    "            cifar_100.validation.images = (f_train_data[b'data'][pos])[0 : num_val_per_class]\n",
    "            cifar_100.validation.labels = (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]\n",
    "\n",
    "            cifar_100.train.images = (f_train_data[b'data'][pos])[num_val_per_class : ]\n",
    "            cifar_100.train.labels = (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]\n",
    "        else:\n",
    "            cifar_100.validation.images = np.concatenate((cifar_100.validation.images, (f_train_data[b'data'][pos])[0 : num_val_per_class]))\n",
    "            cifar_100.validation.labels = np.concatenate((cifar_100.validation.labels, (f_train_data[b'fine_labels'][pos])[0 : num_val_per_class]))\n",
    "\n",
    "            cifar_100.train.images = np.concatenate((cifar_100.train.images, (f_train_data[b'data'][pos])[num_val_per_class : ]))\n",
    "            cifar_100.train.labels = np.concatenate((cifar_100.train.labels, (f_train_data[b'fine_labels'][pos])[num_val_per_class : ]))\n",
    "        \n",
    "    cifar_100.test.images = f_test_data[b'data']\n",
    "    cifar_100.test.labels = f_test_data[b'fine_labels']\n",
    "    \n",
    "    shuffle_train_perm = np.random.permutation(cifar_100.train.images.shape[0])\n",
    "    cifar_100.train.images = cifar_100.train.images[shuffle_train_perm]\n",
    "    cifar_100.train.labels = cifar_100.train.labels[shuffle_train_perm]\n",
    "    \n",
    "    if (label_smooth_param != 0):\n",
    "        smoothLabels(cifar_100)\n",
    "        \n",
    "    task_list = splitDataset(cifar_100, split, seed)\n",
    "    return split, num_tasks, task_weights, task_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tuner object and train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "output_shape = (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../classifiers.py:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, network=network, \n",
    "                            input_shape=input_shape, output_shape=output_shape,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            readDatasets=readDatasets, load_best_hparams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.setPerExampleAppend(int(1))\n",
    "tuner.updateTunerHparams({'mask_softmax' : True})\n",
    "tuner.updateTunerHparams({'bf_num_images' : 2000})\n",
    "tuner.setPerExampleAppend(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training each task separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(best_avg, best_hparams)\n",
    "VALIDATION_BATCH_SIZE = 128\n",
    "print(tuner.validationAccuracy(t, VALIDATION_BATCH_SIZE, restore_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = 1\n",
    "learning_rates = [1e-1]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "\n",
    "tuner.hparams_list[t] = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 160\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "tuner.print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_avg, best_hparams = tuner.tuneOnTask(t, BATCH_SIZE, \n",
    "                                          save_weights=False, \n",
    "                                          num_updates=num_updates, verbose=True, \n",
    "                                          random_crop_flip=True)\n",
    "print(\"time taken : %d\" % (time.time() - start_time))\n",
    "sound_file = '/mnt/a99/d0/shriramsb/code/Alan Walker - Alone.mp3'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['learning_rate']) for h in hparams])\n",
    "dropout_scatter = [h['dropout_hidden_prob'] for h in hparams]\n",
    "colors = []\n",
    "for i in range(len(hparams)):\n",
    "    cur_hparam_tuple = tuner.hparamsDictToTuple(hparams[i], tuner.tuner_hparams)\n",
    "    colors.append(tuner.results_list[t][cur_hparam_tuple]['best_avg'])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_avg_updates = cur_res['best_avg_updates']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_avg_updates))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_avg_updates // updates_per_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][0], color='b', )\n",
    "plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tasks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "learning_rates = [(((49, 1e-1), (63, 1e-1 / 5), 1e-1 / (5 * 5)), (1e-1, ))]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(0, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "learning_rates = [(((20, 1e-1), (30, 1e-1 / 5), 1e-1 / 25), ((10, 1e-2), (20, 1e-2 / 5), 1e-2 / 25))]\n",
    "momentums = [0.9]\n",
    "regs = [0.0001]\n",
    "dropout_input_probs = [1.0]\n",
    "dropout_hidden_probs = [0.9]\n",
    "prod = list(itertools.product(regs, dropout_input_probs, dropout_hidden_probs, momentums, learning_rates))\n",
    "hparams = []\n",
    "for hparams_tuple in prod:\n",
    "    cur_dict = {}\n",
    "    cur_dict['reg'] = hparams_tuple[0]\n",
    "    cur_dict['dropout_input_prob'] = hparams_tuple[2]\n",
    "    cur_dict['dropout_hidden_prob'] = hparams_tuple[2]\n",
    "    cur_dict['momentum'] = hparams_tuple[3]\n",
    "    cur_dict['learning_rate'] = hparams_tuple[4]\n",
    "    hparams.append(cur_dict)\n",
    "    \n",
    "for i in range(t, t + 1):\n",
    "    tuner.hparams_list[i] = hparams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(hparams)\n",
    "num_epochs = 40\n",
    "num_updates = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs\n",
    "num_epochs_bf = 30\n",
    "num_updates_bf = math.ceil(tuner.task_list[t].train.images.shape[0] / BATCH_SIZE) * num_epochs_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints_99_1_0/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=0.0001,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-26040\n",
      "Training with dropout_hidden_prob=0.9,dropout_input_prob=0.9,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=0.0001,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=1\n",
      "Restoring paramters from dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=0.0001,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints_99_1_0/dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=0.0001,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=0.ckpt-26040\n",
      "epoch: 1, iter: 0/4, validation accuracies: [0.0989899  0.89999998], average train loss: 4.441552, average train accuracy: 0.402344\n",
      "epoch: 2, iter: 0/4, validation accuracies: [0.26010101 0.        ], average train loss: 3.828120, average train accuracy: 0.511719\n",
      "epoch: 3, iter: 0/4, validation accuracies: [0.19292929 0.75      ], average train loss: 1.900216, average train accuracy: 0.623047\n",
      "epoch: 4, iter: 0/4, validation accuracies: [0.20353536 0.15000001], average train loss: 2.062253, average train accuracy: 0.609375\n",
      "epoch: 5, iter: 0/4, validation accuracies: [0.2010101  0.85000002], average train loss: 1.499821, average train accuracy: 0.605469\n",
      "epoch: 6, iter: 0/4, validation accuracies: [0.21717172 0.69999999], average train loss: 1.024874, average train accuracy: 0.707031\n",
      "epoch: 7, iter: 0/4, validation accuracies: [0.20050505 0.94999999], average train loss: 0.780469, average train accuracy: 0.767578\n",
      "epoch: 8, iter: 0/4, validation accuracies: [0.22929293 0.94999999], average train loss: 0.700923, average train accuracy: 0.775391\n",
      "epoch: 9, iter: 0/4, validation accuracies: [0.24747475 0.94999999], average train loss: 0.595769, average train accuracy: 0.820312\n",
      "epoch: 10, iter: 0/4, validation accuracies: [0.20808081 1.        ], average train loss: 0.484709, average train accuracy: 0.851562\n",
      "epoch: 11, iter: 0/4, validation accuracies: [0.2030303 1.       ], average train loss: 0.459454, average train accuracy: 0.845703\n",
      "epoch: 12, iter: 0/4, validation accuracies: [0.23535353 1.        ], average train loss: 0.332928, average train accuracy: 0.886719\n",
      "epoch: 13, iter: 0/4, validation accuracies: [0.25454545 0.94999999], average train loss: 0.357976, average train accuracy: 0.892578\n",
      "epoch: 14, iter: 0/4, validation accuracies: [0.2520202 1.       ], average train loss: 0.302377, average train accuracy: 0.908203\n",
      "epoch: 15, iter: 0/4, validation accuracies: [0.26515151 0.94999999], average train loss: 0.258285, average train accuracy: 0.925781\n",
      "epoch: 16, iter: 0/4, validation accuracies: [0.23484848 0.94999999], average train loss: 0.210761, average train accuracy: 0.937500\n",
      "epoch: 17, iter: 0/4, validation accuracies: [0.24242425 0.94999999], average train loss: 0.166348, average train accuracy: 0.947266\n",
      "epoch: 18, iter: 0/4, validation accuracies: [0.21717172 1.        ], average train loss: 0.139663, average train accuracy: 0.964844\n",
      "epoch: 19, iter: 0/4, validation accuracies: [0.24343434 1.        ], average train loss: 0.132844, average train accuracy: 0.955078\n",
      "epoch: 20, iter: 0/4, validation accuracies: [0.26717172 0.89999998], average train loss: 0.121083, average train accuracy: 0.968750\n",
      "epoch: 21, iter: 0/4, validation accuracies: [0.26666667 0.94999999], average train loss: 0.105556, average train accuracy: 0.970703\n",
      "epoch: 22, iter: 0/4, validation accuracies: [0.25707071 0.94999999], average train loss: 0.080330, average train accuracy: 0.996094\n",
      "epoch: 23, iter: 0/4, validation accuracies: [0.2580808  0.94999999], average train loss: 0.112080, average train accuracy: 0.974609\n",
      "epoch: 24, iter: 0/4, validation accuracies: [0.25505051 0.94999999], average train loss: 0.108317, average train accuracy: 0.964844\n",
      "epoch: 25, iter: 0/4, validation accuracies: [0.25858586 0.94999999], average train loss: 0.088744, average train accuracy: 0.976562\n",
      "epoch: 26, iter: 0/4, validation accuracies: [0.25606061 0.94999999], average train loss: 0.091992, average train accuracy: 0.980469\n",
      "epoch: 27, iter: 0/4, validation accuracies: [0.2560606  0.94999999], average train loss: 0.102618, average train accuracy: 0.962891\n",
      "epoch: 28, iter: 0/4, validation accuracies: [0.2590909  0.94999999], average train loss: 0.073938, average train accuracy: 0.986328\n",
      "epoch: 29, iter: 0/4, validation accuracies: [0.25959595 0.94999999], average train loss: 0.083414, average train accuracy: 0.986328\n",
      "epoch: 30, iter: 0/4, validation accuracies: [0.26363637 0.94999999], average train loss: 0.083704, average train accuracy: 0.978516\n",
      "epoch: 31, iter: 0/4, validation accuracies: [0.26313131 0.94999999], average train loss: 0.095183, average train accuracy: 0.968750\n",
      "epoch: 32, iter: 0/4, validation accuracies: [0.26363637 0.94999999], average train loss: 0.066416, average train accuracy: 0.980469\n",
      "epoch: 33, iter: 0/4, validation accuracies: [0.26363636 0.94999999], average train loss: 0.066095, average train accuracy: 0.978516\n",
      "epoch: 34, iter: 0/4, validation accuracies: [0.26464647 0.94999999], average train loss: 0.077085, average train accuracy: 0.980469\n",
      "epoch: 35, iter: 0/4, validation accuracies: [0.26666666 0.94999999], average train loss: 0.068399, average train accuracy: 0.984375\n",
      "epoch: 36, iter: 0/4, validation accuracies: [0.26212122 0.94999999], average train loss: 0.079527, average train accuracy: 0.976562\n",
      "epoch: 37, iter: 0/4, validation accuracies: [0.260101   0.94999999], average train loss: 0.059197, average train accuracy: 0.982422\n",
      "epoch: 38, iter: 0/4, validation accuracies: [0.25959596 0.94999999], average train loss: 0.091269, average train accuracy: 0.970703\n",
      "epoch: 39, iter: 0/4, validation accuracies: [0.25959596 0.94999999], average train loss: 0.068412, average train accuracy: 0.982422\n",
      "epoch: 40, iter: 0/4, validation accuracies: [0.260101   0.94999999], average train loss: 0.065381, average train accuracy: 0.990234\n",
      "epochs: 40.000000, final train loss: 0.087334, validation accuracies: [0.260101   0.94999999]\n",
      "best epochs: 21.000000, best_avg: 0.273500, validation accuracies: [0.26666667 0.94999999]\n",
      "Training with dropout_hidden_prob=0.9,dropout_input_prob=0.9,fisher_multiplier=0.0,learning_rate=too_long,momentum=0.9,reg=0.0001,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=1\n",
      "epoch: 1, iter: 0/4, validation accuracies: [0.32626263 0.89999998], average train loss: 3.933922, average train accuracy: 0.253906\n",
      "epoch: 2, iter: 0/4, validation accuracies: [0.3939394  0.60000002], average train loss: 2.592639, average train accuracy: 0.382812\n",
      "epoch: 3, iter: 0/4, validation accuracies: [0.43888889 0.25      ], average train loss: 1.913069, average train accuracy: 0.523438\n",
      "epoch: 4, iter: 0/4, validation accuracies: [0.46262627 0.1       ], average train loss: 1.544729, average train accuracy: 0.556641\n",
      "epoch: 5, iter: 0/4, validation accuracies: [0.47676767 0.05      ], average train loss: 1.226603, average train accuracy: 0.634766\n",
      "epoch: 6, iter: 0/4, validation accuracies: [0.48787879 0.05      ], average train loss: 1.203019, average train accuracy: 0.675781\n",
      "epoch: 7, iter: 0/4, validation accuracies: [0.50656565 0.        ], average train loss: 1.244783, average train accuracy: 0.646484\n",
      "epoch: 8, iter: 0/4, validation accuracies: [0.52020202 0.        ], average train loss: 1.052738, average train accuracy: 0.685547\n",
      "epoch: 9, iter: 0/4, validation accuracies: [0.52878789 0.05      ], average train loss: 0.997778, average train accuracy: 0.714844\n",
      "epoch: 10, iter: 0/4, validation accuracies: [0.54343435 0.05      ], average train loss: 1.014645, average train accuracy: 0.683594\n",
      "epoch: 11, iter: 0/4, validation accuracies: [0.54646465 0.05      ], average train loss: 0.912562, average train accuracy: 0.728516\n",
      "epoch: 12, iter: 0/4, validation accuracies: [0.54595958 0.05      ], average train loss: 0.860777, average train accuracy: 0.736328\n",
      "epoch: 13, iter: 0/4, validation accuracies: [0.54747474 0.05      ], average train loss: 0.842739, average train accuracy: 0.753906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, iter: 0/4, validation accuracies: [0.54797979 0.05      ], average train loss: 0.952109, average train accuracy: 0.699219\n",
      "epoch: 15, iter: 0/4, validation accuracies: [0.5469697 0.05     ], average train loss: 0.884039, average train accuracy: 0.750000\n",
      "epoch: 16, iter: 0/4, validation accuracies: [0.54494951 0.05      ], average train loss: 0.882967, average train accuracy: 0.740234\n",
      "epoch: 17, iter: 0/4, validation accuracies: [0.54646465 0.1       ], average train loss: 0.776057, average train accuracy: 0.765625\n",
      "epoch: 18, iter: 0/4, validation accuracies: [0.54494949 0.05      ], average train loss: 0.849611, average train accuracy: 0.716797\n",
      "epoch: 19, iter: 0/4, validation accuracies: [0.54747474 0.05      ], average train loss: 0.890114, average train accuracy: 0.716797\n",
      "epoch: 20, iter: 0/4, validation accuracies: [0.55000001 0.05      ], average train loss: 0.840905, average train accuracy: 0.753906\n",
      "epoch: 21, iter: 0/4, validation accuracies: [0.5510101 0.05     ], average train loss: 0.898837, average train accuracy: 0.746094\n",
      "epoch: 22, iter: 0/4, validation accuracies: [0.54797979 0.05      ], average train loss: 0.877695, average train accuracy: 0.750000\n",
      "epoch: 23, iter: 0/4, validation accuracies: [0.55151516 0.05      ], average train loss: 0.931224, average train accuracy: 0.732422\n",
      "epoch: 24, iter: 0/4, validation accuracies: [0.55000001 0.05      ], average train loss: 0.890019, average train accuracy: 0.744141\n",
      "epoch: 25, iter: 0/4, validation accuracies: [0.55151515 0.05      ], average train loss: 0.816240, average train accuracy: 0.757812\n",
      "epoch: 26, iter: 0/4, validation accuracies: [0.5510101 0.05     ], average train loss: 0.863970, average train accuracy: 0.751953\n",
      "epoch: 27, iter: 0/4, validation accuracies: [0.55151515 0.05      ], average train loss: 0.897648, average train accuracy: 0.736328\n",
      "epoch: 28, iter: 0/4, validation accuracies: [0.55151515 0.05      ], average train loss: 0.811805, average train accuracy: 0.757812\n",
      "epoch: 29, iter: 0/4, validation accuracies: [0.55000001 0.05      ], average train loss: 0.857659, average train accuracy: 0.740234\n",
      "epoch: 30, iter: 0/4, validation accuracies: [0.54949495 0.05      ], average train loss: 0.897749, average train accuracy: 0.720703\n",
      "epochs: 30.000000, final train loss: 0.767730, validation accuracies: [0.54949495 0.05      ]\n",
      "best epochs: 23.000000, best_avg: 0.546500, validation accuracies: [0.55151516 0.05      ]\n",
      "saving model dropout_hidden_prob=0.9,dropout_input_prob=0.9,learning_rate=too_long,momentum=0.9,reg=0.0001,bf_num_images=2000,mask_softmax=True,old:new=1.0,task=1 at time step 280\n",
      "calculating penultimate output...\n",
      "time taken: %f 3.582606792449951\n",
      "saving penultimate output...\n"
     ]
    }
   ],
   "source": [
    "best_avg, best_hparams_index = tuner.tuneTasksInRange(1, t, BATCH_SIZE, num_hparams, \n",
    "                                                        num_updates=num_updates, verbose=True, \n",
    "                                                        random_crop_flip=True, \n",
    "                                                        is_sampling_reweighing=False, \n",
    "                                                        do_bf_finetuning=True, num_updates_bf=num_updates_bf, \n",
    "                                                        bf_only_penultimate_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "best_hparams_tuple = tuner.hparamsDictToTuple(hparams[0], tuner.tuner_hparams)\n",
    "cur_res = tuner.results_list[t][best_hparams_tuple]\n",
    "x = np.arange(0, cur_res['total_updates'], cur_res['updates_per_epoch'])\n",
    "cur_best_avg = cur_res['best_avg']\n",
    "cur_best_epoch = cur_res['best_epoch']\n",
    "updates_per_epoch = cur_res['updates_per_epoch']\n",
    "# print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "print(\"cur_best_avg: %e, num_updates: %d\" % (cur_best_avg, cur_best_epoch))\n",
    "print(\"best val_acc: %s\" % (str(np.array(cur_res['val_acc'])[:, (cur_best_epoch - 1) // tuner.eval_frequency])))\n",
    "# plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "plt.plot(cur_res['loss'], color='m')\n",
    "plt.plot(x, cur_res['val_loss'][-1], color='b')\n",
    "plt.show()\n",
    "# plt.ylim(ymin=0.9)\n",
    "plt.plot(cur_res['val_acc'][-1], color='b', )\n",
    "# plt.plot(cur_res['val_acc'][1], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weights given to each class\n",
    "num_class = 100\n",
    "for i in range(num_class):\n",
    "    print(i, np.sum(tuner.appended_task_list[-1].train.weights[(np.argmax(tuner.appended_task_list[-1].train.labels, axis=1) == i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.saveResultsList()\n",
    "tuner.saveBestHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "test_accuracies = []\n",
    "for i in range(t + 1):\n",
    "    accuracy = tuner.test(i, TEST_BATCH_SIZE, restore_model=True)\n",
    "    test_accuracies.append(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracies)\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r DB1_test_accuracies\n",
    "plt.plot(test_accuracies, color='g')\n",
    "plt.plot(DB1_test_accuracies, color='b')\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tuner.appended_task_list[-1].train.weights[0 : tuner.appended_task_list[-2].train.weights.shape[0]], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "filename='code_state.bak'\n",
    "my_shelf = shelve.open(filename,'n') # 'n' for new\n",
    "\n",
    "for key in dir():\n",
    "    try:\n",
    "        my_shelf[key] = globals()[key]\n",
    "    except TypeError:\n",
    "        #\n",
    "        # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "        #\n",
    "        print('ERROR shelving: {0}'.format(key))\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
