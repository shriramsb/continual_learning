{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "\n",
    "from tensorflow.contrib import tpu\n",
    "from tensorflow.contrib.cluster_resolver import TPUClusterResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 2\n",
    "hidden_units = 500\n",
    "num_perms = 2\n",
    "trials = 10\n",
    "epochs = 10\n",
    "task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "checkpoint_path = task_home + 'logs/checkpoints/'\n",
    "summaries_path = task_home + 'logs/summaries/'\n",
    "data_path = task_home + 'MNIST_data/'\n",
    "tpu_name = 'gectpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x03\\x02\\x02\\x02\\x10\\x01\\x18\\x08\"\\x18\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x01\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x01\\x01\\x00\\x01\\x01\\x01'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "sess = tf.Session(tpu_cluster)\n",
    "sess.run(tpu.initialize_system())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sbshriram16/continual_learning/EWC_disjointMNIST/tuner.py:119: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/sbshriram16/tf_venv/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/sbshriram16/tf_venv/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sbshriram16/tf_venv/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sbshriram16/tf_venv/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sbshriram16/tf_venv/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/sbshriram16/continual_learning/EWC_disjointMNIST/classifiers.py:86: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, hidden_layers=hidden_layers, hidden_units=hidden_units,\n",
    "                                        num_perms=num_perms, trials=trials, epochs=epochs,\n",
    "                                        checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                                        data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layers=2,hidden=500,lr=0.00094,multiplier=1067.63,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00094,multiplier=1067.63,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00094,multiplier=1067.63,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00094,multiplier=1067.63,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00038,multiplier=2599.17,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00038,multiplier=2599.17,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00038,multiplier=2599.17,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00038,multiplier=2599.17,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00027,multiplier=3766.10,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00027,multiplier=3766.10,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00027,multiplier=3766.10,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00027,multiplier=3766.10,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00028,multiplier=3519.83,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00028,multiplier=3519.83,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00028,multiplier=3519.83,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00028,multiplier=3519.83,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00061,multiplier=1636.79,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00061,multiplier=1636.79,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00061,multiplier=1636.79,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00061,multiplier=1636.79,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00064,multiplier=1572.35,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00064,multiplier=1572.35,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00064,multiplier=1572.35,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00064,multiplier=1572.35,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00097,multiplier=1032.99,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00097,multiplier=1032.99,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00097,multiplier=1032.99,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00097,multiplier=1032.99,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00069,multiplier=1453.79,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00069,multiplier=1453.79,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00069,multiplier=1453.79,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00069,multiplier=1453.79,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "training layers=2,hidden=500,lr=0.00069,multiplier=1453.04,mbsize=250,epochs=10,perm=0 with weights initialized at None\n",
      "saving model layers=2,hidden=500,lr=0.00069,multiplier=1453.04,mbsize=250,epochs=10,perm=0 at time step 1119\n",
      "finished training layers=2,hidden=500,lr=0.00069,multiplier=1453.04,mbsize=250,epochs=10,perm=0\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00069,multiplier=1453.04,mbsize=250,epochs=10,perm=0.ckpt-1119\n"
     ]
    }
   ],
   "source": [
    "# task 0\n",
    "t = 0\n",
    "queue = PriorityQueue()\n",
    "for learning_rate in tuner.trial_learning_rates:\n",
    "    tuner.train_on_task(t, learning_rate, queue)\n",
    "tuner.best_parameters.append(queue.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "task0 0.9910086\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "task1 0.0\n"
     ]
    }
   ],
   "source": [
    "# accuracy on validation sets\n",
    "t = 1\n",
    "tuner.best_parameters = []\n",
    "tuner.best_parameters.append((0.00077, 'layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0'))\n",
    "print(\"task0\", tuner.classifier.test(sess=tuner.sess, model_name=tuner.best_parameters[t - 1][1], batch_xs=tuner.task_list[0].validation.images, batch_ys=tuner.task_list[0].validation.labels))\n",
    "print(\"task1\", tuner.classifier.test(sess=tuner.sess, model_name=tuner.best_parameters[t - 1][1], batch_xs=tuner.task_list[1].validation.images, batch_ys=tuner.task_list[1].validation.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1\n",
    "t = 1\n",
    "\n",
    "N_learning_rates = 10\n",
    "learning_rates = 10.0 ** np.arange(-9, 2)\n",
    "# lr = 0.00094\n",
    "# 1e19 works good - [62, 72] for 15000\n",
    "#1e22 - [69, 68] for 15000\n",
    "# fisher_multiplier = 1e24\n",
    "fisher_multipliers = 10.0 ** np.arange(18, 28)\n",
    "dataset_train = tuner.task_list[t].train\n",
    "dataset_lagged = tuner.task_list[t - 1].train if t > 0 else None\n",
    "model_init_name = tuner.best_parameters[t - 1][1] if t > 0 else None\n",
    "MINI_BATCH_SIZE = 256\n",
    "LOG_FREQUENCY = 100\n",
    "dataset_train.initialize_iterator(MINI_BATCH_SIZE)\n",
    "if (dataset_lagged is not None):\n",
    "    dataset_lagged.initialize_iterator(MINI_BATCH_SIZE)\n",
    "\n",
    "eval_frequency = 100\n",
    "num_updates = 3000\n",
    "\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fisher_multiplier: 1.000000e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 49.730911\n",
      "loss with penalty: 0.724411, loss: 0.459475, val0 accuracy: 0.922205, val1 accuracy: 0.821867\n",
      "fisher_multiplier: 1.274275e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 53.957276\n",
      "loss with penalty: 0.746294, loss: 0.446002, val0 accuracy: 0.926114, val1 accuracy: 0.819820\n",
      "fisher_multiplier: 1.623777e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 53.473317\n",
      "loss with penalty: 0.750146, loss: 0.413693, val0 accuracy: 0.933151, val1 accuracy: 0.816953\n",
      "fisher_multiplier: 2.069138e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 55.094959\n",
      "loss with penalty: 0.741282, loss: 0.368148, val0 accuracy: 0.919859, val1 accuracy: 0.832924\n",
      "fisher_multiplier: 2.636651e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 56.088342\n",
      "loss with penalty: 0.813579, loss: 0.412193, val0 accuracy: 0.901095, val1 accuracy: 0.857903\n",
      "fisher_multiplier: 3.359818e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 60.043911\n",
      "loss with penalty: 0.848077, loss: 0.419280, val0 accuracy: 0.901876, val1 accuracy: 0.865274\n",
      "fisher_multiplier: 4.281332e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 60.814298\n",
      "loss with penalty: 0.812297, loss: 0.355765, val0 accuracy: 0.895231, val1 accuracy: 0.876331\n",
      "fisher_multiplier: 5.455595e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 65.771620\n",
      "loss with penalty: 0.876312, loss: 0.400360, val0 accuracy: 0.910477, val1 accuracy: 0.868141\n",
      "fisher_multiplier: 6.951928e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 69.033237\n",
      "loss with penalty: 0.924385, loss: 0.418876, val0 accuracy: 0.899922, val1 accuracy: 0.881245\n",
      "fisher_multiplier: 8.858668e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 73.783682\n",
      "loss with penalty: 0.975514, loss: 0.462547, val0 accuracy: 0.910868, val1 accuracy: 0.871826\n",
      "fisher_multiplier: 1.128838e+05, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 80.382990\n",
      "loss with penalty: 0.981030, loss: 0.457103, val0 accuracy: 0.909695, val1 accuracy: 0.868550\n",
      "fisher_multiplier: 1.438450e+05, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 86.382956\n",
      "loss with penalty: 0.884371, loss: 0.356412, val0 accuracy: 0.917905, val1 accuracy: 0.855446\n",
      "fisher_multiplier: 1.832981e+05, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 87.719557\n",
      "loss with penalty: 0.987464, loss: 0.460803, val0 accuracy: 0.918686, val1 accuracy: 0.850123\n",
      "fisher_multiplier: 2.335721e+05, lr: 5.000000e-06\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6ebda63546a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                             \u001b[0mmodel_init_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_init_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                             \u001b[0mfisher_multiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfisher_multiplier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                             learning_rate=lr)\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/continual_learning/EWC_disjointMNIST/classifiers.py\u001b[0m in \u001b[0;36mprepare_for_training\u001b[0;34m(self, sess, model_name, model_init_name, fisher_multiplier, learning_rate)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_init_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisher_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfisher_multiplier\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_init_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/venv/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\u001b[0m\n\u001b[1;32m    365\u001b[0m       event_writer = EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[1;32m    366\u001b[0m                                      filename_suffix)\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/venv/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0;31m# graph may itself be a graph_def due to positional arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m       maybe_graph_as_def = (graph.as_graph_def(add_shapes=True)\n\u001b[0;32m---> 87\u001b[0;31m                             if isinstance(graph, ops.Graph) else graph)\n\u001b[0m\u001b[1;32m     88\u001b[0m       self.add_meta_graph(\n\u001b[1;32m     89\u001b[0m           meta_graph.create_meta_graph_def(graph_def=graph_def or\n",
      "\u001b[0;32m~/tf_venv/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3248\u001b[0m     \"\"\"\n\u001b[1;32m   3249\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3190\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m           \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GraphToGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m           \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_updates = 5000\n",
    "fisher_multipliers = np.logspace(4, 6, 20)\n",
    "learning_rates = [5e-6]\n",
    "best_avg = 0.0\n",
    "best_params = -1\n",
    "for fisher_multiplier in fisher_multipliers:\n",
    "    for lr in learning_rates:\n",
    "        print(\"fisher_multiplier: %e, lr: %e\" % (fisher_multiplier, lr))\n",
    "        start_time = time.time()\n",
    "        model_name = tuner.file_name(lr, t)\n",
    "        tuner.classifier.prepare_for_training(sess=tuner.sess, \n",
    "                                            model_name=model_name, \n",
    "                                            model_init_name=model_init_name, \n",
    "                                            fisher_multiplier=fisher_multiplier, \n",
    "                                            learning_rate=lr)\n",
    "        val_acc = [[], []]\n",
    "        val_loss = [[], []]\n",
    "        loss = []\n",
    "        loss_with_penalty = []\n",
    "        cur_best_avg = 0.0\n",
    "        cur_best_avg_num_updates = -1\n",
    "        i = 0\n",
    "        count_not_improving = 0\n",
    "        while (True):\n",
    "            cur_loss, cur_loss_with_penalty = tuner.classifier.minibatch_sgd(tuner.sess, i, dataset_train, MINI_BATCH_SIZE, LOG_FREQUENCY)\n",
    "            loss.append(cur_loss)\n",
    "            loss_with_penalty.append(cur_loss_with_penalty)\n",
    "            if (i % eval_frequency == 0):\n",
    "                cur_iter_avg = 0.0\n",
    "                for j in range(num_perms):\n",
    "                    val_data = tuner.task_list[j].validation\n",
    "                    feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "                    accuracy = sess.run([tuner.classifier.loss, tuner.classifier.accuracy], feed_dict=feed_dict)\n",
    "                    val_loss[j].append(accuracy[0])\n",
    "                    val_acc[j].append(accuracy[1])\n",
    "                    cur_iter_avg += accuracy[1]\n",
    "                cur_iter_avg /= num_perms\n",
    "                \n",
    "                if (val_acc[0][-1] < val_acc[1][-1]):\n",
    "                    if (cur_best_avg >= cur_iter_avg):\n",
    "                        count_not_improving += 1\n",
    "                    else:\n",
    "                        count_not_improving = 0\n",
    "                        \n",
    "                if (cur_iter_avg > cur_best_avg):\n",
    "                    cur_best_avg = cur_iter_avg\n",
    "                    cur_best_avg_num_updates = i\n",
    "                \n",
    "                if (count_not_improving >= 5):\n",
    "                    break\n",
    "            i += 1\n",
    "        result[(fisher_multiplier, lr)] = {}\n",
    "        result[(fisher_multiplier, lr)][\"val_acc\"] = val_acc\n",
    "        result[(fisher_multiplier, lr)][\"val_loss\"] = val_loss\n",
    "        result[(fisher_multiplier, lr)][\"loss\"] = loss\n",
    "        result[(fisher_multiplier, lr)][\"loss_with_penalty\"] = loss_with_penalty\n",
    "        result[(fisher_multiplier, lr)][\"best_avg\"] = (cur_best_avg, cur_best_avg_num_updates)\n",
    "        if (best_avg < cur_best_avg):\n",
    "            best_avg = cur_best_avg\n",
    "            best_params = (fisher_multiplier, lr)\n",
    "        print(\"time taken: %f\" % (time.time() - start_time))\n",
    "        print(\"loss with penalty: %f, loss: %f, val0 accuracy: %f, val1 accuracy: %f\"\n",
    "              % (loss_with_penalty[-1], loss[-1], \n",
    "                 val_acc[0][cur_best_avg_num_updates // eval_frequency], val_acc[1][cur_best_avg_num_updates // eval_frequency]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = (10 ** (18 / 19 + 4), 5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting gs://continual_learning/permMNIST_EWC/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tuner_before_training = HyperparameterTuner(sess=sess, hidden_layers=hidden_layers, hidden_units=hidden_units,\n",
    "                                            num_perms=num_perms, trials=trials, epochs=epochs,\n",
    "                                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                                            data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fisher_multiplier: 8.858668e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "[0 4 1 ... 2 1 2]\n",
      "[5 9 5 ... 7 8 7]\n",
      "  0   0   1   2   3   4   5   6   7   8   9 \n",
      "  0 477   0   0   1   1   0   0   0   0   0 \n",
      "  1   0 558   2   1   2   0   0   0   0   0 \n",
      "  2   4   3 480   1   0   0   0   0   0   0 \n",
      "  3   0   0   4 488   1   0   0   0   0   0 \n",
      "  4   0   2   1   0 532   0   0   0   0   0 \n",
      "  5  46  20   4 292  72   0   0   0   0   0 \n",
      "  6  53  35 171  13 229   0   0   0   0   0 \n",
      "  7  42  27 100 213 168   0   0   0   0   0 \n",
      "  8  13  81  68 266  34   0   0   0   0   0 \n",
      "  9   4   5   2  14 470   0   0   0   0   0 \n"
     ]
    }
   ],
   "source": [
    "# confusion matrix before training\n",
    "# train on best hyperparameters\n",
    "fisher_multiplier, lr = best_params\n",
    "\n",
    "print(\"fisher_multiplier: %e, lr: %e\" % (fisher_multiplier, lr))\n",
    "start_time = time.time()\n",
    "model_name = tuner.file_name(lr, t)\n",
    "tuner.classifier.prepare_for_training(sess=tuner.sess, \n",
    "                                    model_name=model_name, \n",
    "                                    model_init_name=model_init_name, \n",
    "                                    fisher_multiplier=fisher_multiplier, \n",
    "                                    learning_rate=lr)\n",
    "\n",
    "num_labels = 10\n",
    "pred = np.array([])\n",
    "actual = np.array([])\n",
    "for j in range(num_perms):\n",
    "    val_data = tuner.task_list[j].validation\n",
    "    feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "    cur_scores, cur_y = tuner.classifier.get_confusion_matrix(sess, feed_dict)\n",
    "    cur_pred = np.argmax(cur_scores, 1)\n",
    "    cur_actual = np.argmax(cur_y, 1)\n",
    "    print(cur_actual)\n",
    "    actual = np.concatenate([actual, cur_actual])\n",
    "    pred = np.concatenate([pred, cur_pred])\n",
    "confusion_matrix = np.zeros((num_labels,num_labels), dtype=np.int64)\n",
    "\n",
    "for i in range(actual.shape[0]):\n",
    "    confusion_matrix[int(actual[i]), int(pred[i])] += 1\n",
    "\n",
    "print(\"%3d\" % (0, ), end=' ')\n",
    "for j in range(confusion_matrix.shape[1]):\n",
    "    print(\"%3d\" % (j, ), end=' ')\n",
    "print(\"\")\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    print(\"%3d\" % (i, ), end=' ')\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        print(\"%3d\" % (confusion_matrix[i][j], ), end= ' ')\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fisher_multiplier: 8.858668e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 71.609308\n",
      "loss with penalty: 0.950625, loss: 0.438365, val0 accuracy: 0.910477, val1 accuracy: 0.871826\n"
     ]
    }
   ],
   "source": [
    "# train on best hyperparameters\n",
    "fisher_multiplier, lr = best_params\n",
    "\n",
    "print(\"fisher_multiplier: %e, lr: %e\" % (fisher_multiplier, lr))\n",
    "start_time = time.time()\n",
    "model_name = tuner.file_name(lr, t)\n",
    "tuner.classifier.prepare_for_training(sess=tuner.sess, \n",
    "                                    model_name=model_name, \n",
    "                                    model_init_name=model_init_name, \n",
    "                                    fisher_multiplier=fisher_multiplier, \n",
    "                                    learning_rate=lr)\n",
    "val_acc = [[], []]\n",
    "val_loss = [[], []]\n",
    "loss = []\n",
    "loss_with_penalty = []\n",
    "cur_best_avg = 0.0\n",
    "cur_best_avg_num_updates = -1\n",
    "i = 0\n",
    "count_not_improving = 0\n",
    "while (True):\n",
    "    cur_loss, cur_loss_with_penalty = tuner.classifier.minibatch_sgd(tuner.sess, i, dataset_train, MINI_BATCH_SIZE, LOG_FREQUENCY)\n",
    "    loss.append(cur_loss)\n",
    "    loss_with_penalty.append(cur_loss_with_penalty)\n",
    "    if (i % eval_frequency == 0):\n",
    "        cur_iter_avg = 0.0\n",
    "        for j in range(num_perms):\n",
    "            val_data = tuner.task_list[j].validation\n",
    "            feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "            accuracy = sess.run([tuner.classifier.loss, tuner.classifier.accuracy], feed_dict=feed_dict)\n",
    "            val_loss[j].append(accuracy[0])\n",
    "            val_acc[j].append(accuracy[1])\n",
    "            cur_iter_avg += accuracy[1]\n",
    "        cur_iter_avg /= num_perms\n",
    "\n",
    "        if (val_acc[0][-1] < val_acc[1][-1]):\n",
    "            if (cur_best_avg >= cur_iter_avg):\n",
    "                count_not_improving += 1\n",
    "            else:\n",
    "                count_not_improving = 0\n",
    "\n",
    "        if (cur_iter_avg > cur_best_avg):\n",
    "            cur_best_avg = cur_iter_avg\n",
    "            cur_best_avg_num_updates = i\n",
    "\n",
    "        if (count_not_improving >= 5):\n",
    "            break\n",
    "    i += 1\n",
    "result[(fisher_multiplier, lr)] = {}\n",
    "result[(fisher_multiplier, lr)][\"val_acc\"] = val_acc\n",
    "result[(fisher_multiplier, lr)][\"val_loss\"] = val_loss\n",
    "result[(fisher_multiplier, lr)][\"loss\"] = loss\n",
    "result[(fisher_multiplier, lr)][\"loss_with_penalty\"] = loss_with_penalty\n",
    "result[(fisher_multiplier, lr)][\"best_avg\"] = (cur_best_avg, cur_best_avg_num_updates)\n",
    "print(\"time taken: %f\" % (time.time() - start_time))\n",
    "print(\"loss with penalty: %f, loss: %f, val0 accuracy: %f, val1 accuracy: %f\"\n",
    "      % (loss_with_penalty[-1], loss[-1], \n",
    "         val_acc[0][cur_best_avg_num_updates // eval_frequency], val_acc[1][cur_best_avg_num_updates // eval_frequency]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_avg: %e, best_params: %s\" % (best_avg, str(best_params)))\n",
    "\n",
    "for k, v in result.items():\n",
    "    cur_res = v\n",
    "    x = np.arange(0, len(cur_res['loss']), eval_frequency)\n",
    "    cur_best_avg = cur_res['best_avg']\n",
    "    print(\"fisher_multiplier: %e, lr: %e\" % (k[0], k[1]))\n",
    "    print(\"cur_best_avg: %e, num_updates: %e\" % (cur_best_avg[0], cur_best_avg[1]))\n",
    "    print(\"val0_acc: %e, val1_acc: %e\" %\n",
    "        (cur_res['val_acc'][0][cur_best_avg[1] // eval_frequency], cur_res['val_acc'][1][cur_best_avg[1] // eval_frequency]))\n",
    "    plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "    plt.plot(cur_res['loss'], color='m')\n",
    "    plt.plot(x, cur_res['val_loss'][1], color='b')\n",
    "    plt.show()\n",
    "    plt.plot(cur_res['val_acc'][0], color='b')\n",
    "    plt.plot(cur_res['val_acc'][1], color='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 ... 2 1 2]\n",
      "[5 9 5 ... 7 8 7]\n",
      "  0   0   1   2   3   4   5   6   7   8   9 \n",
      "  0 451   0   0   0   0  16   9   1   2   0 \n",
      "  1   0 552   1   0   1   1   2   1   5   0 \n",
      "  2   1   4 443   1   0   4  13   2  20   0 \n",
      "  3   0   1   3 395   0  53   0   8  29   4 \n",
      "  4   0   1   1   0 381   0  18   0   0 134 \n",
      "  5   1   3   1  12   0 378  11   2  18   8 \n",
      "  6   2   3   2   0   1   8 480   0   5   0 \n",
      "  7   4   5   7   0   3   5   7 495   2  22 \n",
      "  8   1   6   4   0   0  35   7   2 394  13 \n",
      "  9   2   4   1   3   1   6   5  15   8 450 \n"
     ]
    }
   ],
   "source": [
    "# confusion matrix \n",
    "# finding which digits are being confused by classifier\n",
    "num_labels = 10\n",
    "pred = np.array([])\n",
    "actual = np.array([])\n",
    "for j in range(num_perms):\n",
    "    val_data = tuner.task_list[j].validation\n",
    "    feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "    cur_scores, cur_y = tuner.classifier.get_confusion_matrix(sess, feed_dict)\n",
    "    cur_pred = np.argmax(cur_scores, 1)\n",
    "    cur_actual = np.argmax(cur_y, 1)\n",
    "    print(cur_actual)\n",
    "    actual = np.concatenate([actual, cur_actual])\n",
    "    pred = np.concatenate([pred, cur_pred])\n",
    "confusion_matrix = np.zeros((num_labels,num_labels), dtype=np.int64)\n",
    "\n",
    "for i in range(actual.shape[0]):\n",
    "    confusion_matrix[int(actual[i]), int(pred[i])] += 1\n",
    "\n",
    "print(\"%3d\" % (0, ), end=' ')\n",
    "for j in range(confusion_matrix.shape[1]):\n",
    "    print(\"%3d\" % (j, ), end=' ')\n",
    "print(\"\")\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    print(\"%3d\" % (i, ), end=' ')\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        print(\"%3d\" % (confusion_matrix[i][j], ), end= ' ')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tpu.shutdown_system())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
