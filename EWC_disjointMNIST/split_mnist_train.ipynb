{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tuner import HyperparameterTuner\n",
    "\n",
    "use_tpu = False\n",
    "\n",
    "if use_tpu:\n",
    "    from tensorflow.contrib import tpu\n",
    "    from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 2\n",
    "hidden_units = 500\n",
    "trials = 10\n",
    "epochs = 10\n",
    "task_home = ''\n",
    "if use_tpu:\n",
    "    task_home = 'gs://continual_learning/permMNIST_EWC/'\n",
    "else:\n",
    "    task_home = './'\n",
    "checkpoint_path = task_home + 'unequal_split_logs/checkpoints/'\n",
    "summaries_path = task_home + 'unequal_split_logs/summaries/'\n",
    "data_path = task_home + 'MNIST_data/'\n",
    "split_path = './split.txt' \n",
    "if use_tpu:\n",
    "    tpu_name = 'gectpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu_cluster = TPUClusterResolver(tpu=[tpu_name]).get_master()\n",
    "    sess = tf.Session(tpu_cluster)\n",
    "    sess.run(tpu.initialize_system())\n",
    "else:\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/shriram/Documents/Academics/7th_sem/BTP/continuous_learning/code/continual_learning/EWC_disjointMNIST/classifiers.py:94: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(sess=sess, hidden_layers=hidden_layers, hidden_units=hidden_units,\n",
    "                            trials=trials, epochs=epochs,\n",
    "                            checkpoint_path=checkpoint_path, summaries_path=summaries_path, \n",
    "                            data_path=data_path, split_path=split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layers=2,hidden=500,lr=0.00094,multiplier=1067.63,mbsize=250,epochs=10,perm=0 with weights initialized at None\n"
     ]
    }
   ],
   "source": [
    "# task 0\n",
    "t = 0\n",
    "queue = PriorityQueue()\n",
    "for learning_rate in tuner.trial_learning_rates:\n",
    "    tuner.train_on_task(t, learning_rate, queue)\n",
    "tuner.best_parameters.append(queue.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "task0 0.9910086\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "task1 0.0\n"
     ]
    }
   ],
   "source": [
    "# accuracy on validation sets\n",
    "t = 1\n",
    "print(\"task0\", tuner.classifier.test(sess=tuner.sess, model_name=tuner.best_parameters[t - 1][1], batch_xs=tuner.task_list[0].validation.images, batch_ys=tuner.task_list[0].validation.labels))\n",
    "print(\"task1\", tuner.classifier.test(sess=tuner.sess, model_name=tuner.best_parameters[t - 1][1], batch_xs=tuner.task_list[1].validation.images, batch_ys=tuner.task_list[1].validation.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1\n",
    "t = 1\n",
    "\n",
    "N_learning_rates = 10\n",
    "learning_rates = 10.0 ** np.arange(-9, 2)\n",
    "# lr = 0.00094\n",
    "# 1e19 works good - [62, 72] for 15000\n",
    "#1e22 - [69, 68] for 15000\n",
    "# fisher_multiplier = 1e24\n",
    "fisher_multipliers = 10.0 ** np.arange(18, 28)\n",
    "dataset_train = tuner.task_list[t].train\n",
    "dataset_lagged = tuner.task_list[t - 1].train if t > 0 else None\n",
    "model_init_name = tuner.best_parameters[t - 1][1] if t > 0 else None\n",
    "MINI_BATCH_SIZE = 256\n",
    "LOG_FREQUENCY = 100\n",
    "dataset_train.initialize_iterator(MINI_BATCH_SIZE)\n",
    "if (dataset_lagged is not None):\n",
    "    dataset_lagged.initialize_iterator(MINI_BATCH_SIZE)\n",
    "\n",
    "eval_frequency = 100\n",
    "num_updates = 3000\n",
    "\n",
    "result = {}\n",
    "dropout_input = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.500000, fisher_multiplier: 0.000000e+00, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 53.323640\n",
      "loss with penalty: 0.792711, loss: 0.792711, val0 accuracy: 0.910868, val1 accuracy: 0.791564\n",
      "dropout: 0.400000, fisher_multiplier: 0.000000e+00, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 57.301655\n",
      "loss with penalty: 1.191719, loss: 1.191719, val0 accuracy: 0.901095, val1 accuracy: 0.791155\n",
      "dropout: 0.300000, fisher_multiplier: 0.000000e+00, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 69.638132\n",
      "loss with penalty: 1.645593, loss: 1.645593, val0 accuracy: 0.903049, val1 accuracy: 0.773546\n",
      "dropout: 0.200000, fisher_multiplier: 0.000000e+00, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 100.189951\n",
      "loss with penalty: 1.850541, loss: 1.850541, val0 accuracy: 0.887412, val1 accuracy: 0.725225\n",
      "dropout: 0.100000, fisher_multiplier: 0.000000e+00, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 163.489267\n",
      "loss with penalty: 1.978940, loss: 1.978940, val0 accuracy: 0.881939, val1 accuracy: 0.452088\n"
     ]
    }
   ],
   "source": [
    "num_updates = 5000\n",
    "dropouts = [0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "# fisher_multipliers = np.logspace(1, 6, 50)\n",
    "fisher_multipliers = [0.0]\n",
    "learning_rates = [5e-6]\n",
    "best_avg = 0.0\n",
    "best_params = -1\n",
    "best_num_updates = -1\n",
    "for dropout in dropouts:\n",
    "    for fisher_multiplier in fisher_multipliers:\n",
    "        for lr in learning_rates:\n",
    "            print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (dropout, fisher_multiplier, lr))\n",
    "            start_time = time.time()\n",
    "            model_name = tuner.file_name(lr, t)\n",
    "            tuner.classifier.set_dropout(dropout_input, dropout)\n",
    "            tuner.classifier.prepare_for_training(sess=tuner.sess, \n",
    "                                                model_name=model_name, \n",
    "                                                model_init_name=model_init_name, \n",
    "                                                fisher_multiplier=fisher_multiplier, \n",
    "                                                learning_rate=lr)\n",
    "            val_acc = [[], []]\n",
    "            val_loss = [[], []]\n",
    "            loss = []\n",
    "            loss_with_penalty = []\n",
    "            cur_best_avg = 0.0\n",
    "            cur_best_avg_num_updates = -1\n",
    "            i = 0\n",
    "            count_not_improving = 0\n",
    "            while (True):\n",
    "                cur_loss, cur_loss_with_penalty = tuner.classifier.minibatch_sgd(tuner.sess, i, dataset_train, MINI_BATCH_SIZE, LOG_FREQUENCY)\n",
    "                loss.append(cur_loss)\n",
    "                loss_with_penalty.append(cur_loss_with_penalty)\n",
    "                if (i % eval_frequency == 0):\n",
    "                    cur_iter_avg = 0.0\n",
    "                    for j in range(num_perms):\n",
    "                        val_data = tuner.task_list[j].validation\n",
    "                        feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "                        accuracy = sess.run([tuner.classifier.loss, tuner.classifier.accuracy], feed_dict=feed_dict)\n",
    "                        val_loss[j].append(accuracy[0])\n",
    "                        val_acc[j].append(accuracy[1])\n",
    "                        cur_iter_avg += accuracy[1]\n",
    "                    cur_iter_avg /= num_perms\n",
    "\n",
    "                    if (val_acc[0][-1] < val_acc[1][-1]):\n",
    "                        if (cur_best_avg >= cur_iter_avg):\n",
    "                            count_not_improving += 1\n",
    "                        else:\n",
    "                            count_not_improving = 0\n",
    "\n",
    "                    if (cur_iter_avg > cur_best_avg):\n",
    "                        cur_best_avg = cur_iter_avg\n",
    "                        cur_best_avg_num_updates = i\n",
    "\n",
    "                    if (count_not_improving >= 5):\n",
    "                        break\n",
    "                i += 1\n",
    "        cur_params = (dropout, fisher_multiplier, lr)\n",
    "        result[cur_params] = {}\n",
    "        result[cur_params][\"val_acc\"] = val_acc\n",
    "        result[cur_params][\"val_loss\"] = val_loss\n",
    "        result[cur_params][\"loss\"] = loss\n",
    "        result[cur_params][\"loss_with_penalty\"] = loss_with_penalty\n",
    "        result[cur_params][\"best_avg\"] = (cur_best_avg, cur_best_avg_num_updates)\n",
    "        if (best_avg < cur_best_avg):\n",
    "            best_avg = cur_best_avg\n",
    "            best_params = cur_params\n",
    "            best_num_updates = cur_best_avg_num_updates\n",
    "        print(\"time taken: %f\" % (time.time() - start_time))\n",
    "        print(\"loss with penalty: %f, loss: %f, val0 accuracy: %f, val1 accuracy: %f\"\n",
    "              % (loss_with_penalty[-1], loss[-1], \n",
    "                 val_acc[0][cur_best_avg_num_updates // eval_frequency], val_acc[1][cur_best_avg_num_updates // eval_frequency]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_avg: %e, best_params: %s\" % (best_avg, str(best_params)))\n",
    "\n",
    "for k, v in result.items():\n",
    "    cur_res = v\n",
    "    x = np.arange(0, len(cur_res['loss']), eval_frequency)\n",
    "    cur_best_avg = cur_res['best_avg']\n",
    "    print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (k[0], k[1], k[2]))\n",
    "    print(\"cur_best_avg: %e, num_updates: %e\" % (cur_best_avg[0], cur_best_avg[1]))\n",
    "    print(\"val0_acc: %e, val1_acc: %e\" %\n",
    "        (cur_res['val_acc'][0][cur_best_avg[1] // eval_frequency], cur_res['val_acc'][1][cur_best_avg[1] // eval_frequency]))\n",
    "    plt.plot(cur_res['loss_with_penalty'], color='g')\n",
    "    plt.plot(cur_res['loss'], color='m')\n",
    "    plt.plot(x, cur_res['val_loss'][1], color='b')\n",
    "    plt.show()\n",
    "    plt.plot(cur_res['val_acc'][0], color='b')\n",
    "    plt.plot(cur_res['val_acc'][1], color='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(tuner):\n",
    "    num_labels = 10\n",
    "    pred = np.array([])\n",
    "    actual = np.array([])\n",
    "    for j in range(num_perms):\n",
    "        val_data = tuner.task_list[j].validation\n",
    "        feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "        cur_scores, cur_y = tuner.classifier.get_predictions(sess, feed_dict)\n",
    "        cur_pred = np.argmax(cur_scores, 1)\n",
    "        cur_actual = np.argmax(cur_y, 1)\n",
    "        print(cur_actual)\n",
    "        actual = np.concatenate([actual, cur_actual])\n",
    "        pred = np.concatenate([pred, cur_pred])\n",
    "    confusion_matrix = np.zeros((num_labels,num_labels), dtype=np.int64)\n",
    "\n",
    "    for i in range(actual.shape[0]):\n",
    "        confusion_matrix[int(actual[i]), int(pred[i])] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix):\n",
    "    print(\"%3d\" % (0, ), end=' ')\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        print(\"%3d\" % (j, ), end=' ')\n",
    "    print(\"\")\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        print(\"%3d\" % (i, ), end=' ')\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            print(\"%3d\" % (confusion_matrix[i][j], ), end= ' ')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "[0 4 1 ... 2 1 2]\n",
      "[5 9 5 ... 7 8 7]\n",
      "  0   0   1   2   3   4   5   6   7   8   9 \n",
      "  0 477   0   0   1   1   0   0   0   0   0 \n",
      "  1   0 558   2   1   2   0   0   0   0   0 \n",
      "  2   4   3 480   1   0   0   0   0   0   0 \n",
      "  3   0   0   4 488   1   0   0   0   0   0 \n",
      "  4   0   2   1   0 532   0   0   0   0   0 \n",
      "  5  46  20   4 292  72   0   0   0   0   0 \n",
      "  6  53  35 171  13 229   0   0   0   0   0 \n",
      "  7  42  27 100 213 168   0   0   0   0   0 \n",
      "  8  13  81  68 266  34   0   0   0   0   0 \n",
      "  9   4   5   2  14 470   0   0   0   0   0 \n"
     ]
    }
   ],
   "source": [
    "# confusion matrix before training\n",
    "# train on best hyperparameters\n",
    "\n",
    "# best_params = (1.0, 10 ** (18 / 19 + 4), 5e-6)\n",
    "# best_num_updates = 3600\n",
    "\n",
    "dropout, fisher_multiplier, lr = best_params\n",
    "\n",
    "model_name = tuner.file_name(lr, t)\n",
    "tuner.classifier.prepare_for_training(sess=tuner.sess, \n",
    "                                    model_name=model_name, \n",
    "                                    model_init_name=model_init_name, \n",
    "                                    fisher_multiplier=fisher_multiplier, \n",
    "                                    learning_rate=lr)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(tuner)\n",
    "print_confusion_matrix(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 1.000000, fisher_multiplier: 8.858668e+04, lr: 5.000000e-06\n",
      "INFO:tensorflow:Restoring parameters from gs://continual_learning/permMNIST_EWC/logs/checkpoints/layers=2,hidden=500,lr=0.00077,multiplier=1291.96,mbsize=250,epochs=10,perm=0.ckpt-1119\n",
      "time taken: 56.365972\n",
      "loss with penalty: 1.133778, loss: 0.486593, val0 accuracy: 0.919077, val1 accuracy: 0.860770\n"
     ]
    }
   ],
   "source": [
    "# train on best hyperparameters\n",
    "dropout, fisher_multiplier, lr = best_params\n",
    "print(\"dropout: %f, fisher_multiplier: %e, lr: %e\" % (dropout, fisher_multiplier, lr))\n",
    "start_time = time.time()\n",
    "model_name = tuner.file_name(lr, t)\n",
    "tuner.classifier.set_dropout(dropout_input, dropout)\n",
    "tuner.classifier.prepare_for_training(sess=tuner.sess, \n",
    "                                    model_name=model_name, \n",
    "                                    model_init_name=model_init_name, \n",
    "                                    fisher_multiplier=fisher_multiplier, \n",
    "                                    learning_rate=lr)\n",
    "val_acc = [[], []]\n",
    "val_loss = [[], []]\n",
    "loss = []\n",
    "loss_with_penalty = []\n",
    "cur_best_avg = 0.0\n",
    "cur_best_avg_num_updates = -1\n",
    "for i in range(best_num_updates):\n",
    "    cur_loss, cur_loss_with_penalty = tuner.classifier.minibatch_sgd(tuner.sess, i, dataset_train, MINI_BATCH_SIZE, LOG_FREQUENCY)\n",
    "    loss.append(cur_loss)\n",
    "    loss_with_penalty.append(cur_loss_with_penalty)\n",
    "    if (i % eval_frequency == 0):\n",
    "        cur_iter_avg = 0.0\n",
    "        for j in range(num_perms):\n",
    "            val_data = tuner.task_list[j].validation\n",
    "            feed_dict = tuner.classifier.create_feed_dict(val_data.images, val_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "            accuracy = sess.run([tuner.classifier.loss, tuner.classifier.accuracy], feed_dict=feed_dict)\n",
    "            val_loss[j].append(accuracy[0])\n",
    "            val_acc[j].append(accuracy[1])\n",
    "            cur_iter_avg += accuracy[1]\n",
    "        cur_iter_avg /= num_perms\n",
    "\n",
    "        if (cur_iter_avg > cur_best_avg):\n",
    "            cur_best_avg = cur_iter_avg\n",
    "            cur_best_avg_num_updates = i\n",
    "\n",
    "cur_params = (dropout, fisher_multiplier, lr)\n",
    "result[cur_params] = {}\n",
    "result[cur_params][\"val_acc\"] = val_acc\n",
    "result[cur_params][\"val_loss\"] = val_loss\n",
    "result[cur_params][\"loss\"] = loss\n",
    "result[cur_params][\"loss_with_penalty\"] = loss_with_penalty\n",
    "result[cur_params][\"best_avg\"] = (cur_best_avg, cur_best_avg_num_updates)\n",
    "print(\"time taken: %f\" % (time.time() - start_time))\n",
    "print(\"loss with penalty: %f, loss: %f, val0 accuracy: %f, val1 accuracy: %f\"\n",
    "      % (loss_with_penalty[-1], loss[-1], \n",
    "         val_acc[0][cur_best_avg_num_updates // eval_frequency], val_acc[1][cur_best_avg_num_updates // eval_frequency]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 ... 2 1 2]\n",
      "[5 9 5 ... 7 8 7]\n",
      "  0   0   1   2   3   4   5   6   7   8   9 \n",
      "  0 460   0   0   0   0  10   7   1   1   0 \n",
      "  1   0 555   1   0   1   1   0   1   3   1 \n",
      "  2   2   4 463   1   0   1   7   1   9   0 \n",
      "  3   0   1   4 419   0  42   1   4  19   3 \n",
      "  4   0   2   2   0 432   0  13   0   0  86 \n",
      "  5   5   6   2  20   1 355  13   3  18  11 \n",
      "  6   4   6   5   0   2   9 470   0   4   1 \n",
      "  7   9   6  14   0   6   6   6 473   7  23 \n",
      "  8   1   9   5   0   0  33   8   3 384  19 \n",
      "  9   3   5   1   3   3   7   5  15   6 447 \n"
     ]
    }
   ],
   "source": [
    "# confusion matrix \n",
    "# finding which digits are being confused by classifier\n",
    "confusion_matrix = get_confusion_matrix(tuner)\n",
    "print_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30288878, 0.89920217]\n",
      "[0.53332376, 0.85969967]\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "for j in range(num_perms):\n",
    "    test_data = tuner.task_list[j].test\n",
    "    feed_dict = tuner.classifier.create_feed_dict(test_data.images, test_data.labels, keep_input=1.0, keep_hidden=1.0)\n",
    "    accuracy = sess.run([tuner.classifier.loss, tuner.classifier.accuracy], feed_dict=feed_dict)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    sess.run(tpu.shutdown_system())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
