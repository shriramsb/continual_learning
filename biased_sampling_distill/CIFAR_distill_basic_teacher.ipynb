{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and limit GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import networks_cifar as networks \n",
    "import utils\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True    # set use_gpu to True if system has gpu\n",
    "gpu_id = 1        # id of gpu to be used\n",
    "cpu_device = torch.device('cpu')\n",
    "# fast_device is where computation (training, inference) happens\n",
    "fast_device = torch.device('cpu')\n",
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'    # set visible devices depending on system configuration\n",
    "    fast_device = torch.device('cuda:' + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducibilitySeed():\n",
    "    \"\"\"\n",
    "    Ensure reproducibility of results; Seeds to 0\n",
    "    \"\"\"\n",
    "    torch_init_seed = 0\n",
    "    torch.manual_seed(torch_init_seed)\n",
    "    numpy_init_seed = 0\n",
    "    np.random.seed(numpy_init_seed)\n",
    "    if use_gpu:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reproducibilitySeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "cifar_image_shape = (32, 32, 3)\n",
    "random_pad_size = 2\n",
    "# Training images augmented by randomly shifting images by at max. 2 pixels in any of 4 directions\n",
    "transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomCrop(cifar_image_shape[ :-1], random_pad_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "train_val_dataset = torchvision.datasets.CIFAR10(root='./CIFAR_dataset/', train=True, \n",
    "                                            download=True, transform=transform_train)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR_dataset/', train=False, \n",
    "                                            download=True, transform=transform_test)\n",
    "\n",
    "num_train = int(1.0 * len(train_val_dataset) * 95 / 100)\n",
    "num_val = len(train_val_dataset) - num_train\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
    "\n",
    "batch_size = 128\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 10\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "index = 17\n",
    "plt.imshow((train_dataset[index][0] * 0.5 + 0.5).numpy().transpose(1, 2, 0))\n",
    "print(classes[train_dataset[index][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train teacher network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = 'CIFAR_checkpoints_teacher/'\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "print_every = 100    # Interval size for which to print statistics of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=1e-05\n",
      "epoch: 0 validation loss: 2.305 validation accuracy: 0.096\n",
      "[1,   100/  372] train loss: 1.881 train accuracy: 0.352\n",
      "[1,   200/  372] train loss: 1.635 train accuracy: 0.391\n",
      "[1,   300/  372] train loss: 1.794 train accuracy: 0.438\n",
      "epoch: 1 validation loss: 1.715 validation accuracy: 0.388\n",
      "[2,   100/  372] train loss: 1.545 train accuracy: 0.461\n",
      "[2,   200/  372] train loss: 1.580 train accuracy: 0.422\n",
      "[2,   300/  372] train loss: 1.622 train accuracy: 0.500\n",
      "epoch: 2 validation loss: 1.584 validation accuracy: 0.446\n",
      "[3,   100/  372] train loss: 1.584 train accuracy: 0.477\n",
      "[3,   200/  372] train loss: 1.486 train accuracy: 0.469\n",
      "[3,   300/  372] train loss: 1.666 train accuracy: 0.469\n",
      "epoch: 3 validation loss: 1.520 validation accuracy: 0.477\n",
      "[4,   100/  372] train loss: 1.327 train accuracy: 0.531\n",
      "[4,   200/  372] train loss: 1.341 train accuracy: 0.492\n",
      "[4,   300/  372] train loss: 1.317 train accuracy: 0.562\n",
      "epoch: 4 validation loss: 1.484 validation accuracy: 0.466\n",
      "[5,   100/  372] train loss: 1.279 train accuracy: 0.523\n",
      "[5,   200/  372] train loss: 1.441 train accuracy: 0.500\n",
      "[5,   300/  372] train loss: 1.339 train accuracy: 0.500\n",
      "epoch: 5 validation loss: 1.449 validation accuracy: 0.496\n",
      "[6,   100/  372] train loss: 1.251 train accuracy: 0.641\n",
      "[6,   200/  372] train loss: 1.244 train accuracy: 0.539\n",
      "[6,   300/  372] train loss: 1.244 train accuracy: 0.570\n",
      "epoch: 6 validation loss: 1.416 validation accuracy: 0.517\n",
      "[7,   100/  372] train loss: 1.257 train accuracy: 0.602\n",
      "[7,   200/  372] train loss: 1.302 train accuracy: 0.570\n",
      "[7,   300/  372] train loss: 1.269 train accuracy: 0.508\n",
      "epoch: 7 validation loss: 1.398 validation accuracy: 0.516\n",
      "[8,   100/  372] train loss: 1.273 train accuracy: 0.547\n",
      "[8,   200/  372] train loss: 1.350 train accuracy: 0.492\n",
      "[8,   300/  372] train loss: 1.145 train accuracy: 0.594\n",
      "epoch: 8 validation loss: 1.375 validation accuracy: 0.535\n",
      "[9,   100/  372] train loss: 1.372 train accuracy: 0.469\n",
      "[9,   200/  372] train loss: 1.326 train accuracy: 0.508\n",
      "[9,   300/  372] train loss: 1.266 train accuracy: 0.570\n",
      "epoch: 9 validation loss: 1.349 validation accuracy: 0.538\n",
      "[10,   100/  372] train loss: 1.169 train accuracy: 0.586\n",
      "[10,   200/  372] train loss: 1.244 train accuracy: 0.539\n",
      "[10,   300/  372] train loss: 1.257 train accuracy: 0.555\n",
      "epoch: 10 validation loss: 1.343 validation accuracy: 0.546\n",
      "[11,   100/  372] train loss: 1.152 train accuracy: 0.578\n",
      "[11,   200/  372] train loss: 1.306 train accuracy: 0.531\n",
      "[11,   300/  372] train loss: 1.051 train accuracy: 0.656\n",
      "epoch: 11 validation loss: 1.347 validation accuracy: 0.524\n",
      "[12,   100/  372] train loss: 1.259 train accuracy: 0.562\n",
      "[12,   200/  372] train loss: 1.205 train accuracy: 0.609\n",
      "[12,   300/  372] train loss: 1.070 train accuracy: 0.617\n",
      "epoch: 12 validation loss: 1.319 validation accuracy: 0.554\n",
      "[13,   100/  372] train loss: 1.101 train accuracy: 0.641\n",
      "[13,   200/  372] train loss: 1.149 train accuracy: 0.555\n",
      "[13,   300/  372] train loss: 1.239 train accuracy: 0.578\n",
      "epoch: 13 validation loss: 1.324 validation accuracy: 0.540\n",
      "[14,   100/  372] train loss: 0.905 train accuracy: 0.648\n",
      "[14,   200/  372] train loss: 1.277 train accuracy: 0.492\n",
      "[14,   300/  372] train loss: 1.015 train accuracy: 0.539\n",
      "epoch: 14 validation loss: 1.299 validation accuracy: 0.560\n",
      "[15,   100/  372] train loss: 1.251 train accuracy: 0.547\n",
      "[15,   200/  372] train loss: 1.120 train accuracy: 0.609\n",
      "[15,   300/  372] train loss: 1.052 train accuracy: 0.594\n",
      "epoch: 15 validation loss: 1.299 validation accuracy: 0.547\n",
      "[16,   100/  372] train loss: 1.080 train accuracy: 0.594\n",
      "[16,   200/  372] train loss: 1.155 train accuracy: 0.641\n",
      "[16,   300/  372] train loss: 1.117 train accuracy: 0.578\n",
      "epoch: 16 validation loss: 1.295 validation accuracy: 0.557\n",
      "[17,   100/  372] train loss: 1.100 train accuracy: 0.602\n",
      "[17,   200/  372] train loss: 1.113 train accuracy: 0.633\n",
      "[17,   300/  372] train loss: 0.980 train accuracy: 0.625\n",
      "epoch: 17 validation loss: 1.284 validation accuracy: 0.558\n",
      "[18,   100/  372] train loss: 0.979 train accuracy: 0.633\n",
      "[18,   200/  372] train loss: 0.975 train accuracy: 0.656\n",
      "[18,   300/  372] train loss: 0.791 train accuracy: 0.734\n",
      "epoch: 18 validation loss: 1.289 validation accuracy: 0.570\n",
      "[19,   100/  372] train loss: 0.889 train accuracy: 0.711\n",
      "[19,   200/  372] train loss: 1.065 train accuracy: 0.609\n",
      "[19,   300/  372] train loss: 1.335 train accuracy: 0.484\n",
      "epoch: 19 validation loss: 1.272 validation accuracy: 0.560\n",
      "[20,   100/  372] train loss: 1.130 train accuracy: 0.570\n",
      "[20,   200/  372] train loss: 0.889 train accuracy: 0.727\n",
      "[20,   300/  372] train loss: 0.960 train accuracy: 0.695\n",
      "epoch: 20 validation loss: 1.266 validation accuracy: 0.574\n",
      "[21,   100/  372] train loss: 0.873 train accuracy: 0.664\n",
      "[21,   200/  372] train loss: 0.940 train accuracy: 0.680\n",
      "[21,   300/  372] train loss: 1.001 train accuracy: 0.625\n",
      "epoch: 21 validation loss: 1.266 validation accuracy: 0.564\n",
      "[22,   100/  372] train loss: 0.941 train accuracy: 0.672\n",
      "[22,   200/  372] train loss: 0.904 train accuracy: 0.656\n",
      "[22,   300/  372] train loss: 1.009 train accuracy: 0.641\n",
      "epoch: 22 validation loss: 1.252 validation accuracy: 0.573\n",
      "[23,   100/  372] train loss: 0.799 train accuracy: 0.734\n",
      "[23,   200/  372] train loss: 0.936 train accuracy: 0.633\n",
      "[23,   300/  372] train loss: 0.860 train accuracy: 0.734\n",
      "epoch: 23 validation loss: 1.247 validation accuracy: 0.578\n",
      "[24,   100/  372] train loss: 0.936 train accuracy: 0.672\n",
      "[24,   200/  372] train loss: 0.940 train accuracy: 0.664\n",
      "[24,   300/  372] train loss: 0.909 train accuracy: 0.734\n",
      "epoch: 24 validation loss: 1.224 validation accuracy: 0.584\n",
      "[25,   100/  372] train loss: 0.764 train accuracy: 0.695\n",
      "[25,   200/  372] train loss: 0.920 train accuracy: 0.648\n",
      "[25,   300/  372] train loss: 0.873 train accuracy: 0.680\n",
      "epoch: 25 validation loss: 1.284 validation accuracy: 0.566\n",
      "[26,   100/  372] train loss: 0.750 train accuracy: 0.750\n",
      "[26,   200/  372] train loss: 0.754 train accuracy: 0.750\n",
      "[26,   300/  372] train loss: 0.965 train accuracy: 0.664\n",
      "epoch: 26 validation loss: 1.236 validation accuracy: 0.585\n",
      "[27,   100/  372] train loss: 0.905 train accuracy: 0.680\n",
      "[27,   200/  372] train loss: 1.141 train accuracy: 0.586\n",
      "[27,   300/  372] train loss: 0.844 train accuracy: 0.719\n",
      "epoch: 27 validation loss: 1.243 validation accuracy: 0.587\n",
      "[28,   100/  372] train loss: 0.838 train accuracy: 0.719\n",
      "[28,   200/  372] train loss: 0.929 train accuracy: 0.664\n",
      "[28,   300/  372] train loss: 0.782 train accuracy: 0.680\n",
      "epoch: 28 validation loss: 1.249 validation accuracy: 0.577\n",
      "[29,   100/  372] train loss: 1.131 train accuracy: 0.594\n",
      "[29,   200/  372] train loss: 0.834 train accuracy: 0.680\n",
      "[29,   300/  372] train loss: 0.990 train accuracy: 0.672\n",
      "epoch: 29 validation loss: 1.237 validation accuracy: 0.579\n",
      "[30,   100/  372] train loss: 0.900 train accuracy: 0.695\n",
      "[30,   200/  372] train loss: 0.746 train accuracy: 0.719\n",
      "[30,   300/  372] train loss: 0.830 train accuracy: 0.719\n",
      "epoch: 30 validation loss: 1.217 validation accuracy: 0.589\n",
      "[31,   100/  372] train loss: 1.057 train accuracy: 0.625\n",
      "[31,   200/  372] train loss: 0.916 train accuracy: 0.664\n",
      "[31,   300/  372] train loss: 0.728 train accuracy: 0.711\n",
      "epoch: 31 validation loss: 1.236 validation accuracy: 0.587\n",
      "[32,   100/  372] train loss: 0.959 train accuracy: 0.695\n",
      "[32,   200/  372] train loss: 0.901 train accuracy: 0.680\n",
      "[32,   300/  372] train loss: 0.809 train accuracy: 0.719\n",
      "epoch: 32 validation loss: 1.232 validation accuracy: 0.588\n",
      "[33,   100/  372] train loss: 0.792 train accuracy: 0.750\n",
      "[33,   200/  372] train loss: 0.912 train accuracy: 0.609\n",
      "[33,   300/  372] train loss: 0.833 train accuracy: 0.727\n",
      "epoch: 33 validation loss: 1.231 validation accuracy: 0.588\n",
      "[34,   100/  372] train loss: 0.972 train accuracy: 0.625\n",
      "[34,   200/  372] train loss: 0.799 train accuracy: 0.703\n",
      "[34,   300/  372] train loss: 0.900 train accuracy: 0.648\n",
      "epoch: 34 validation loss: 1.228 validation accuracy: 0.593\n",
      "[35,   100/  372] train loss: 0.912 train accuracy: 0.695\n",
      "[35,   200/  372] train loss: 0.588 train accuracy: 0.820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,   300/  372] train loss: 0.710 train accuracy: 0.758\n",
      "epoch: 35 validation loss: 1.231 validation accuracy: 0.583\n",
      "[36,   100/  372] train loss: 0.697 train accuracy: 0.750\n",
      "[36,   200/  372] train loss: 0.879 train accuracy: 0.641\n",
      "[36,   300/  372] train loss: 0.883 train accuracy: 0.711\n",
      "epoch: 36 validation loss: 1.229 validation accuracy: 0.581\n",
      "[37,   100/  372] train loss: 0.745 train accuracy: 0.750\n",
      "[37,   200/  372] train loss: 0.834 train accuracy: 0.734\n",
      "[37,   300/  372] train loss: 0.801 train accuracy: 0.734\n",
      "epoch: 37 validation loss: 1.201 validation accuracy: 0.590\n",
      "[38,   100/  372] train loss: 0.772 train accuracy: 0.789\n",
      "[38,   200/  372] train loss: 0.847 train accuracy: 0.672\n",
      "[38,   300/  372] train loss: 0.896 train accuracy: 0.719\n",
      "epoch: 38 validation loss: 1.230 validation accuracy: 0.589\n",
      "[39,   100/  372] train loss: 0.777 train accuracy: 0.758\n",
      "[39,   200/  372] train loss: 0.746 train accuracy: 0.758\n",
      "[39,   300/  372] train loss: 0.693 train accuracy: 0.797\n",
      "epoch: 39 validation loss: 1.241 validation accuracy: 0.588\n",
      "[40,   100/  372] train loss: 0.820 train accuracy: 0.695\n",
      "[40,   200/  372] train loss: 0.673 train accuracy: 0.797\n",
      "[40,   300/  372] train loss: 0.818 train accuracy: 0.688\n",
      "epoch: 40 validation loss: 1.224 validation accuracy: 0.582\n",
      "[41,   100/  372] train loss: 0.603 train accuracy: 0.805\n",
      "[41,   200/  372] train loss: 0.731 train accuracy: 0.766\n",
      "[41,   300/  372] train loss: 0.935 train accuracy: 0.695\n",
      "epoch: 41 validation loss: 1.214 validation accuracy: 0.589\n",
      "[42,   100/  372] train loss: 0.684 train accuracy: 0.797\n",
      "[42,   200/  372] train loss: 0.820 train accuracy: 0.758\n",
      "[42,   300/  372] train loss: 0.696 train accuracy: 0.734\n",
      "epoch: 42 validation loss: 1.202 validation accuracy: 0.586\n",
      "[43,   100/  372] train loss: 0.690 train accuracy: 0.797\n",
      "[43,   200/  372] train loss: 0.636 train accuracy: 0.805\n",
      "[43,   300/  372] train loss: 0.717 train accuracy: 0.750\n",
      "epoch: 43 validation loss: 1.238 validation accuracy: 0.590\n",
      "[44,   100/  372] train loss: 0.705 train accuracy: 0.766\n",
      "[44,   200/  372] train loss: 0.773 train accuracy: 0.789\n",
      "[44,   300/  372] train loss: 0.672 train accuracy: 0.742\n",
      "epoch: 44 validation loss: 1.215 validation accuracy: 0.600\n",
      "[45,   100/  372] train loss: 0.623 train accuracy: 0.820\n",
      "[45,   200/  372] train loss: 0.684 train accuracy: 0.727\n",
      "[45,   300/  372] train loss: 0.780 train accuracy: 0.734\n",
      "epoch: 45 validation loss: 1.227 validation accuracy: 0.592\n",
      "[46,   100/  372] train loss: 0.767 train accuracy: 0.781\n",
      "[46,   200/  372] train loss: 0.704 train accuracy: 0.773\n",
      "[46,   300/  372] train loss: 0.685 train accuracy: 0.773\n",
      "epoch: 46 validation loss: 1.241 validation accuracy: 0.591\n",
      "[47,   100/  372] train loss: 0.703 train accuracy: 0.781\n",
      "[47,   200/  372] train loss: 0.901 train accuracy: 0.711\n",
      "[47,   300/  372] train loss: 0.740 train accuracy: 0.734\n",
      "epoch: 47 validation loss: 1.226 validation accuracy: 0.601\n",
      "[48,   100/  372] train loss: 0.660 train accuracy: 0.773\n",
      "[48,   200/  372] train loss: 0.704 train accuracy: 0.750\n",
      "[48,   300/  372] train loss: 0.717 train accuracy: 0.758\n",
      "epoch: 48 validation loss: 1.212 validation accuracy: 0.592\n",
      "[49,   100/  372] train loss: 0.678 train accuracy: 0.828\n",
      "[49,   200/  372] train loss: 0.826 train accuracy: 0.781\n",
      "[49,   300/  372] train loss: 0.864 train accuracy: 0.648\n",
      "epoch: 49 validation loss: 1.209 validation accuracy: 0.600\n",
      "[50,   100/  372] train loss: 0.698 train accuracy: 0.789\n",
      "[50,   200/  372] train loss: 0.742 train accuracy: 0.742\n",
      "[50,   300/  372] train loss: 0.597 train accuracy: 0.820\n",
      "epoch: 50 validation loss: 1.218 validation accuracy: 0.594\n",
      "[51,   100/  372] train loss: 0.568 train accuracy: 0.766\n",
      "[51,   200/  372] train loss: 0.585 train accuracy: 0.820\n",
      "[51,   300/  372] train loss: 0.745 train accuracy: 0.734\n",
      "epoch: 51 validation loss: 1.231 validation accuracy: 0.603\n",
      "[52,   100/  372] train loss: 0.687 train accuracy: 0.727\n",
      "[52,   200/  372] train loss: 0.725 train accuracy: 0.758\n",
      "[52,   300/  372] train loss: 0.746 train accuracy: 0.781\n",
      "epoch: 52 validation loss: 1.201 validation accuracy: 0.598\n",
      "[53,   100/  372] train loss: 0.598 train accuracy: 0.789\n",
      "[53,   200/  372] train loss: 0.678 train accuracy: 0.773\n",
      "[53,   300/  372] train loss: 0.693 train accuracy: 0.766\n",
      "epoch: 53 validation loss: 1.207 validation accuracy: 0.597\n",
      "[54,   100/  372] train loss: 0.717 train accuracy: 0.734\n",
      "[54,   200/  372] train loss: 0.676 train accuracy: 0.789\n",
      "[54,   300/  372] train loss: 0.669 train accuracy: 0.781\n",
      "epoch: 54 validation loss: 1.219 validation accuracy: 0.581\n",
      "[55,   100/  372] train loss: 0.701 train accuracy: 0.750\n",
      "[55,   200/  372] train loss: 0.670 train accuracy: 0.773\n",
      "[55,   300/  372] train loss: 0.637 train accuracy: 0.750\n",
      "epoch: 55 validation loss: 1.231 validation accuracy: 0.587\n",
      "[56,   100/  372] train loss: 0.796 train accuracy: 0.742\n",
      "[56,   200/  372] train loss: 0.572 train accuracy: 0.820\n",
      "[56,   300/  372] train loss: 0.508 train accuracy: 0.844\n",
      "epoch: 56 validation loss: 1.207 validation accuracy: 0.597\n",
      "[57,   100/  372] train loss: 0.592 train accuracy: 0.859\n",
      "[57,   200/  372] train loss: 0.698 train accuracy: 0.773\n",
      "[57,   300/  372] train loss: 0.573 train accuracy: 0.805\n",
      "epoch: 57 validation loss: 1.220 validation accuracy: 0.594\n",
      "[58,   100/  372] train loss: 0.613 train accuracy: 0.781\n",
      "[58,   200/  372] train loss: 0.708 train accuracy: 0.758\n",
      "[58,   300/  372] train loss: 0.701 train accuracy: 0.773\n",
      "epoch: 58 validation loss: 1.190 validation accuracy: 0.600\n",
      "[59,   100/  372] train loss: 0.639 train accuracy: 0.719\n",
      "[59,   200/  372] train loss: 0.704 train accuracy: 0.797\n",
      "[59,   300/  372] train loss: 0.701 train accuracy: 0.797\n",
      "epoch: 59 validation loss: 1.221 validation accuracy: 0.596\n",
      "[60,   100/  372] train loss: 0.534 train accuracy: 0.828\n",
      "[60,   200/  372] train loss: 0.802 train accuracy: 0.750\n",
      "[60,   300/  372] train loss: 0.547 train accuracy: 0.812\n",
      "epoch: 60 validation loss: 1.217 validation accuracy: 0.595\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamters can be tuned by setting required range below\n",
    "# learning_rates = list(np.logspace(-4, -2, 3))\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]    # learning rate decays at every epoch\n",
    "# weight_decays = [0.0] + list(np.logspace(-5, -1, 5))\n",
    "weight_decays = [1e-5]           # regularization weight\n",
    "momentums = [0.9]\n",
    "# dropout_probabilities = [(0.2, 0.5), (0.0, 0.0)]\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['dropout_input'] = hparam_tuple[0][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[0][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[1]\n",
    "    hparam['lr_decay'] = hparam_tuple[2]\n",
    "    hparam['momentum'] = hparam_tuple[3]\n",
    "    hparam['lr'] = hparam_tuple[4]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    teacher_net = networks.TeacherNetwork(cifar_image_shape, num_class)\n",
    "    teacher_net = teacher_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results[hparam_tuple] = utils.trainTeacherOnHparam(teacher_net, hparam, num_epochs, \n",
    "                                                        train_loader, val_loader, \n",
    "                                                        print_every=print_every, \n",
    "                                                        fast_device=fast_device)\n",
    "    save_path = checkpoints_path + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results[hparam_tuple], \n",
    "                'model_state_dict' : teacher_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.6128\n"
     ]
    }
   ],
   "source": [
    "# Calculate test accuracy\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparemeter search utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "lr_scatter = ([math.log10(h['lr']) for h in hparams_list])\n",
    "dropout_scatter = [int(h['dropout_input'] == 0.2) for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(lr_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(lr_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (lr_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hparam in hparams_list:\n",
    "    load_path = 'checkpoints/' + utils.hparamToString(hparam) + '_final.tar'\n",
    "    print(utils.hparamToString(hparam))\n",
    "    load_dict = torch.load(load_path)\n",
    "    plt.plot(utils.getTrainMetricPerEpoch(load_dict['results']['train_loss'], len(train_loader)))\n",
    "    plt.show()\n",
    "    plt.plot(utils.getTrainMetricPerEpoch(load_dict['results']['train_acc'], len(train_loader)))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
