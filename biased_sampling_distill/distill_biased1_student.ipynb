{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and limit GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import networks\n",
    "import utils\n",
    "import biased_sampler\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True    # set use_gpu to True if system has gpu\n",
    "gpu_id = 1        # id of gpu to be used\n",
    "cpu_device = torch.device('cpu')\n",
    "# fast_device is where computation (training, inference) happens\n",
    "fast_device = torch.device('cpu')\n",
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'    # set visible devices depending on system configuration\n",
    "    fast_device = torch.device('cuda:' + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "def reproducibilitySeed():\n",
    "    \"\"\"\n",
    "    Ensure reproducibility of results; Seeds to 0\n",
    "    \"\"\"\n",
    "    torch_init_seed = 0\n",
    "    torch.manual_seed(torch_init_seed)\n",
    "    numpy_init_seed = 0\n",
    "    np.random.seed(numpy_init_seed)\n",
    "    if use_gpu:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reproducibilitySeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Student trained without data augmentation\n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "train_val_dataset = torchvision.datasets.MNIST(root='./MNIST_dataset/', train=True, \n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./MNIST_dataset/', train=False, \n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "num_train = int(1.0 * len(train_val_dataset) * 95 / 100)\n",
    "num_val = len(train_val_dataset) - num_train\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_class = 10\n",
    "class_prob = [0.01 for _ in range(num_class)]\n",
    "class_prob[7] += 0.45\n",
    "class_prob[8] += 0.45\n",
    "\n",
    "\n",
    "train_val_biased_sampler = biased_sampler.MNISTClassBiasedSampler(train_val_dataset, class_prob)\n",
    "train_biased_sampler = biased_sampler.MNISTClassBiasedSampler(train_dataset, class_prob)\n",
    "\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=128, \n",
    "                                                sampler=train_val_biased_sampler, \n",
    "                                                num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, \n",
    "                                            sampler=train_biased_sampler, \n",
    "                                            num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path_teacher = 'checkpoints_teacher/'\n",
    "checkpoints_path_student = 'checkpoints_student_biased1/'\n",
    "summaries_path_student = 'summaries_student_biased1/'\n",
    "if not os.path.exists(checkpoints_path_student):\n",
    "    os.makedirs(checkpoints_path_student)\n",
    "if not os.path.exists(summaries_path_student):\n",
    "    os.makedirs(summaries_path_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load teacher network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hparams used for training teacher to load the teacher network\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "# keeping dropout input = dropout hidden\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['dropout_input'] = hparam_tuple[0][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[0][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[1]\n",
    "    hparam['lr_decay'] = hparam_tuple[2]\n",
    "    hparam['momentum'] = hparam_tuple[3]\n",
    "    hparam['lr'] = hparam_tuple[4]\n",
    "    hparams_list.append(hparam)\n",
    "    \n",
    "load_path = checkpoints_path_teacher + utils.hparamToString(hparams_list[0]) + '_final.tar'\n",
    "teacher_net = networks.TeacherNetwork()\n",
    "teacher_net.load_state_dict(torch.load(load_path, map_location=fast_device)['model_state_dict'])\n",
    "teacher_net = teacher_net.to(fast_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher test accuracy:  0.9892\n"
     ]
    }
   ],
   "source": [
    "# Calculate teacher test accuracy\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
    "print('teacher test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train student network without distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hparamsT=1, alpha=0.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=1e-05\n",
      "epoch: 0 validation accuracy: 0.100\n",
      "[1,   100/  446] train loss: 0.271 train accuracy: 0.930\n",
      "[1,   200/  446] train loss: 0.115 train accuracy: 0.984\n",
      "[1,   300/  446] train loss: 0.216 train accuracy: 0.953\n",
      "[1,   400/  446] train loss: 0.077 train accuracy: 0.984\n",
      "epoch: 1 validation accuracy: 0.713\n",
      "[2,   100/  446] train loss: 0.116 train accuracy: 0.969\n",
      "[2,   200/  446] train loss: 0.124 train accuracy: 0.953\n",
      "[2,   300/  446] train loss: 0.052 train accuracy: 0.984\n",
      "[2,   400/  446] train loss: 0.172 train accuracy: 0.945\n",
      "epoch: 2 validation accuracy: 0.811\n",
      "[3,   100/  446] train loss: 0.130 train accuracy: 0.953\n",
      "[3,   200/  446] train loss: 0.033 train accuracy: 1.000\n",
      "[3,   300/  446] train loss: 0.104 train accuracy: 0.961\n",
      "[3,   400/  446] train loss: 0.085 train accuracy: 0.992\n",
      "epoch: 3 validation accuracy: 0.860\n",
      "[4,   100/  446] train loss: 0.144 train accuracy: 0.938\n",
      "[4,   200/  446] train loss: 0.064 train accuracy: 0.977\n",
      "[4,   300/  446] train loss: 0.082 train accuracy: 0.969\n",
      "[4,   400/  446] train loss: 0.052 train accuracy: 0.977\n",
      "epoch: 4 validation accuracy: 0.869\n",
      "[5,   100/  446] train loss: 0.124 train accuracy: 0.969\n",
      "[5,   200/  446] train loss: 0.050 train accuracy: 0.984\n",
      "[5,   300/  446] train loss: 0.029 train accuracy: 1.000\n",
      "[5,   400/  446] train loss: 0.043 train accuracy: 0.992\n",
      "epoch: 5 validation accuracy: 0.874\n",
      "[6,   100/  446] train loss: 0.039 train accuracy: 0.992\n",
      "[6,   200/  446] train loss: 0.048 train accuracy: 0.977\n",
      "[6,   300/  446] train loss: 0.120 train accuracy: 0.977\n",
      "[6,   400/  446] train loss: 0.020 train accuracy: 1.000\n",
      "epoch: 6 validation accuracy: 0.883\n",
      "[7,   100/  446] train loss: 0.137 train accuracy: 0.977\n",
      "[7,   200/  446] train loss: 0.072 train accuracy: 0.977\n",
      "[7,   300/  446] train loss: 0.018 train accuracy: 1.000\n",
      "[7,   400/  446] train loss: 0.044 train accuracy: 0.984\n",
      "epoch: 7 validation accuracy: 0.894\n",
      "[8,   100/  446] train loss: 0.101 train accuracy: 0.984\n",
      "[8,   200/  446] train loss: 0.087 train accuracy: 0.984\n",
      "[8,   300/  446] train loss: 0.022 train accuracy: 0.992\n",
      "[8,   400/  446] train loss: 0.023 train accuracy: 1.000\n",
      "epoch: 8 validation accuracy: 0.898\n",
      "[9,   100/  446] train loss: 0.028 train accuracy: 0.992\n",
      "[9,   200/  446] train loss: 0.027 train accuracy: 0.992\n",
      "[9,   300/  446] train loss: 0.105 train accuracy: 0.977\n",
      "[9,   400/  446] train loss: 0.019 train accuracy: 1.000\n",
      "epoch: 9 validation accuracy: 0.906\n",
      "[10,   100/  446] train loss: 0.018 train accuracy: 1.000\n",
      "[10,   200/  446] train loss: 0.012 train accuracy: 1.000\n",
      "[10,   300/  446] train loss: 0.080 train accuracy: 0.977\n",
      "[10,   400/  446] train loss: 0.008 train accuracy: 1.000\n",
      "epoch: 10 validation accuracy: 0.903\n",
      "[11,   100/  446] train loss: 0.007 train accuracy: 1.000\n",
      "[11,   200/  446] train loss: 0.026 train accuracy: 1.000\n",
      "[11,   300/  446] train loss: 0.020 train accuracy: 1.000\n",
      "[11,   400/  446] train loss: 0.053 train accuracy: 0.984\n",
      "epoch: 11 validation accuracy: 0.917\n",
      "[12,   100/  446] train loss: 0.055 train accuracy: 0.984\n",
      "[12,   200/  446] train loss: 0.017 train accuracy: 1.000\n",
      "[12,   300/  446] train loss: 0.013 train accuracy: 1.000\n",
      "[12,   400/  446] train loss: 0.042 train accuracy: 0.984\n",
      "epoch: 12 validation accuracy: 0.917\n",
      "[13,   100/  446] train loss: 0.053 train accuracy: 0.992\n",
      "[13,   200/  446] train loss: 0.030 train accuracy: 0.992\n",
      "[13,   300/  446] train loss: 0.017 train accuracy: 1.000\n",
      "[13,   400/  446] train loss: 0.010 train accuracy: 1.000\n",
      "epoch: 13 validation accuracy: 0.923\n",
      "[14,   100/  446] train loss: 0.042 train accuracy: 0.992\n",
      "[14,   200/  446] train loss: 0.033 train accuracy: 0.984\n",
      "[14,   300/  446] train loss: 0.054 train accuracy: 0.992\n",
      "[14,   400/  446] train loss: 0.024 train accuracy: 0.992\n",
      "epoch: 14 validation accuracy: 0.926\n",
      "[15,   100/  446] train loss: 0.043 train accuracy: 0.977\n",
      "[15,   200/  446] train loss: 0.080 train accuracy: 0.984\n",
      "[15,   300/  446] train loss: 0.013 train accuracy: 1.000\n",
      "[15,   400/  446] train loss: 0.012 train accuracy: 0.992\n",
      "epoch: 15 validation accuracy: 0.926\n",
      "[16,   100/  446] train loss: 0.018 train accuracy: 0.992\n",
      "[16,   200/  446] train loss: 0.012 train accuracy: 1.000\n",
      "[16,   300/  446] train loss: 0.044 train accuracy: 0.992\n",
      "[16,   400/  446] train loss: 0.011 train accuracy: 1.000\n",
      "epoch: 16 validation accuracy: 0.928\n",
      "[17,   100/  446] train loss: 0.032 train accuracy: 0.992\n",
      "[17,   200/  446] train loss: 0.022 train accuracy: 1.000\n",
      "[17,   300/  446] train loss: 0.007 train accuracy: 1.000\n",
      "[17,   400/  446] train loss: 0.042 train accuracy: 0.984\n",
      "epoch: 17 validation accuracy: 0.932\n",
      "[18,   100/  446] train loss: 0.052 train accuracy: 0.984\n",
      "[18,   200/  446] train loss: 0.075 train accuracy: 0.984\n",
      "[18,   300/  446] train loss: 0.016 train accuracy: 0.992\n",
      "[18,   400/  446] train loss: 0.024 train accuracy: 0.992\n",
      "epoch: 18 validation accuracy: 0.929\n",
      "[19,   100/  446] train loss: 0.006 train accuracy: 1.000\n",
      "[19,   200/  446] train loss: 0.051 train accuracy: 0.969\n",
      "[19,   300/  446] train loss: 0.007 train accuracy: 1.000\n",
      "[19,   400/  446] train loss: 0.033 train accuracy: 0.984\n",
      "epoch: 19 validation accuracy: 0.934\n",
      "[20,   100/  446] train loss: 0.032 train accuracy: 0.992\n",
      "[20,   200/  446] train loss: 0.015 train accuracy: 0.992\n",
      "[20,   300/  446] train loss: 0.004 train accuracy: 1.000\n",
      "[20,   400/  446] train loss: 0.009 train accuracy: 1.000\n",
      "epoch: 20 validation accuracy: 0.936\n"
     ]
    }
   ],
   "source": [
    "temperatures = [1]    # temperature for distillation loss\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "alphas = [0.0]\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "# No dropout used\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['alpha'] = hparam_tuple[0]\n",
    "    hparam['T'] = hparam_tuple[1]\n",
    "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[3]\n",
    "    hparam['lr_decay'] = hparam_tuple[4]\n",
    "    hparam['momentum'] = hparam_tuple[5]\n",
    "    hparam['lr'] = hparam_tuple[6]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results_no_distill = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net = student_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results_no_distill[hparam_tuple] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs, \n",
    "                                                                    train_loader, val_loader, \n",
    "                                                                    print_every=print_every, \n",
    "                                                                    fast_device=fast_device)\n",
    "    save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results_no_distill[hparam_tuple], \n",
    "                'model_state_dict' : student_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student test accuracy (w/o distillation):  0.9387\n"
     ]
    }
   ],
   "source": [
    "# Calculate student test accuracy\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "print('student test accuracy (w/o distillation): ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "weight_decay_scatter = ([math.log10(h['weight_decay']) if h['weight_decay'] > 0 else -6 for h in hparams_list])\n",
    "dropout_scatter = [int(h['dropout_input'] == 0.2) for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results_no_distill[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(weight_decay_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(weight_decay_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (weight_decay_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train student network using distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hparamsT=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=1e-05\n",
      "epoch: 0 validation accuracy: 0.100\n",
      "[1,   100/  446] train loss: 1.898 train accuracy: 0.938\n",
      "[1,   200/  446] train loss: 1.445 train accuracy: 0.969\n",
      "[1,   300/  446] train loss: 1.115 train accuracy: 0.969\n",
      "[1,   400/  446] train loss: 0.952 train accuracy: 0.969\n",
      "epoch: 1 validation accuracy: 0.876\n",
      "[2,   100/  446] train loss: 0.674 train accuracy: 0.984\n",
      "[2,   200/  446] train loss: 0.634 train accuracy: 0.984\n",
      "[2,   300/  446] train loss: 0.536 train accuracy: 1.000\n",
      "[2,   400/  446] train loss: 0.527 train accuracy: 0.992\n",
      "epoch: 2 validation accuracy: 0.934\n",
      "[3,   100/  446] train loss: 0.471 train accuracy: 0.953\n",
      "[3,   200/  446] train loss: 0.327 train accuracy: 0.992\n",
      "[3,   300/  446] train loss: 0.402 train accuracy: 0.992\n",
      "[3,   400/  446] train loss: 0.370 train accuracy: 0.977\n",
      "epoch: 3 validation accuracy: 0.948\n",
      "[4,   100/  446] train loss: 0.493 train accuracy: 0.992\n",
      "[4,   200/  446] train loss: 0.332 train accuracy: 0.977\n",
      "[4,   300/  446] train loss: 0.277 train accuracy: 0.992\n",
      "[4,   400/  446] train loss: 0.303 train accuracy: 1.000\n",
      "epoch: 4 validation accuracy: 0.962\n",
      "[5,   100/  446] train loss: 0.241 train accuracy: 0.992\n",
      "[5,   200/  446] train loss: 0.262 train accuracy: 0.992\n",
      "[5,   300/  446] train loss: 0.234 train accuracy: 1.000\n",
      "[5,   400/  446] train loss: 0.221 train accuracy: 0.984\n",
      "epoch: 5 validation accuracy: 0.962\n",
      "[6,   100/  446] train loss: 0.202 train accuracy: 0.992\n",
      "[6,   200/  446] train loss: 0.180 train accuracy: 1.000\n",
      "[6,   300/  446] train loss: 0.194 train accuracy: 0.984\n",
      "[6,   400/  446] train loss: 0.150 train accuracy: 1.000\n",
      "epoch: 6 validation accuracy: 0.967\n",
      "[7,   100/  446] train loss: 0.229 train accuracy: 0.977\n",
      "[7,   200/  446] train loss: 0.195 train accuracy: 0.961\n",
      "[7,   300/  446] train loss: 0.169 train accuracy: 0.992\n",
      "[7,   400/  446] train loss: 0.187 train accuracy: 0.992\n",
      "epoch: 7 validation accuracy: 0.970\n",
      "[8,   100/  446] train loss: 0.121 train accuracy: 0.969\n",
      "[8,   200/  446] train loss: 0.169 train accuracy: 0.969\n",
      "[8,   300/  446] train loss: 0.159 train accuracy: 0.992\n",
      "[8,   400/  446] train loss: 0.182 train accuracy: 0.992\n",
      "epoch: 8 validation accuracy: 0.971\n",
      "[9,   100/  446] train loss: 0.162 train accuracy: 0.992\n",
      "[9,   200/  446] train loss: 0.187 train accuracy: 0.992\n",
      "[9,   300/  446] train loss: 0.194 train accuracy: 0.992\n",
      "[9,   400/  446] train loss: 0.120 train accuracy: 0.992\n",
      "epoch: 9 validation accuracy: 0.976\n",
      "[10,   100/  446] train loss: 0.145 train accuracy: 0.992\n",
      "[10,   200/  446] train loss: 0.121 train accuracy: 0.992\n",
      "[10,   300/  446] train loss: 0.186 train accuracy: 0.984\n",
      "[10,   400/  446] train loss: 0.120 train accuracy: 0.969\n",
      "epoch: 10 validation accuracy: 0.975\n",
      "[11,   100/  446] train loss: 0.103 train accuracy: 0.992\n",
      "[11,   200/  446] train loss: 0.121 train accuracy: 0.984\n",
      "[11,   300/  446] train loss: 0.102 train accuracy: 0.992\n",
      "[11,   400/  446] train loss: 0.109 train accuracy: 0.984\n",
      "epoch: 11 validation accuracy: 0.977\n",
      "[12,   100/  446] train loss: 0.138 train accuracy: 0.984\n",
      "[12,   200/  446] train loss: 0.107 train accuracy: 0.992\n",
      "[12,   300/  446] train loss: 0.129 train accuracy: 0.984\n",
      "[12,   400/  446] train loss: 0.128 train accuracy: 0.992\n",
      "epoch: 12 validation accuracy: 0.978\n",
      "[13,   100/  446] train loss: 0.111 train accuracy: 1.000\n",
      "[13,   200/  446] train loss: 0.130 train accuracy: 0.977\n",
      "[13,   300/  446] train loss: 0.118 train accuracy: 0.992\n",
      "[13,   400/  446] train loss: 0.101 train accuracy: 0.977\n",
      "epoch: 13 validation accuracy: 0.977\n",
      "[14,   100/  446] train loss: 0.103 train accuracy: 0.992\n",
      "[14,   200/  446] train loss: 0.108 train accuracy: 0.992\n",
      "[14,   300/  446] train loss: 0.104 train accuracy: 0.984\n",
      "[14,   400/  446] train loss: 0.128 train accuracy: 0.992\n",
      "epoch: 14 validation accuracy: 0.979\n",
      "[15,   100/  446] train loss: 0.117 train accuracy: 0.992\n",
      "[15,   200/  446] train loss: 0.162 train accuracy: 0.992\n",
      "[15,   300/  446] train loss: 0.087 train accuracy: 1.000\n",
      "[15,   400/  446] train loss: 0.078 train accuracy: 1.000\n",
      "epoch: 15 validation accuracy: 0.980\n",
      "[16,   100/  446] train loss: 0.074 train accuracy: 0.992\n",
      "[16,   200/  446] train loss: 0.093 train accuracy: 1.000\n",
      "[16,   300/  446] train loss: 0.076 train accuracy: 0.992\n",
      "[16,   400/  446] train loss: 0.090 train accuracy: 0.984\n",
      "epoch: 16 validation accuracy: 0.980\n",
      "[17,   100/  446] train loss: 0.087 train accuracy: 1.000\n",
      "[17,   200/  446] train loss: 0.100 train accuracy: 0.984\n",
      "[17,   300/  446] train loss: 0.082 train accuracy: 0.992\n",
      "[17,   400/  446] train loss: 0.091 train accuracy: 0.992\n",
      "epoch: 17 validation accuracy: 0.980\n",
      "[18,   100/  446] train loss: 0.081 train accuracy: 1.000\n",
      "[18,   200/  446] train loss: 0.102 train accuracy: 0.984\n",
      "[18,   300/  446] train loss: 0.071 train accuracy: 1.000\n",
      "[18,   400/  446] train loss: 0.090 train accuracy: 0.992\n",
      "epoch: 18 validation accuracy: 0.980\n",
      "[19,   100/  446] train loss: 0.083 train accuracy: 1.000\n",
      "[19,   200/  446] train loss: 0.134 train accuracy: 1.000\n",
      "[19,   300/  446] train loss: 0.080 train accuracy: 0.992\n",
      "[19,   400/  446] train loss: 0.080 train accuracy: 0.992\n",
      "epoch: 19 validation accuracy: 0.980\n",
      "[20,   100/  446] train loss: 0.090 train accuracy: 0.984\n",
      "[20,   200/  446] train loss: 0.096 train accuracy: 0.992\n",
      "[20,   300/  446] train loss: 0.087 train accuracy: 1.000\n",
      "[20,   400/  446] train loss: 0.096 train accuracy: 1.000\n",
      "epoch: 20 validation accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "temperatures = [5]\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "alphas = [1.0]\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['alpha'] = hparam_tuple[0]\n",
    "    hparam['T'] = hparam_tuple[1]\n",
    "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[3]\n",
    "    hparam['lr_decay'] = hparam_tuple[4]\n",
    "    hparam['momentum'] = hparam_tuple[5]\n",
    "    hparam['lr'] = hparam_tuple[6]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results_distill = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net = student_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results_distill[hparam_tuple] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs, \n",
    "                                                                train_loader, val_loader, \n",
    "                                                                print_every=print_every, \n",
    "                                                                fast_device=fast_device)\n",
    "    save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results_distill[hparam_tuple], \n",
    "                'model_state_dict' : student_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "T_scatter = [math.log(h['T']) for h in hparams_list]\n",
    "alpha_scatter = [h['alpha'] for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results_distill[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(T_scatter, alpha_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(T_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (T_scatter[i], alpha_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['r', 'b', 'g', 'm', 'y', 'k']\n",
    "for hparam, c in zip(hparams_list, color):\n",
    "    cur_results = results_distill[utils.hparamDictToTuple(hparam)]\n",
    "    plt.plot(cur_results['val_acc'], color=c, label=str(hparam['T']))\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('Validation Accuracy vs Epoch for different T')\n",
    "plt.savefig(summaries_path_student + 'val_acc_vs_epoch_wrt_T.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=1e-05\n",
      "test accuracy:  0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hparam in hparams_list:\n",
    "    load_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    load_dict = torch.load(load_path)\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net.load_state_dict(load_dict['model_state_dict'])\n",
    "    student_net = student_net.to(fast_device)\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "    print(utils.hparamToString(hparam))\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [5]\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "alphas = [0.8, 0.5, 0.4, 0.2]\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['alpha'] = hparam_tuple[0]\n",
    "    hparam['T'] = hparam_tuple[1]\n",
    "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[3]\n",
    "    hparam['lr_decay'] = hparam_tuple[4]\n",
    "    hparam['momentum'] = hparam_tuple[5]\n",
    "    hparam['lr'] = hparam_tuple[6]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results_distill = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net = student_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results_distill[hparam_tuple] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs, \n",
    "                                                                train_loader, val_loader, \n",
    "                                                                print_every=print_every, \n",
    "                                                                fast_device=fast_device)\n",
    "    save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results_distill[hparam_tuple], \n",
    "                'model_state_dict' : student_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "T_scatter = [math.log(h['T']) for h in hparams_list]\n",
    "alpha_scatter = [h['alpha'] for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results_distill[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(T_scatter, alpha_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(T_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (T_scatter[i], alpha_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['r', 'b', 'g', 'm']\n",
    "for hparam, c in zip(hparams_list, color):\n",
    "    cur_results = results_distill[utils.hparamDictToTuple(hparam)]\n",
    "    plt.plot(cur_results['val_acc'], color=c, label=str(hparam['alpha']))\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('Validation Accuracy vs Epoch for different alpha')\n",
    "plt.savefig(summaries_path_student + 'val_acc_vs_epoch_wrt_alpha.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hparam in hparams_list:\n",
    "    load_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    load_dict = torch.load(load_path)\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net.load_state_dict(load_dict['model_state_dict'])\n",
    "    student_net = student_net.to(fast_device)\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "    print(utils.hparamToString(hparam))\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
