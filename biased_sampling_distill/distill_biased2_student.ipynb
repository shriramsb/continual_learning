{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and limit GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import networks\n",
    "import utils\n",
    "import biased_sampler\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True    # set use_gpu to True if system has gpu\n",
    "gpu_id = 1        # id of gpu to be used\n",
    "cpu_device = torch.device('cpu')\n",
    "# fast_device is where computation (training, inference) happens\n",
    "fast_device = torch.device('cpu')\n",
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'    # set visible devices depending on system configuration\n",
    "    fast_device = torch.device('cuda:' + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "def reproducibilitySeed():\n",
    "    \"\"\"\n",
    "    Ensure reproducibility of results; Seeds to 0\n",
    "    \"\"\"\n",
    "    torch_init_seed = 0\n",
    "    torch.manual_seed(torch_init_seed)\n",
    "    numpy_init_seed = 0\n",
    "    np.random.seed(numpy_init_seed)\n",
    "    if use_gpu:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reproducibilitySeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Student trained without data augmentation\n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "train_val_dataset = torchvision.datasets.MNIST(root='./MNIST_dataset/', train=True, \n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./MNIST_dataset/', train=False, \n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "num_train = int(1.0 * len(train_val_dataset) * 95 / 100)\n",
    "num_val = len(train_val_dataset) - num_train\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_class = 10\n",
    "class_prob = [0.001 for _ in range(num_class)]\n",
    "class_prob[7] += 0.495\n",
    "class_prob[8] += 0.495\n",
    "\n",
    "\n",
    "train_val_biased_sampler = biased_sampler.MNISTClassBiasedSampler(train_val_dataset, class_prob)\n",
    "train_biased_sampler = biased_sampler.MNISTClassBiasedSampler(train_dataset, class_prob)\n",
    "\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=128, \n",
    "                                                sampler=train_val_biased_sampler, \n",
    "                                                num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, \n",
    "                                            sampler=train_biased_sampler, \n",
    "                                            num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path_teacher = 'checkpoints_teacher/'\n",
    "checkpoints_path_student = 'checkpoints_student_biased2/'\n",
    "summaries_path_student = 'summaries_student_biased2/'\n",
    "if not os.path.exists(checkpoints_path_student):\n",
    "    os.makedirs(checkpoints_path_student)\n",
    "if not os.path.exists(summaries_path_student):\n",
    "    os.makedirs(summaries_path_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load teacher network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hparams used for training teacher to load the teacher network\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "# keeping dropout input = dropout hidden\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['dropout_input'] = hparam_tuple[0][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[0][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[1]\n",
    "    hparam['lr_decay'] = hparam_tuple[2]\n",
    "    hparam['momentum'] = hparam_tuple[3]\n",
    "    hparam['lr'] = hparam_tuple[4]\n",
    "    hparams_list.append(hparam)\n",
    "    \n",
    "load_path = checkpoints_path_teacher + utils.hparamToString(hparams_list[0]) + '_final.tar'\n",
    "teacher_net = networks.TeacherNetwork()\n",
    "teacher_net.load_state_dict(torch.load(load_path, map_location=fast_device)['model_state_dict'])\n",
    "teacher_net = teacher_net.to(fast_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher test accuracy:  0.9892\n"
     ]
    }
   ],
   "source": [
    "# Calculate teacher test accuracy\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
    "print('teacher test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train student network without distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temperatures = [1]    # temperature for distillation loss\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "alphas = [0.0]\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "# No dropout used\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['alpha'] = hparam_tuple[0]\n",
    "    hparam['T'] = hparam_tuple[1]\n",
    "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[3]\n",
    "    hparam['lr_decay'] = hparam_tuple[4]\n",
    "    hparam['momentum'] = hparam_tuple[5]\n",
    "    hparam['lr'] = hparam_tuple[6]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results_no_distill = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net = student_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results_no_distill[hparam_tuple] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs, \n",
    "                                                                    train_loader, val_loader, \n",
    "                                                                    print_every=print_every, \n",
    "                                                                    fast_device=fast_device)\n",
    "    save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results_no_distill[hparam_tuple], \n",
    "                'model_state_dict' : student_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student test accuracy (w/o distillation):  0.723\n"
     ]
    }
   ],
   "source": [
    "# Calculate student test accuracy\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "print('student test accuracy (w/o distillation): ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1   0   1   2   3   4   5   6   7   8   9 \n",
      "  0 252   0   0   0   1   9   5   6  41   0 \n",
      "  1   0 208   0   0   0   1   0  47  59   0 \n",
      "  2   5   1 187   1   2   0   6  30  71   1 \n",
      "  3   0   0   5 159   1  11   2  15 117   0 \n",
      "  4   1   0   1   0 201   0   2  29  36  25 \n",
      "  5   2   1   3   0   1 137   2   4 119   0 \n",
      "  6   1   0   3   0   0   0 255   1  48   0 \n",
      "  7   0   1   0   0   0   0   0 303   2   0 \n",
      "  8   0   0   0   0   0   0   0   0 289   0 \n",
      "  9   1   0   0   2   2   2   0  75  51 157 \n"
     ]
    }
   ],
   "source": [
    "# get confusion matrix\n",
    "for hparam in hparams_list:\n",
    "    load_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    load_dict = torch.load(load_path)\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net.load_state_dict(load_dict['model_state_dict'])\n",
    "    student_net = student_net.to(fast_device)\n",
    "    confusion_matrix = utils.getConfusionMatrix(student_net, val_loader, fast_device, num_class)\n",
    "    utils.printConfusionMatrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "weight_decay_scatter = ([math.log10(h['weight_decay']) if h['weight_decay'] > 0 else -6 for h in hparams_list])\n",
    "dropout_scatter = [int(h['dropout_input'] == 0.2) for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results_no_distill[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(weight_decay_scatter, dropout_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(weight_decay_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (weight_decay_scatter[i], dropout_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train student network using distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temperatures = [5]\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "alphas = [1.0]\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['alpha'] = hparam_tuple[0]\n",
    "    hparam['T'] = hparam_tuple[1]\n",
    "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[3]\n",
    "    hparam['lr_decay'] = hparam_tuple[4]\n",
    "    hparam['momentum'] = hparam_tuple[5]\n",
    "    hparam['lr'] = hparam_tuple[6]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results_distill = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net = student_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results_distill[hparam_tuple] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs, \n",
    "                                                                train_loader, val_loader, \n",
    "                                                                print_every=print_every, \n",
    "                                                                fast_device=fast_device)\n",
    "    save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results_distill[hparam_tuple], \n",
    "                'model_state_dict' : student_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "T_scatter = [math.log(h['T']) for h in hparams_list]\n",
    "alpha_scatter = [h['alpha'] for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results_distill[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(T_scatter, alpha_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(T_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (T_scatter[i], alpha_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['r', 'b', 'g', 'm', 'y', 'k']\n",
    "for hparam, c in zip(hparams_list, color):\n",
    "    cur_results = results_distill[utils.hparamDictToTuple(hparam)]\n",
    "    plt.plot(cur_results['val_acc'], color=c, label=str(hparam['T']))\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('Validation Accuracy vs Epoch for different T')\n",
    "plt.savefig(summaries_path_student + 'val_acc_vs_epoch_wrt_T.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=1e-05\n",
      "test accuracy:  0.9662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hparam in hparams_list:\n",
    "    load_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    load_dict = torch.load(load_path)\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net.load_state_dict(load_dict['model_state_dict'])\n",
    "    student_net = student_net.to(fast_device)\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "    print(utils.hparamToString(hparam))\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1   0   1   2   3   4   5   6   7   8   9 \n",
      "  0 305   0   0   0   0   2   1   0   5   1 \n",
      "  1   0 308   0   1   0   1   0   0   5   0 \n",
      "  2   0   3 289   0   0   0   5   4   3   0 \n",
      "  3   0   0   2 292   0   8   1   2   5   0 \n",
      "  4   1   0   0   0 282   0   0   1   3   8 \n",
      "  5   2   0   1   4   0 252   1   0   6   3 \n",
      "  6   1   0   1   0   0   0 301   0   5   0 \n",
      "  7   0   0   0   0   0   0   0 305   1   0 \n",
      "  8   0   0   0   0   0   1   0   1 287   0 \n",
      "  9   0   0   0   2   2   2   0   9   3 272 \n"
     ]
    }
   ],
   "source": [
    "# get confusion matrix\n",
    "for hparam in hparams_list:\n",
    "    load_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    load_dict = torch.load(load_path)\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net.load_state_dict(load_dict['model_state_dict'])\n",
    "    student_net = student_net.to(fast_device)\n",
    "    confusion_matrix = utils.getConfusionMatrix(student_net, val_loader, fast_device, num_class)\n",
    "    utils.printConfusionMatrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [5]\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "alphas = [0.8, 0.5, 0.4, 0.2]\n",
    "learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]\n",
    "weight_decays = [1e-5]\n",
    "momentums = [0.9]\n",
    "dropout_probabilities = [(0.0, 0.0)]\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['alpha'] = hparam_tuple[0]\n",
    "    hparam['T'] = hparam_tuple[1]\n",
    "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[3]\n",
    "    hparam['lr_decay'] = hparam_tuple[4]\n",
    "    hparam['momentum'] = hparam_tuple[5]\n",
    "    hparam['lr'] = hparam_tuple[6]\n",
    "    hparams_list.append(hparam)\n",
    "\n",
    "results_distill = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams' + utils.hparamToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net = student_net.to(fast_device)\n",
    "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
    "    results_distill[hparam_tuple] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs, \n",
    "                                                                train_loader, val_loader, \n",
    "                                                                print_every=print_every, \n",
    "                                                                fast_device=fast_device)\n",
    "    save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    torch.save({'results' : results_distill[hparam_tuple], \n",
    "                'model_state_dict' : student_net.state_dict(), \n",
    "                'epoch' : num_epochs}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "T_scatter = [math.log(h['T']) for h in hparams_list]\n",
    "alpha_scatter = [h['alpha'] for h in hparams_list]\n",
    "colors = []\n",
    "for i in range(len(hparams_list)):\n",
    "    cur_hparam_tuple = utils.hparamDictToTuple(hparams_list[i])\n",
    "    colors.append(results_distill[cur_hparam_tuple]['val_acc'][-1])\n",
    "    \n",
    "marker_size = 100\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(T_scatter, alpha_scatter, marker_size, c=colors, edgecolors='black')\n",
    "plt.colorbar()\n",
    "for i in range(len(T_scatter)):\n",
    "    ax.annotate(str('%0.4f' % (colors[i], )), (T_scatter[i], alpha_scatter[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['r', 'b', 'g', 'm']\n",
    "for hparam, c in zip(hparams_list, color):\n",
    "    cur_results = results_distill[utils.hparamDictToTuple(hparam)]\n",
    "    plt.plot(cur_results['val_acc'], color=c, label=str(hparam['alpha']))\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('Validation Accuracy vs Epoch for different alpha')\n",
    "plt.savefig(summaries_path_student + 'val_acc_vs_epoch_wrt_alpha.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hparam in hparams_list:\n",
    "    load_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
    "    load_dict = torch.load(load_path)\n",
    "    student_net = networks.StudentNetwork()\n",
    "    student_net.load_state_dict(load_dict['model_state_dict'])\n",
    "    student_net = student_net.to(fast_device)\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "    print(utils.hparamToString(hparam))\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
